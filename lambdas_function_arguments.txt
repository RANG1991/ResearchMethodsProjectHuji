        labels_and_scores.sort(key=lambda x: x[1], reverse=True)
        labels_and_scores.sort(key=lambda x: x[1], reverse=True)
    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)[
    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)['state_dict']
            checkpoint = torch.load(args.resume, map_location=lambda storage, loc: storage)['state_dict']
    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)['state_dict']
    fps = sorted(fps, key=lambda x: int(x.split('/')[-1].replace('.jpg', '')))
    obj = eval(js_var, type('Dummy', (dict,), dict(__getitem__=lambda s, n: n))())
    sorted_scores = sorted(result, key=lambda p_x: p_x[1][1], reverse=True)
            batch = sorted(batch, key=lambda t: t[0].size(0), reverse=True)

        batch_idx = sorted(range(len(batch)), key=lambda i: batch[i][0].size(0), reverse=True)
        batch_idx = sorted(range(len(batch)), key=lambda i: batch[i][0].size(1), reverse=True)
        checkpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)
            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_classifier_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_classifier_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_localizer_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_localizer_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_classifier_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_classifier_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_localizer_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_localizer_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_iou_threshold=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_iou_threshold=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_victim_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_victim_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_target_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_target_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_victim_cw_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_victim_cw_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_victim_cw_confidence=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_victim_cw_confidence=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_target_cw_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_target_cw_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_target_cw_confidence=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, box_target_cw_confidence=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_iou_threshold=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_iou_threshold=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_background_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_background_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_foreground_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_foreground_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_cw_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_cw_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_cw_confidence=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, rpn_cw_confidence=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, similarity_weight=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, similarity_weight=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, learning_rate=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, learning_rate=-1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, optimizer="test")

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, optimizer="MomentumOptimizer", momentum=1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, sign_gradients="true")

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, random_size=1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, random_size=-1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, max_iter=1.0)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, max_iter=-1)

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, texture_as_input="true")

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, use_spectral="true")

            _ = ShapeShifter(obj_dec, random_transform=lambda x: x + 1e-10, soft_clip="true")
    scored_parts = sorted(scored_parts, key=lambda x: x[0], reverse=True)
            orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)]

    bigL = [v[0] for v in sorted(headerTable.items(), key=lambda p: p[1])]

    print('-----', sorted(headerTable.items(), key=lambda p: p[1]))
    return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[: N]
    sortedSF=sorted(topSF,key=lambda pair:pair[1],reverse=True)

    sortedNY=sorted(topNY,key=lambda pair:pair[1],reverse=True)
            orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)]

    bigL = [v[0] for v in sorted(headerTable.items(), key=lambda p: p[1][0])]

    print('-----', sorted(headerTable.items(), key=lambda p: p[1][0]))
    return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[: N]
    sorted_sf = sorted(top_sf, key=lambda pair: pair[1], reverse=True)

    sorted_ny = sorted(top_ny, key=lambda pair: pair[1], reverse=True)
    wordPairs = sorted([(k,v) for k,v in counted_words.items() if v>=2], key=lambda  x: x[1], reverse=True)
            for action_command in sorted(actions, key=lambda d: d.name):

            for group_command in sorted(groups, key=lambda d: d.name):
        data=sorted(dagbag.dags.values(), key=lambda d: d.dag_id),

    dag_runs.sort(key=lambda x: x.execution_date, reverse=True)
    yield from sorted(optional, key=lambda x: get_long_option(x).lower())

    for command in sorted(subcommands, key=lambda x: x.name):
    AirflowConsole().print_as(data=variables, output=args.output, mapper=lambda x: {"key": x.key})
        rows = sorted(rows, key=lambda x: x[3] or 0.0)
        sorted_adopted_task_timeouts = sorted(self.adopted_task_timeouts.items(), key=lambda k: k[1])
            ENV.from_string(json.dumps(unstructure(obj), default=lambda o: None))
        ``dict(hello=lambda name: 'Hello %s' % name)`` to this argument allows

        for orm_dag in sorted(orm_dags, key=lambda d: d.dag_id):
        self.dagbag_stats = sorted(stats, key=lambda x: x.duration, reverse=True)
            ordered_tis_by_start_date.sort(key=lambda ti: ti.start_date, reverse=False)
        snapshots.sort(key=lambda x: x['SnapshotCreateTime'], reverse=True)
        result = sorted(grouped_logs.items(), key=lambda kv: getattr(kv[1][0], 'message', '_'))
    for child in sorted(task_group.children.values(), key=lambda t: t.node_id if t.node_id else ""):
    for menu_link in sorted(plugins_manager.flask_appbuilder_menu_links, key=lambda x: x["name"]):
        task_group_to_dict(child) for child in sorted(task_group.children.values(), key=lambda t: t.label)

            operators=sorted({op.task_type: op for op in dag.tasks}.values(), key=lambda x: x.task_type),

            operators=sorted({op.task_type: op for op in dag.tasks}.values(), key=lambda x: x.task_type),
        for package_name in sorted(packages_names, key=lambda k: k or '')
        config["options"] = sorted(config["options"], key=lambda o: o["name"])

    configs = sorted(configs, key=lambda l: l["name"])

        section.sort(key=lambda i: i["name"])  # type: ignore
    return sorted(results, key=lambda d: d["integration"]["integration-name"].lower())
        'dev_index_template.html.jinja2', providers=sorted(providers, key=lambda k: k['package-name'])
        tables = sorted(metastore.get_tables(db=db), key=lambda x: x.tableName)
result_integrations = sorted(result_integrations, key=lambda x: x['name'].lower())
        pools = sorted(pool_api.get_pools(), key=lambda p: p.pool)
        pools = sorted(self.client.get_pools(), key=lambda p: p[0])
    task_d = EmptyOperator(task_id="test_task_on_execute", on_execute_callback=lambda *args, **kwargs: None)

    task_e = EmptyOperator(task_id="test_task_on_success", on_success_callback=lambda *args, **kwargs: None)
task = PythonOperator(task_id='task1', python_callable=lambda x: sleep(x), op_args=[600], dag=dag)
            task1 = EmptyOperator(task_id=task_id_1, on_failure_callback=lambda x: print("hi"))

            task1 = EmptyOperator(task_id=task_id_1, on_failure_callback=lambda x: print("hi"))

        tis = sorted(tis, key=lambda ti: ti.key)
        pools = sorted(Pool.get_pools(), key=lambda p: p.pool)
        ti0, ti1 = sorted(dr.task_instances, key=lambda ti: ti.task_id)

        ti0, ti1 = sorted(dr.task_instances, key=lambda ti: ti.task_id)

        ti0, ti1 = sorted(dr.task_instances, key=lambda ti: ti.task_id)

        ti0, ti1 = sorted(dr.task_instances, key=lambda ti: ti.task_id)

        ti0, ti1 = sorted(dr.task_instances, key=lambda ti: ti.task_id)

        ti0, ti1 = sorted(dr.task_instances, key=lambda ti: ti.task_id)

        ti1, ti2 = sorted(dr.task_instances, key=lambda ti: ti.task_id)
    @mock.patch('airflow.utils.log.secrets_masker.redact', autospec=True, side_effect=lambda d, _=None: d)
    ti1, _ = sorted(dr.task_instances, key=lambda ti: ti.task_id)

    _, make_list_ti = sorted(dr.task_instances, key=lambda ti: ti.task_id)

    tis = sorted(dr.task_instances, key=lambda ti: ti.task_id)
            PythonVirtualenvOperator(python_callable=lambda x: 4, task_id='task', dag=self.dag)
        tasks = sorted((ti for ti in dagrun.task_instances), key=lambda ti: ti.task_id)
            dag_dict["dag"]["tasks"] = sorted(dag_dict["dag"]["tasks"], key=lambda x: sorted(x.keys()))
        for i, pool in enumerate(sorted(pools, key=lambda p: p['pool'])):
            candidates = sorted(self.sessions.keys(), key=lambda k: -self.sessions[k].get_age())
            for task in sorted(self.tasks.values(), key=lambda x: x.started)
    # pruned_indices = sorted(pruned_indices, key=lambda i: -np.max(results[attrib_idx][i]))
        >>> aug = A.Compose([A.FDA([target_image], p=1, read_fn=lambda x: x)])
        pts = np.array(sorted(pts, key=lambda x: x[0]))
    transforms = [Mock(p=1, side_effect=lambda **kw: {"image": kw["image"]}) for _ in range(10)]

    transforms = [Mock(side_effect=lambda **kw: kw) for _ in range(10)]
    for transform, info in sorted(transforms_info.items(), key=lambda kv: kv[0]):

        "- " + info["docs_link"] for transform, info in sorted(transforms_info.items(), key=lambda kv: kv[0])
        for i in sorted(intervals, key=lambda i: i.start):

    intervals.sort(key=lambda i: i[0])
        kee = min(self.val.keys(), key=lambda x: len(self.val[x]))
    edges.sort(key=lambda edge: edge.weight)
    people.sort(key=lambda x: (-x[0], x[1]))
    intervals = sorted(intervals, key=lambda x: x.start)
    symbols = sorted(symbols, key=lambda _: len(_), reverse=True)
    return min((a,b), key=lambda x: abs(target-x))
        entry.regular_files.sort(key=lambda meta: meta.creation_time, reverse=True)

        entry.extraction_dirs.sort(key=lambda meta: meta.creation_time, reverse=True)
            for key, val in sorted(dictionary.items(), key=lambda item: order_func(item[0])):
            token_counts.sort(key=lambda x: x[1], reverse=True)

                tokens_with_counts.sort(key=lambda x: x[1], reverse=True)

                tokens_with_counts.sort(key=lambda x: len(x[0]), reverse=True)
        with_indices.sort(key=lambda x: x[0][0])
            candidates = heapq.nsmallest(self.beam_size, candidates, key=lambda x: get_length(x[0]))
    tensor_dims.sort(key=lambda x: x[0])
    nav_entries.sort(key=lambda x: list(x)[0], reverse=False)
            top_k_sequences = sorted(scored_sequences, key=lambda r: r[0], reverse=True)[:top_k]
    predictions.sort(key=lambda x: x[1], reverse=True)

    predictions.sort(key=lambda x: x[1][0])

    labels.sort(key=lambda x: x[1], reverse=True)

    labels.sort(key=lambda x: x[1][0])
    examples = sorted(examples, key=lambda x: x["title"])
    for run in sorted(ended, key=lambda x: x['finishedDate']):
    return sorted(tests, key=lambda test: test.name)
    return sorted(groups, key=lambda g: (g.depth, g.priority, g.name))
    for file_info in sorted(file_manifest['files'], key=lambda x: x['name']):
    for suffix, limit in sorted(iteritems(SIZE_RANGES), key=lambda item: -item[1]):
        sorted_walk.sort(key=lambda x: x[0])
        ret.sort(key=lambda p: p.path.endswith('/windows'))
        patches = sorted(patches, key=lambda k: k.new.path)  # type: t.List[FileDiff]
    candidates = sorted(get_subclasses(CIProvider), key=lambda c: (c.priority, c.__name__))
    include_targets = sorted(filter_targets(targets, includes, directories=False), key=lambda include_target: include_target.name)

    return tuple(sorted(internal_targets, key=lambda sort_target: sort_target.name))
    return sorted(subclasses, key=lambda sc: sc.__name__)
    fallback = sorted(candidates, key=lambda value: str_to_version(value.version), reverse=True)[0]
        targets=[name for name, index in sorted(target_indexes.items(), key=lambda kvp: kvp[1])],

        target_sets=[sorted(data) for data, index in sorted(set_indexes.items(), key=lambda kvp: kvp[1])],
    return sorted(get_subclasses(provider_type), key=lambda c: (c.priority, c.__name__))
    sanity_tests = tuple(sorted(sanity_tests + collect_code_smell_tests(), key=lambda k: k.name))
        return sorted(all_instances, key=lambda x: x['InstanceId'])
    images.sort(key=lambda e: e.get('creation_date', ''))  # it may be possible that creation_date does not always exist
        checked_list.sort(key=lambda x: sorted(x.items()) if isinstance(x, dict) else x)
        instance_dict_array.sort(key=lambda x: x['id'])

        tagged_instances.sort(key=lambda x: x['id'])
            vifs_list = sorted(vifs_list, key=lambda i: i["vlan_id"])
            r_lst = sorted(rules_lst, key=lambda i: i["number"])

            a_lst = sorted(app_lst, key=lambda i: i["application"])
        mocker.patch('os.path.exists', side_effect=lambda x: False)

        mocker.patch('os.path.exists', side_effect=lambda x: True)

        mocker.patch('os.path.exists', side_effect=lambda x: True)
mock_unfrackpath_noop = MagicMock(spec_set=unfrackpath, side_effect=lambda x, *args, **kwargs: x)
    result.sort(key=lambda entry: entry['msg'])
    @patch('ansible.module_utils.facts.system.pkg_mgr.os.path.exists', side_effect=lambda x: x == '/opt/homebrew/bin/brew')

    @patch('ansible.module_utils.facts.system.pkg_mgr.os.path.exists', side_effect=lambda x: x == '/usr/local/bin/brew')

    @patch('ansible.module_utils.facts.system.pkg_mgr.os.path.exists', side_effect=lambda x: x == '/opt/local/bin/port')

    @patch('ansible.module_utils.facts.system.service_mgr.os.path.islink', side_effect=lambda x: x == '/sbin/init')

    @patch('ansible.module_utils.facts.system.service_mgr.os.readlink', side_effect=lambda x: '/sbin/runit-init' if x == '/sbin/init' else '/bin/false')

    @patch('ansible.module_utils.facts.system.service_mgr.os.path.islink', side_effect=lambda x: x == '/sbin/init')

    @patch('ansible.module_utils.facts.system.service_mgr.os.readlink', side_effect=lambda x: '/sbin/runit-init' if x == '/sbin/init' else '/bin/false')
@pytest.mark.parametrize("stdin, testcase", product([{}], TESTSETS), ids=lambda x: x.get('name'), indirect=['stdin'])
    @patch.multiple(DataLoader, path_exists=lambda s, x: True, is_file=lambda s, x: True)
        with patch('os.path.isdir', side_effect=lambda x: b'bogus' not in x):

    with patch.object(sys, 'path', ['/bogus', '/playbookdir']) and patch('os.path.isdir', side_effect=lambda x: b'bogus' in x):
        # temp = sorted(values.items(), key=lambda x: x[1], reverse=True)
            temp = sorted(values.items(), key=lambda x: x[1], reverse=True)
        fake_uniform.configure_mock(side_effect=lambda a, b: jitter)

        fake_uniform.configure_mock(side_effect=lambda a, b: jitter)
        title = max(possible_titles, key=lambda t: len(t))
            edges = sorted(edges, key=lambda e: e.x)

            edges = sorted(new_edges, key=lambda e: e.x)
            for x, y, z, tile, satellite in sorted(self._tiles.values(), key=lambda k: k[0]):
        for conf in sorted(subclasses, key=lambda x: x.__module__):
        macro.patch.sort(key=lambda x: x[2],reverse=True)
            f.sort(key=lambda x: x[1].__code__.co_firstlineno)

            s.sort(key=lambda x: len(x[1]), reverse=True)
                         sorted(models_1D.items(), key=lambda x: str(x[0])))

                         sorted(models_2D.items(), key=lambda x: str(x[0])))
def kuiper(data, cdf=lambda x: x, args=()):
        sorted_keywords = sorted(keywords, key=lambda x: x[1]._priority, reverse=True)
    units.sort(key=lambda x: x.name.lower())
            results.sort(key=lambda x: np.abs(x.scale))

            results.sort(key=lambda x: np.sum(np.abs(x.powers)))

            results.sort(key=lambda x: np.sum(x.powers) < 0.0)

            results.sort(key=lambda x: not is_effectively_unity(x.scale))

        new_parts.sort(key=lambda x: (-x[1], getattr(x[0], 'name', '')))
        units.sort(key=lambda x: cls._get_unit_name(x[0]).lower())
        units.sort(key=lambda x: cls._get_unit_name(x[0]).lower())
        units.sort(key=lambda x: cls._get_unit_name(x[0]).lower())
                    sorted(unannotated, key=lambda n: cd.get(n).counter)
EqCSameType = cmp_using(eq=lambda a, b: a == b, class_name="EqCSameType")

    cls = cmp_using(eq=lambda a, b: a == b)

            cmp_using(lt=lambda a, b: a < b)

        C = cmp_using(eq=lambda a, b: NotImplemented if a == 1 else a == b)
        @attr.s(auto_attribs=True, field_transformer=lambda c, a: list(a))
            "C", {"x": attr.ib(converter=lambda v: v + 1), "y": attr.ib()}

                "x": attr.ib(validator=validator, converter=lambda v: 1 / 0),

            "C", {"x": attr.ib(converter=lambda v: int(v))}, frozen=True

            "C", {"x": attr.ib(validator=lambda *a: None), "y": attr.ib()}
    x = attr.ib(on_setattr=lambda *args: None)

                x = attr.ib(on_setattr=lambda *args: None)
    c: str = attr.ib(repr=lambda value: "c is for cookie")

    c: str = attrs.field(repr=lambda value: "c is for cookie")
    merged.sort(key=lambda i: i[0].startswith('oauth_'))
        oauth.init_app(app, update_token=lambda o: o)

        oauth.init_app(app, fetch_token=lambda name: token)

        oauth.init_app(app, fetch_token=lambda name: expired_token)
    for root, _, files in sorted(walk, key=lambda x: x[0]):
    results.sort(key=lambda pair: pair[0])
                result_list = sorted(result_list, key=lambda x: x.index)

                val = sorted(val, key=lambda x: x.index)
                found_fs.sort(key=lambda f0: f0.full_path.lower())
        or sorted(exist_function_config.layers, key=lambda x: x.full_path)

        != sorted(function_config.layers, key=lambda x: x.full_path)
            group_list = sorted([{'id': g.id, 'name': g.name} for g in obj.groups.all()], key=lambda x: x['id'])[:5]
            response['instance_groups'] = sorted(response['instance_groups'], key=lambda x: x['name'].lower())
        auth_backends.sort(key=lambda x: 'g' if x[0] == 'google-oauth2' else x[0])

        group_data['children'].sort(key=lambda x: x['name'])
        host_counts = sorted(host_counts.items(), key=lambda item: [e.total_seconds() for e in item[1]], reverse=True)
        queue_order = sorted(range(len(self.workers)), key=lambda x: -1 if x == preferred_queue else x)
        all_zones.sort(key=lambda x: -len(x))
        return sorted(results, key=lambda x: smart_str(x).lower())

        return sorted(results, key=lambda x: smart_str(x).lower())
        all_tasks = sorted(jobs + project_updates + inventory_updates + system_jobs + ad_hoc_commands + workflow_jobs, key=lambda task: task.created)
        for g in sorted(actual_groups, key=lambda x: len(x.instances)):

            for i in sorted(actual_instances, key=lambda x: len(x.groups)):

        for g in sorted(actual_groups, key=lambda x: len(x.instances)):

            for i in sorted(actual_instances, key=lambda x: len(x.groups)):
    copied_node_list.sort(key=lambda x: int(x.unified_job_template.name[-1]))
    filename_list = sorted(filename_list, key=lambda fn: inverse_env.get(os.path.join(private_data_dir, fn), [fn])[0])
        before = sorted(instances, key=lambda x: random.random())
    rj = RunJob(instance=containerized_job, build_execution_environment_params=lambda x: {})
        good_role = mocker.MagicMock(__contains__=lambda self, user: True)

        bad_role = mocker.MagicMock(__contains__=lambda self, user: False)
            return mock.Mock(__iter__=lambda *args: iter(creds), first=lambda: creds[0] if len(creds) else None)
    ret.sort(key=lambda x: len(x))
    threading.Thread(target=lambda p, d: open(p, 'wb').write(d), args=(path, data)).start()
        default_methods.sort(key=lambda x: x.pattern == '.*')

                keys.sort(key=lambda x: x.pattern == '.*')
        process_info_list = sorted(process_info_list,key=lambda x:x['cpu_time_total'],reverse=True)
        sortedParameters = sorted(parameters.items(), key=lambda parameters: parameters[0])
        menus = sorted(result, key=lambda x: x['sort'])
        networkList = sorted(networkList, key=lambda x : x['status'], reverse=True)

        processList = sorted(processList, key=lambda x : x['memory_percent'], reverse=True)

        processList = sorted(processList, key=lambda x : x['cpu_times'], reverse=True)
        data['apps'] = sorted(data['apps'],key=lambda x: x['time'],reverse=True)

        data['binds'] = sorted(data['binds'],key=lambda x: x['time'],reverse=True)

                binds = sorted(binds,key=lambda x: x['time'],reverse=True)
            tmp_files = sorted(tmp_files, key=lambda x: x[sort_key], reverse=reverse)

        return sorted(v_data,key=lambda x:x['name'])
        sortedParameters = sorted(parameters.items(), key=lambda parameters: parameters[0])
            OOO0000O0OO00O00O .sort (key =lambda O000O00OOOOOOOO0O :O000O00OOOOOOOO0O ["time"])#line:638

            O000O000OOOOO000O .sort (key =lambda O000OOOO0000O00O0 :O000OOOO0000O00O0 ["time"])#line:654
        data['risk'] = sorted(data['risk'],key=lambda x: x['level'],reverse=True)

        data['security'] = sorted(data['security'],key=lambda x: x['level'],reverse=True)

        data['ignore'] = sorted(data['ignore'],key=lambda x: x['level'],reverse=True)
        new_node_list = sorted(node_list,key=lambda x: x['ping'],reverse=False)

    menus = sorted(data, key=lambda x: x['sort'])
        sortedParameters = sorted(parameters.items(), key=lambda parameters: parameters[0])
        host_list = sorted(host_list,key=lambda x: x['sort'],reverse=False)
            [item[1] for item in sorted(pbounds.items(), key=lambda x: x[0])],
    extra_items.sort(key=lambda i: (i.disc, i.track, i.title))

    extra_tracks.sort(key=lambda t: (t.index, t.title))

    return sorted(candidates, key=lambda match: match.distance)
    pairs.sort(key=lambda item_and_track_info: item_and_track_info[1].index)
        subcommands.sort(key=lambda c: c.name)
    spans.sort(key=lambda x: x['from'])
        for i in sorted(task.items, key=lambda i: i.track):
        matches.sort(key=lambda x: x['likes'], reverse=True)
        release = Bag(data={}, refresh=lambda *args: None)
        displayed_nodes.sort(key=lambda elem: elem[0].col_offset)

        values.sort(key=lambda e: e[1])
            answer.features.sort(key=lambda x: x.location.start.position)
            self._records.sort(key=lambda r: r.id, reverse=reverse)
        possible.sort(key=lambda x: (len(self.ambiguous_protein[x]), x))
    consensus_ids.sort(key=lambda x: len(consensus.node(x).data.taxon))
        return sorted(self.child_dict.values(), key=lambda a: ord(a.altloc))

            child = sorted(self.child_dict.values(), key=lambda a: a.occupancy)[-1]
    return sorted((pn for pn in p if p[pn]), key=lambda pn: p[pn])
        lines.append("Minum signal %.2f at time %.2f" % min(self, key=lambda x: x[1]))

        lines.append("Maximum signal %.2f at time %.2f" % max(self, key=lambda x: x[1]))

        self.max = max(self, key=lambda x: x[1])[1]

        self.min = min(self, key=lambda x: x[1])[1]
    strict_bitstrs.sort(key=lambda bitstr: bitstr.count("1"), reverse=True)

        bsckeys = sorted(bitstr_clades, key=lambda bs: bs.count("1"), reverse=True)

                    for ta in sorted(to_add, key=lambda bs: bs.count("1")):
    for attrname, child in sorted(elem.__dict__.items(), key=lambda kv: kv[0]):

        self.root.clades.sort(key=lambda c: c.count_terminals(), reverse=reverse)

            new_max = max(self.depths().items(), key=lambda nd: nd[1])
        terms.sort(key=lambda term: term.name)
        ls.sort(key=lambda x: len(x[1]))
        for n in sorted(self._sunidDict.values(), key=lambda x: x.sunid):

        for n in sorted(self._sunidDict.values(), key=lambda x: x.sunid):

        for n in sorted(self._sidDict.values(), key=lambda x: x.sunid):
        right_rows = sorted(right_rows, key=lambda x: x[0], reverse=True)

        left_rows = sorted(left_rows, key=lambda x: x[0], reverse=True)
        alignment.sort(key=lambda record: GC(record.seq))

        alignment.sort(key=lambda record: GC(record.seq), reverse=True)
        rxs.sort(key=lambda x: str(x))  # noqa: E731
        records.sort(key=lambda rec: rec.id)  # noqa: E731

        records.sort(key=lambda rec: rec.id)  # noqa: E731

        records.sort(key=lambda rec: rec.id)  # noqa: E731

        records.sort(key=lambda rec: rec.id)  # noqa: E731

        records.sort(key=lambda rec: rec.id)  # noqa: E731

        records.sort(key=lambda rec: rec.id)  # noqa: E731
                points1.sort(key=lambda point: point.index)  # noqa: E731
        self.assertEqual(max(w, key=lambda x: x[1]), (16.75, 313.0))  # noqa: E731

        self.assertEqual(min(w, key=lambda x: x[1]), (0.25, 29.0))  # noqa: E731

        self.assertEqual(max(w2, key=lambda x: x[1]), (18.25, 357.0))  # noqa: E731

        self.assertEqual(min(w2, key=lambda x: x[1]), (0.25, 55.0))  # noqa: E731

        self.assertEqual(max(w2, key=lambda x: x[1]), (15.75, 274.0))  # noqa: E731

        self.assertEqual(min(w2, key=lambda x: x[1]), (3.25, -20.0))  # noqa: E731
        for _, cls in sorted(Model.model_class_reverse_map.items(), key=lambda arg: arg[0]):

    result = Urls(urls=lambda components, kind: [mk_url(component, kind) for component in components])

    return Urls(urls=lambda components, kind: [mk_url(component, kind) for component in components])
    results = sorted(results, key=lambda attr: attr.name)
for name, palettes in sorted(all_palettes.items(), key=lambda arg: arg[0]):
    for warning in sorted(warnings, key=lambda warning: warning.code):

    for error in sorted(errors, key=lambda error: error.code):
        kwarg_params.sort(key=lambda x: x[0].name)
    details_iter = status_iterator(details, "creating gallery file entries... ", "brown", len(details), app.verbosity, stringify_func=lambda x: x["name"] + ".rst")
    files_iter = status_iterator(files, "copying bokeh-plot files... ", "brown", len(files), app.verbosity, stringify_func=lambda x: basename(x[0]))
    ordered_models = sorted(custom_models.values(), key=lambda model: model.full_name)

        dependencies = sorted(dependencies, key=lambda name_version: name_version[0])

    exports = sorted(exports, key=lambda spec: spec[1])

    modules = sorted(modules, key=lambda spec: spec[0])
sorted_fruits = sorted(fruits, key=lambda x: counts[fruits.index(x)])
names = [node['name'] for node in sorted(data['nodes'], key=lambda x: x['group'])]
            tmp.items(), key=lambda item: StrictVersion(item[0]), reverse=True
sorted_fruits = sorted(fruits, key=lambda x: counts[fruits.index(x)])
    return sorted(os.scandir(path), key=lambda entry: entry.name)
        ret = sorted(self.iteritems(), key=lambda x: x[1], reverse=True)
        >>> omd.sorted(key=lambda i: i[1])  # i[0] is the key, i[1] is the val
    >>> bucketize(range(5), value_transform=lambda x: x*x)

    >>> bucketize(range(10), key=lambda x: x % 3, key_filter=lambda k: k % 3 != 1)

    >>> list(unique_iter(pleasantries, key=lambda x: len(x)))

    >>> one((10, 20, 30, 42), key=lambda i: i > 40)

    >>> first([1, 1, 3, 4, 5], key=lambda x: x % 2 == 0)

def research(root, query=lambda p, k, v: True, reraise=False):

    >>> res = research(root, query=lambda p, k, v: isinstance(v, int))

        first = sorted([x for x in seq if key(x) in first], key=lambda x: first.index(key(x)))

        last = sorted([x for x in seq if key(x) in last], key=lambda x: last.index(key(x)))
    bc = LRI(cache_size, on_miss=lambda k: k.upper())
        >>> omd.sorted(key=lambda i: i[1])  # i[0] is the key, i[1] is the val
    def default_non_roundtrippable_repr(x=lambda y: y + 1):
    def kwonly_non_roundtrippable_repr(*, x=lambda y: y + 1):
    res = research(root, query=lambda p, k, v: v == 'a')
for cls in sorted(classes, key=lambda cls: (cls.__module__, cls.__qualname__)):
            add_common_option('--umask', metavar='M', dest='umask', type=lambda s: int(s, 8), default=UMASK_DEFAULT,

        subparser.add_argument('--stdin-mode', metavar='M', dest='stdin_mode', type=lambda s: int(s, 8), default=STDIN_MODE_DEFAULT,
        self.chunks = LRUCache(capacity=10, dispose=lambda _: None)

        self._inode_cache = LRUCache(capacity=FILES, dispose=lambda _: None)

        self.data_cache = LRUCache(capacity=data_cache_capacity, dispose=lambda _: None)

        self._last_pos = LRUCache(capacity=FILES, dispose=lambda _: None)
zero_chunk_ids = LRUCache(10, dispose=lambda _: None)

            for item in archive.iter_items(filter=lambda item: os.path.basename(item.path) == CACHE_TAG_NAME):
        ranges = [k for k, v in sorted(multiplier.items(), key=lambda t: t[1])]
        c = LRUCache(2, dispose=lambda _: None)

        c = LRUCache(2, dispose=lambda f: f.close())
        keys.sort(key=lambda x: x.lower())

        keys.sort(cmp=lambda x, y: cmp(x.lower(), y.lower()))

        keys.sort(key=lambda x: x.lower())
            qsa.sort(key=lambda x: x[0])
        all_snapshots.sort(key=lambda x: x.start_time)
        snaps.sort(cmp=lambda x, y: cmp(x.date, y.date))
        for key, value in sorted(params.items(), key=lambda x: x[0]):
      items = sorted(dictionary.items(), key=lambda x:x[0])
        tags = sorted(response[0], key=lambda tag: tag.key)
        update.sort(key=lambda x: x['Update']['IndexName'])

        update.sort(key=lambda x: x['Update']['IndexName'])
            journal = {k: v for k, v in sorted(journal.items(), key=lambda item: item[1]["timestamp"], reverse=True)}
        list.sort(key=lambda a: a.lower())
            possible_quotes.sort(key=lambda q: q[0] == escaped_string[-1])
                    modes[char] = max(items, key=lambda x: x[1])
            members.sort(key=lambda t: (t[1], t[0]))

            members.sort(key=lambda t: t[0])

    members.sort(key=lambda m: m._value_, reverse=True)
    results.sort(key=lambda pair: pair[0])
        directories.sort(key=lambda a: a.name)
        L.sort(key=lambda item:item[1].index)
    code_items.sort(key=lambda item:(len(item[1]), item[0]))

    lengths.sort(key=lambda item:(item[1], item[0]))
    def general_op_literal(self, ctx, compare, decorate=lambda x: x):

    def general_op_in(self, ctx, decorate=lambda x: x):

    def general_op_groupref(self, ctx, decorate=lambda x: x):
        self._test_recursive_list(REX_six, aslist=lambda x: x.items)

        self._test_recursive_tuple_and_list(REX_six, aslist=lambda x: x.items)

        self._test_recursive_dict(REX_seven, asdict=lambda x: x.table)

        self._test_recursive_tuple_and_dict(REX_seven, asdict=lambda x: x.table)

        self._test_recursive_dict_key(REX_seven, asdict=lambda x: x.table)

        self._test_recursive_tuple_and_dict_key(REX_seven, asdict=lambda x: x.table)
        data.sort(key=lambda r: r[1])
        self.assertEqual(data, sorted(copy, key=lambda x: -x))
        cf = self.fromstring(ini, optionxform=lambda opt: opt)
        (x, y), (z, t) = sorted(v.items(), key=lambda pair: pair[0].i)
    test_classes = sorted(set(test_classes), key=lambda cls: cls.__qualname__)
    >>> print(sorted(a.keys(), key=lambda x: (str(type(x)), x)))
            fi = FileInput(inplace=1, openhook=lambda f, m: None)
        self.client.storbinary('stor', f, callback=lambda x: flag.append(None))

        self.client.storlines('stor foo', f, callback=lambda x: flag.append(None))
        L.sort(key=lambda x: (type(x).__name__, x))
        sorted_ = lambda l: sorted(l, key=lambda x: id(x))
        self.assertRaises(TypeError, data.sort, key=lambda x,y: 0)

        data.sort(key=lambda t: t[0])   # sort on the random first field

        self.assertRaises(ZeroDivisionError, data.sort, key=lambda x: 1/x)

        copy2.sort(key=lambda x: x[0], reverse=True)
    >>> def f(*, x=lambda __debug__:0): pass
            t = threading.Thread(target=lambda : time.sleep(0.3))
        b = ET.TreeBuilder(pi_factory=lambda target, text: (len(target), text))
                zipfp.writepy(packagedir, filterfunc=lambda whatever: False)
        self.assertEqual(self.loads(s, object_pairs_hook=lambda x: x), p)
mx, my = min(borders_distance, key=lambda m: abs(m[0] + m[1]))

def add_seen_k(k, f=lambda x: 0):
data.sort(key=lambda r: r[1])
assert sorted(['a2', 'b3', 'c1'], key=lambda a: a[1]) == ['c1', 'a2', 'b3']

ls = ['a2', 'b3', 'c1']; ls.sort(key=lambda a:a[1])
    code_items.sort(key=lambda item:(len(item[1]), item[0]))
    lengths.sort(key=lambda item:(item[1], item[0]))
        files.sort(key=lambda a: a['name'])
        unclaim_brs.sort(key=lambda brd: brd['submitted_at'])
            brdicts.sort(key=lambda brd: brd['submitted_at'])
                [getBuildInfo(build) for build in builds], key=lambda bi: bi['name'])
        fields = [val for k, val in sorted(fields_dict.items(), key=lambda x: x[0]) if val]
            return max(changesByCodebase[codebase], key=lambda change: change["changeid"])
            params = sorted(self.params.items(), key=lambda x: x[0])
        return defer.succeed(sorted(testData.values(), key=lambda v: v['testid']))

        return defer.succeed(sorted(data, key=lambda v: v['stepid']))
        rv.sort(key=lambda bs: -bs['bsid'])
        ret.sort(key=lambda r: r['number'])
                    exp = sorted(exp + implied, key=lambda k: k["name"])
        callback = mock.Mock(side_effect=lambda *a, **kw: d.callback(None))

        callback = mock.Mock(side_effect=lambda *a, **kw: d.callback(None))
        yield self._new_change_source(owner='owner', slug='slug', pullrequest_filter=lambda x: True)
        self.setfilter(filter_fn=lambda ch: ch.x > 3)
        sortedList = sorted(noneInList, key=lambda x: ReverseComparator(NoneComparator(x)))
        self.assertEqual(sorted(bdicts, key=lambda bd: bd['id']),

        self.assertEqual(sorted(bdicts, key=lambda bd: bd['id']),

        self.assertEqual(sorted(bdicts, key=lambda bd: bd['id']),

        self.assertEqual(sorted(bdicts, key=lambda bd: bd['id']),

        self.assertEqual(sorted(bdicts, key=lambda bd: bd['id']),

        self.assertEqual(sorted(bdicts, key=lambda bd: bd['id']),

        self.assertEqual(sorted(bdicts, key=lambda bd: bd['id']),

        self.assertEqual(sorted(bdicts, key=lambda bd: bd['id']),

        self.assertEqual(sorted(bdicts, key=lambda bd: bd['id']),

        self.assertEqual(sorted(bdicts, key=lambda bd: bd['id']),
            changes.sort(key=lambda c: c['changeid'])
        result_dicts = sorted(result_dicts, key=lambda x: x['id'])
                   mock.Mock(side_effect=lambda c: defer.succeed(None)))
                       lambda lst: sorted(lst, key=lambda m: m.name)[-1])
        workers.sort(key=lambda a: a.name)

            return sorted(builders, key=lambda b1: b1.name)

            bldrs.sort(key=lambda b1: b1.name)

            return sorted(builders, key=lambda b: b.name)

            return defer.succeed(sorted(builders, key=lambda b: b.name))
        self.setup_step(self.FakeBuildStep(doStepIf=lambda step: False))
        command = WithProperties('%(foo)s', foo=lambda _: 'bar')

        command = WithProperties('%(x)s', x=lambda _: 20)

            '%(z)s', z=lambda props: props.getProperty('x') + props.getProperty('y'))

        self.assertNotEqual(WithProperties("%(key)s", key=lambda p: 'val'), WithProperties(

            "%(key)s", key=lambda p: 'val'))
        function = mock.Mock(side_effect=lambda x: {'key': 'value'})

        function = mock.Mock(side_effect=lambda x: {'key': 'value'})
            dict(fileIsImportant=lambda c: True),

            dict(fileIsImportant=lambda c: False),

            dict(fileIsImportant=lambda c: 1 / 0),

            dict(fileIsImportant=lambda c: False, onlyImportant=True),

            dict(fileIsImportant=lambda c: True, onlyImportant=True),
        setup = mock.Mock(side_effect=lambda **kwargs: defer.succeed(None))

        upgrade = mock.Mock(side_effect=lambda **kwargs: defer.succeed(None))
        setup = mock.Mock(side_effect=lambda **kwargs: defer.succeed(None))

        upgrade = mock.Mock(side_effect=lambda **kwargs: defer.succeed(None))

        setup = mock.Mock(side_effect=lambda **kwargs: defer.succeed(None))
    @debounce.method(wait=4.0, get_reactor=lambda self: self.reactor)
        self.run_process_obj.send_signal = mock.Mock(side_effect=lambda sig: override_kill_success)
            getLoginURL = mock.Mock(side_effect=lambda x: defer.succeed("://"))
                got['content'][typeName].sort(key=lambda x: sorted(x.items()))

            exp['content'][typeName].sort(key=lambda x: sorted(x.items()))
    l.sort(key=lambda x: keys[id(x)])
        reconfigurable_services.sort(key=lambda svc: -svc.reconfig_priority)

        reconfigurable_services.sort(key=lambda svc: -svc.reconfig_priority)
        sorted_query = sorted(query.items(), key=lambda x: x[0])

        sorted_url = sorted(url.items(), key=lambda x: x[0])
        sorted_oauth_params = sorted(oauth_params.items(), key=lambda val: val[0])
        content = [(l, sorted(content[l], key=lambda tup: tup[0].lower()))
    POSTS = sorted(POSTS, key=lambda x: x['date'])
        self.plugins = sorted(plugins, key=lambda plugin: plugin.ORDER)
    POSTS = sorted(POSTS, key=lambda x: x['date'])
def expand_dirs(items, exclude=lambda x: x.endswith('.so')):
    for pl in sorted(input_format_plugins(), key=lambda x: x.name):

    for pl in sorted(output_format_plugins(), key=lambda x: x.name):

        for opt in sorted(options, key=lambda x: x.get_opt_string()):

    documented_cmds.sort(key=lambda x: x[0])
    for x in sorted(files, key=lambda x: os.stat(x).st_size, reverse=True):
    def add_tree(self, base, prefix, ignore=lambda n:False):
    inits.sort(key=lambda x:x.count('/'))

    pstats = list(map(plugin_stats, sorted(stats.items(), reverse=True, key=lambda x:x[1])))
        for x in sorted(entries, key=lambda x:name_getter(x).lower()):
        for x in sorted(times, key=lambda x: times[x]):

    _initialized_plugins.sort(key=lambda x: x.priority, reverse=True)

        zf.add_dir(path, simple_filter=lambda x:x in {'.git', '.bzr', '.svn', '.hg'})
                plugin_classes.sort(key=lambda c:(getattr(c, '__module__', None) or '').count('.'))

        candidates.sort(key=lambda x: x.count('/'))
input_profiles.sort(key=lambda x: x.name.lower())

output_profiles.sort(key=lambda x: x.name.lower())
            return sorted(book_ids, key=lambda x:ff('series_index', x))
    def __init__(self, library_path, default_prefs=None, restore_all_prefs=False, progress_callback=lambda x, y:True):
        self.items = OrderedDict(sorted(items, key=lambda x:order.get(x[0], 0)))
    def _get(self, field, idx, index_is_id=True, default_value=None, fmt=lambda x:x):
def uniq(vals, kmap=lambda x:x):
    categories.sort(key=lambda x: x if x[0] != '#' else x[1:])
                iteritems(vals), key=lambda k: 1 if k[0].endswith('_index') else 0):
        for lid in sorted(library_map, key=lambda lid: (lid != default_library, lid)):
        q(['added.epub ignored.md'.split()], find_books_in_directory('', True, listdir_impl=lambda x: files))

        q([['added.epub'], ['ignored.md']], find_books_in_directory('', False, listdir_impl=lambda x, **k: files))

            q(['added.epub non-book.other'.split()], find_books_in_directory('', True, compiled_rules=rules, listdir_impl=lambda x: files))
            ans.sort(key=lambda x:x['seq'])
        lq = sorted(lmap, key=lambda x: calibre_langcode_to_name((lmap[x] or ('',))[0]))
    devplugins = list(sorted(devplugins, key=lambda x: x.__class__.__name__))
    devplugins = list(sorted(device_plugins(), key=lambda x: x.__class__.__name__))
        for idx in sorted(itervalues(bl_cache), reverse=True, key=lambda x: x or -1):

        for idx in sorted(itervalues(bl_cache), reverse=True, key=lambda x: x or -1):
            for e in sorted(c, key=lambda x:sort_key(x.name)):
        storage.sort(key=lambda x:x.get('id', 'zzzzz'))
                    device_offset = max(time_offsets, key=lambda a: time_offsets.get(a))
            items.sort(key=lambda x: int(x.get('id')))
                    device_offset = max(time_offsets, key=lambda a: time_offsets.get(a))
        vols.sort(key=lambda x: x['node'])
        ans.sort(key=lambda x: x[5:] if x else 'zzzzz')

        nodes.sort(key=lambda x: x[1])
        for idx in sorted(itervalues(bl_cache), reverse=True, key=lambda x: -1 if x is None else x):
def mobi_exploder(path, tdir, question=lambda x:True):

def zip_exploder(path, tdir, question=lambda x:True):

def docx_exploder(path, tdir, question=lambda x:True):
def render_pages(tasks, dest, opts, notification=lambda x, y: x):
        possible_new_codes = [x[0] for x in sorted(new_codes_count, key=lambda c: c[1])]
        html_files.sort(key=lambda x: x[1])
        items = sorted(xe_fields, key=lambda x:sort_key(x['text']))
        for (cls, css) in sorted(itervalues(self.classes), key=lambda x:x[0]):
        directory.sort(key=lambda x: x.name.lower())
    style = property(fget=lambda self : self._document.objects[self.style_id])

    evenheader = property(fget=lambda self : self._document.objects[self.style.evenheaderid])

    evenfooter = property(fget=lambda self : self._document.objects[self.style.evenfooterid])

    oddheader  = property(fget=lambda self : self._document.objects[self.style.oddheaderid])

    oddfooter  = property(fget=lambda self : self._document.objects[self.style.oddfooterid])

    style = property(fget=lambda self : self._document.objects[self.style_id])

    textstyle = property(fget=lambda self : self._document.objects[self.textstyle_id])

    style = property(fget=lambda self : self._document.objects[self.style_id])

    encoding = property(fget=lambda self : self._document.objects[self.refstream].encoding)

    data = property(fget=lambda self : self._document.objects[self.refstream].stream)

    encoding = property(fget=lambda self : self.imgext[self.stream_flags & 0xFF].upper())

    refpage = property(fget=lambda self : self.jump_action(2)[0])

    refobj = property(fget=lambda self : self.jump_action(2)[1])

    data = property(fget=lambda self: self.stream)
    covers.sort(key=lambda x: len(x[0]), reverse=True)
    def get_all(self, predicate=lambda x: x):
    formats.sort(key=lambda x: METADATA_PRIORITIES[path_to_ext(x)])
        recs = sorted(recs, key=lambda x:(x[0],x[0]))
    def deepcopy(self, class_generator=lambda : Metadata(None)):
def uniq(vals, kmap=lambda x:x):

    for namespace in sorted(groups, key=lambda x:{NS_MAP['dc']:'a', NS_MAP['xmp']:'b', NS_MAP['calibre']:'c'}.get(x, 'z'+x)):
def explode(path, dest, question=lambda x:True):
        self.records.sort(key=lambda x:x.type)
                    l.sort(key=lambda x:x.offset)

            sections = sorted(sections, key=lambda x:x.offset)

                nodes.sort(key=lambda x:x.index)

        indices.sort(key=lambda x:x.offset)

                normalized_articles.sort(key=lambda x:x.offset)

        normalized_sections.sort(key=lambda x:x[0].offset)

                sorted(sections, key=lambda x:x.offset))
            self.guide_table.sort(key=lambda x:x.type)  # Needed by the Kindle
        return sorted(itervalues(self.refs), key=lambda ref: ref.ORDER.get(ref.type, 10000))
        replace_links(container, link_sub, frag_map=lambda x, y:None)
def download_external_resources(container, urls, timeout=60, progress_report=lambda url, done, total: None):
def compress_images(container, report=None, names=None, jpeg_quality=None, progress_callback=lambda n, t, name:True):
def replace_links(container, link_map, frag_map=lambda name, frag:frag, replace_in_opf=False):
    root = parse('<html><body><div>%s</div></body></html>' % text, decoder=lambda x:x.decode('utf-8'))

    root = parse('<html><body><div>%s</div></body></html>' % text, decoder=lambda x:x.decode('utf-8'))

    return sorted(locations, key=lambda l:(order[l.file_name], l.sourceline))
            covers.sort(key=lambda x: x[1], reverse=True)
        items = sorted(((key, val) for (val, key) in iteritems(styles)), key=lambda x:numeric_sort_key(x[0]))
        page_breaks.sort(key=lambda x:int(x.get('pb_order')))
    fonts.sort(key=lambda f: len(f['Data'] or b''), reverse=True)

def convert(opf_path, opts, metadata=None, output_path=None, log=default_log, cover_data=None, report_progress=lambda x, y: None):
        self.elements.sort(key=lambda x: x.bottom)

            for x in sorted(columns, key=lambda x: x.left):

        self.elements.sort(key=lambda x: x.top)

        neighbors = sorted(neighbors, key=lambda x: x.left)

                columns.sort(key=lambda x: x.left)
        sorted_candidates = sorted(candidates.values(), key=lambda x: x['content_score'], reverse=True)
        self.files.sort(key=lambda x: x.fileName)
    for k in sorted(items, key=lambda k: name_for(k).lower()):
        for i, vl in enumerate(sorted(virt_libs, key=lambda x:(order.get(x, 0), sort_key(x)))):
        self.themes.sort(key=lambda x:sort_key(x.get('title', '')))

            self.themes.sort(key=lambda x:x.get('number', 0), reverse=True)

            self.themes.sort(key=lambda x:self.usage.get(x.get('name'), 0), reverse=True)
        self.order.sort(key=lambda x : sort_key(self.descriptions[x]))
        items.sort(key=lambda item: row_map[id(item)])
        for name in sorted(db.saved_search_names(), key=lambda x: primary_sort_key(x.strip())):
    def __init__(self, parent=None, completer_widget=None, sort_func=lambda x:b''):

        EditWithComplete.__init__(self, parent, sort_func=lambda x:b'', strip_completion_entries=strip_completion_entries)
        locs.sort(key=lambda x: self.stats[x], reverse=True)
            sorted_locations = sorted(self.locations, key=lambda name_loc: numeric_sort_key(name_loc[0]))
        for n, p in sorted(self.gui.istores.items(), key=lambda x: x[0].lower()):
        rules = sorted(rules, key=lambda k: k['ordinal'])

            for index in sorted(self.COLUMNS.keys(), key=lambda c: self.COLUMNS[c]['ordinal'])]

            for index in sorted(self.COLUMNS.keys(), key=lambda c: self.COLUMNS[c]['ordinal'])]
        all_authors.sort(key=lambda x : sort_key(x[1]))
        for dev, x in sorted(devs, key=lambda x:x[1][1], reverse=True):
        self.widgets = sorted(self.widgets, key=lambda x: x.TITLE)
def uniq(vals, kmap=lambda x:x):
        for lpath in sorted(lpaths, key=lambda x:numeric_sort_key(os.path.basename(x))):

        for lpath in sorted(self.importer.metadata['libraries'], key=lambda x:numeric_sort_key(os.path.basename(x))):
        all_authors.sort(key=lambda x : sort_key(x[1]))
    for k, v in sorted(vals, key=lambda k_v: sort_key(k_v[1])):
    display_plugins = sorted(display_plugins, key=lambda k: k.name)
        self.all_items_sorted = sorted(self.all_items, key=lambda x: sort_key(x.name))

        self.applied_items.sort(key=lambda x:sort_key(self.all_items[x].name))

        self.applied_items.sort(key=lambda x:sort_key(self.all_items[x].name))
        for item in sorted(deletes, key=lambda r: r.row(), reverse=True):

        row_indices.sort(key=lambda r: r.row(), reverse=True)

        row_indices.sort(key=lambda r: r.row(), reverse=True)
        self.column_map.sort(key=lambda x: col_idx(x))
            hcols.sort(key=lambda x: primary_sort_key(x[1]))

            pairs.sort(key=lambda x: x[1])
    num_of_pages = property(fget=lambda self: len(self.pages))

    num_of_pages = property(fget=lambda self: sum(self.chapter_layout or ()))
    has_content = property(fget=lambda self: self.peek_index < len(self.lines)-1)
        self.fields.sort(key=lambda x:self.descs.get(x, x))

            items.sort(key=lambda k_v: sort_key(k_v[1]))
        return self.db.new_api.author_sort_from_authors(authors, key_func=lambda x: x)
        colmap.sort(key=lambda x: positions[x])
        categories.sort(key=lambda x: category_map[x])

            plugins.sort(key=lambda x: x.name_order)
        items.sort(key=lambda x:sort_key(x[1]))

        items.sort(key=lambda x: x[1].lower())
        self.fields.sort(key=lambda x:self.descs.get(x, x))
        self.devices.sort(key=lambda x: x.lower())

            for d in sorted(self.devices + self.disabled_devices, key=lambda x:x.lower()):
            plugins.sort(key=lambda x: x.name.lower())
        for name in sorted(options, key=lambda n: options[n].shortdoc.lower()):

        items.sort(key=lambda x: primary_sort_key(x[0]))
        images.sort(key=lambda x:sort_key(x['name']))
        indices = sorted(indices, key=lambda i: i.row(), reverse=delta > 0)
        self.matches.sort(key=lambda x: sort_key(str(self.data_as_text(x, col))), reverse=descending)
        for i, x in enumerate(sorted(self.gui.istores.keys(), key=lambda x: x.lower())):
            ans = sorted(ans, key=lambda x:sort_map.get(x, -1))

        items = sorted(self.selectedItems(), key=lambda x:sort_map.get(id(x), -1))
        self.root_item.children.sort(key=lambda x: self.row_map.index(x.category_key))

        self.row_map = sorted(self.row_map, key=lambda x: order.get(x, defvalue))
    def __init__(self, settings=None, dispatch_on_main_thread=lambda f: f()):
    def __init__(self, settings=None, dispatch_on_main_thread=lambda f: f()):
    def __init__(self, settings=None, dispatch_on_main_thread=lambda f: f()):

    def speak_marked_text(self, marked_text, callback=lambda ev: None):
            editor.get_raw_data(), decoder=lambda x: x.decode('utf-8'),
        for err in sorted(errors, key=lambda e:(100 - e.level, e.name)):
        names = sorted(names, key=lambda x: current_order.get(x, -1))

            names = sorted(names, key=lambda x: current_order.get(x, -1))

        names = sorted(names, key=lambda x: order_map.get(x, -1))

        for c in sorted(removals, key=lambda x:x.parent().indexOfChild(x), reverse=True):

            ans = list(sorted(ans, key=lambda idx:idx.row()))
        for key, human in sorted(iteritems(choices), key=lambda key_human: key_human[1] or key_human[0]):

            items.sort(key=lambda x:order_map.get(x, limit))

        self('editor_font_family', widget=fc, getter=attrgetter('font_family'), setter=lambda x, val: setattr(x, 'font_family', val))

        for key, human in sorted(iteritems(choices), key=lambda key_human1: key_human1[1] or key_human1[0]):

        for key, ac in sorted(iteritems(all_items), key=lambda k_ac: str(k_ac[1].text())):
    root = parse(raw, decoder=lambda x:x.decode('utf-8'), line_numbers=True, linenumber_attribute='data-lnum')
        for dic in sorted(dictionaries.all_user_dictionaries, key=lambda d:sort_key(d.name)):

        for word, lang in sorted(d.words, key=lambda x:sort_key(x[0])):

        for lc in sorted(languages, key=lambda x:sort_key(calibre_langcode_to_name(x))):

            for countrycode in sorted(languages[lc], key=lambda x: country_map()['names'].get(x, x)):

                for dictionary in sorted(languages[lc][countrycode], key=lambda d:(d.name or '')):

        for dic in sorted(dictionaries.active_user_dictionaries, key=lambda x:sort_key(x.name)):
        for name in sorted(actions, key=lambda x:sort_key(actions[x].text())):

            for ac in sorted(self.plugin_menu_actions, key=lambda x:sort_key(str(x.text()))):
                root = parse(raw, decoder=lambda x:x.decode('utf-8'))
            ac.sort(key=lambda text_frag: numeric_sort_key(text_frag[0] or text_frag[1]))

        all_types.sort(key=lambda x: sort_key(x[1]))
    def __init__(self, result_callback=lambda x:x, worker_entry_point='main'):
            for name, other_name in sorted(iteritems(changed_names), key=lambda x:numeric_sort_key(x[0])):

        for name, new_name in sorted(iteritems(renamed_names), key=lambda x:numeric_sort_key(x[0])):
        for top, bottom, kind in sorted(lines, key=lambda t_b_k:{'replace':0}.get(t_b_k[2], 1)):

        for kind, path, aa in sorted(lines, key=lambda x:{'replace':0}.get(x[0], 1)):
    remove = sorted(instances, key=lambda x: x['atime'], reverse=True)[1:]
        for key in sorted(ucs, key=lambda x: primary_sort_key(ucs[x]['name'])):

        for key in sorted(self.default_color_schemes, key=lambda x: primary_sort_key(self.default_color_schemes[x]['name'])):

        return sorted(actions, key=lambda name: primary_sort_key(getattr(aa, name).text) if name else primary_sort_key(''))
    return sorted(ans, key=lambda x: x.name)

        items.sort(key=lambda x: x[1])
                ans.sort(key=lambda x:x.lower())

                ans.sort(key=lambda x: x.lower())
        ans.sort(key=lambda x: self.series_index(x, True))
            categories['formats'].sort(key=lambda x: x.count, reverse=True)

            categories['formats'].sort(key=lambda x:x.name)

            categories['identifiers'].sort(key=lambda x: x.count, reverse=True)

            categories['identifiers'].sort(key=lambda x:x.name)

                    sorted(items, key=lambda x: x.count, reverse=True)

                    sorted(items, key=lambda x: sort_key(x.sort))

                    sorted(items, key=lambda x:x.avg_rating, reverse=True)
                locales.sort(key=lambda x: (0, x) if x == locale else (1, x))

def uniq(vals, kmap=lambda x:x):
            self.books_by_title = sorted(self.books_to_catalog, key=lambda x: sort_key(x['title_sort'].upper()))

            self.books_by_date_range = sorted(nspt, key=lambda x: (x['timestamp'], x['timestamp']), reverse=True)

        self.books_by_series = sorted(self.books_by_series, key=lambda x: sort_key(self._kf_books_by_series_sorter(x)))

            nspt = sorted(nspt, key=lambda x: sort_key(x['title_sort'].upper()))

            titles = sorted(titles, key=lambda x: (self.generate_sort_title(x), self.generate_sort_title(x)))
    for k in sorted(exact_matches, key=lambda x: (1, None) if x is None else (0, x)):

        for d in sorted(collection, key=lambda d: d.name or ''):
        for category in sorted(categories, key=lambda x: sort_key(getter(x))):

        ans.sort(key=lambda x: sort_key(x['name']))
    def get_valid(prompt, invalidq=lambda x: None):
    scats = sorted(categories, key=lambda x: order.get(x, defvalue))
        for library_id, library_name in sorted(iteritems(request_context.library_map), key=lambda item: sort_key(item[1])):

    for category in sorted(categories, key=lambda x: sort_key(getter(x))):
def digest(un, pw, nonce=None, uri=None, method='GET', nc=1, qop='auth', realm=REALM, cnonce=None, algorithm='MD5', body=b'', modify=lambda x:None):
        u('a:url(  "(/*)"  )', 'a:url(  "(/*)"  )', url_callback=lambda x: x)
        def makeroute(route, func=lambda c,d:None, **kwargs):
def partition_by_first_letter(items, reverse=False, key=lambda x:x):

    items = sorted(items, key=lambda x:sort_key(key(x)), reverse=reverse)
            data = sorted(fmt_data.items(), key=lambda x:x[1]['mtime'], reverse=True)
def get_items_from_dir(basedir, acceptq=lambda x: True):
    answers.sort(key=lambda x: int(getattr(x, 'preference', sys.maxsize)))
        # keys.sort(key=lambda x:order.get(x, 1000))
    def add_dir(self, path, prefix='', simple_filter=lambda x:False):
    def __init__(self, description, done=lambda x: x):
    ans.sort(key=lambda d:sort_key(d.get('Name')))
        bmps = list(sorted(self.bitmaps, key=lambda x: len(x)))
        bmps = list(sorted(self.bitmaps, key=lambda x: len(x)))
    def __init__(self, get_article_url=lambda item: item.get('link', None),
        index.sort(key=lambda x: weights[x])
                        for atag in soup.findAll('a', href=lambda x: x and x.startswith('/')):
            css2 = sorted(css2, key=lambda x:{'margin':0}.get(x[0], 1))
    Audit(on_task_error=lambda line, *_: ctx.obj.echo(line)).run(files)
    for old_key in reversed(sorted(source, key=lambda x: len(x))):
@memoize(maxsize=1000, keyfun=lambda a, _: a[0])
    @patch('random.randrange', side_effect=lambda i: i - 1)

    @patch('random.randrange', side_effect=lambda i: i - 2)

    @patch('random.randrange', side_effect=lambda i: i - 2)
    return sorted(FQDNs, key=lambda fqdn: fqdn.split('.')[::-1][1:])
        for _, achalls in sorted(problems.items(), key=lambda item: item[0]):
        zones.sort(key=lambda z: len(z[0]), reverse=True)
        for i, elem in enumerate(self.statements.iterate(match=lambda x: 'sentence' in x)):
            return max(wildcards, key=lambda x: len(x['name']))['vhost']

        return sorted(matches, key=lambda x: x['rank'])

        return sorted(matches, key=lambda x: x['rank'])
        outputs.sort(key=lambda x: x[0])
            set(global_names), key=lambda name: name_to_global_ranks[name])
    for w, c in sorted(counts.items(), key=lambda x: (-x[1], x[0])):
    items[:] = sorted(items, key=lambda item: item.location)
    return sorted(found_array, key=lambda match: match and match[2][0])
                for version, count in sorted(self.versions.items(), key=lambda kv: kv[1], reverse=True):
                hashes.sort(key=lambda x: x[0])
    valid_spendable_coins.sort(reverse=True, key=lambda r: r.amount)
            original_private_keys.sort(key=lambda e: str(e[0]))

            post_migration_private_keys.sort(key=lambda e: str(e[0]))
        spendable.sort(reverse=True, key=lambda record: record.coin.amount)
        unspent.sort(key=lambda r: r.confirmed_block_height)
        spendable.sort(reverse=True, key=lambda record: record.coin.amount)
    for coin in sorted(spent, key=lambda _: _.name()):

    for coin in sorted(created, key=lambda _: _.name()):

        for coin in sorted(ephemeral, key=lambda _: _.name()):

        for announcement, hashed in sorted(created_coin_announcement_pairs, key=lambda _: _[-1]):

        for announcement, hashed in sorted(created_puzzle_announcement_pairs, key=lambda _: _[-1]):
                    for required_iters, proof_of_space in sorted(qualified_proofs, key=lambda t: t[0]):

                    for required_iters, proof_of_space in sorted(qualified_proofs, key=lambda t: t[0]):
            p2_singleton_coin = sorted(p2_singleton_coin, key=lambda x: x.coin.amount)[0].coin
            harvester_dict["plots"] = sorted(harvester_dict["plots"], key=lambda item: item["filename"])

            harvester_plots = sorted(harvester_plots, key=lambda item: item["filename"])
            sorted(new_dict.items(), key=lambda x: x[1], reverse=True)
        return sorted(listtosort, key=lambda x: hashes[x])
        possible_lens.sort(key=lambda i: i.p_value)
        ret.sort(key=lambda x: x.priority(), reverse=True)

                chunk.sort(key=lambda i: i.score)
        insertions = sorted(insertions, key=lambda e: e[0])

        groups = _group_until_different(insertions, key=lambda e: e[0], val=lambda e: e[1])

            preferred_exponent_index=max(range(len(labels)), key=lambda i: label_map[labels[i]]),

def _group_until_different(items: Iterable[_TIn], key: Callable[[_TIn], _TKey], val=lambda e: e):
        return sorted(self.operations, key=lambda op: op.qubits) == sorted(

            sorted(self.operations, key=lambda op: op.qubits),

            sorted(other.operations, key=lambda op: op.qubits),

        return hash((Moment, tuple(sorted(self.operations, key=lambda op: op.qubits))))
            next_ops_sorted = sorted(next_ops_list, key=lambda e: str(e.qubits))
    compiler_mock = MagicMock(side_effect=lambda circuit: circuit)

    compiler_mock = MagicMock(side_effect=lambda circuit: circuit)

    compiler_mock = MagicMock(side_effect=lambda circuit: circuit)

    router_mock = MagicMock(side_effect=lambda circuit, network: ccr.SwapNetwork(circuit, {}))

    compiler_mock = MagicMock(side_effect=lambda circuit: circuit)
            duration_equality = sorted(self._gate_durations.items(), key=lambda x: repr(x[0]))

            duration_payload = sorted(self._gate_durations.items(), key=lambda x: repr(x[0]))
    return CellMaker(identifier, size=0, maker=lambda _: None)
    yield from _size_dependent_arithmetic_family(identifier_prefix, size_to_func=lambda _: func)
                ops.append(_HangingNode(func=lambda _, b: token.unary_action(b), weight=np.inf))
    return CellMaker(identifier, size=1, maker=lambda _: operation)
        identifier=identifier, size=gate.num_qubits(), maker=lambda args: gate.on(*args.qubits)
    a, b = max(((i, j) for i in range(4) for j in range(4)), key=lambda t: abs(matrix[t]))
    k = max(np.ndindex(*a.shape), key=lambda t: abs(b[t]))

    best_candidate = max(candidates, key=lambda c: np.linalg.norm(c, 2))
    assert cirq.measure_each(a, b, key_func=lambda e: e.name + '!') == [
        cirq.MergeSingleQubitGates(rewriter=lambda ops: cirq.H(ops[0].qubits[0])).optimize_circuit(
            else tuple(sorted(self.label_map.items(), key=lambda e: e[0])),
    assert cirq.decompose(cirq.SWAP(a, b), keep=lambda e: isinstance(e.gate, cirq.CNotPowGate)) == [

    assert cirq.decompose(cirq.SWAP(a, b), keep=lambda _: True) == [cirq.SWAP(a, b)]

    assert cirq.decompose(DecomposeGiven(cirq.SWAP(b, a)), keep=lambda _: True) == [cirq.SWAP(b, a)]

    assert cirq.decompose([[[cirq.SWAP(a, b)]]], keep=lambda _: True) == [cirq.SWAP(a, b)]

        _ = cirq.decompose(NoMethod(), keep=lambda _: False)

    assert cirq.decompose([], keep=lambda _: False) == []

    assert cirq.decompose(no_method, keep=lambda _: False, on_stuck_raise=None) == [no_method]

    assert cirq.decompose(no_method, keep=lambda _: False, on_stuck_raise=lambda _: None) == [

        _ = cirq.decompose(no_method, keep=lambda _: False, on_stuck_raise=TypeError('test'))
                k = max(np.ndindex(*u.shape), key=lambda t: abs(u[t]))
        flatten_expressions._ParamFlattener({'a': 1}, get_param_name=lambda expr: 'x')
    assert result.histogram(key='ab', fold_func=lambda e: None) == collections.Counter({None: 5})
        return self.multi_measurement_histogram(keys=[key], fold_func=lambda e: fold_func(e[0]))
    return min(solutions, key=lambda c: len(c))
        for op in sorted(current_moment.operations, key=lambda op: op.qubits):
            @alternative(requires='missing_alt', implementation=lambda self: None)

        @alternative(requires='alt', implementation=lambda self: None)
    linear_dict = cirq.LinearDict(terms, validator=lambda v: v in valid_vectors)

    linear_dict = cirq.LinearDict(terms, validator=lambda v: v in valid_vectors)
    circles = np.array(sorted(circles, key=lambda x: (x.fill, x.center[0], -x.center[1]))).reshape(

    rects = np.array(sorted(rects, key=lambda x: x.get_x()))
        parities = result.histogram(key='out', fold_func=lambda bits: np.sum(bits) % 2)
    out.valid_gates.extend(sorted(gate_specs, key=lambda s: s.WhichOneof('gate')))
        after = cg.optimized_for_xmon(before, qubit_map=lambda q: cirq.GridQubit(q.x, 0))
    for extra in sorted(package_extras, key=lambda x: x['key']):

    return sorted(output, key=lambda x: x[0])

    allowed_view_types.sort(key=lambda item: item[1])

    sorted_licenses = sorted(register.values(), key=lambda x: x.title)
    return sorted(result_list, key=lambda x: x["position"])

    return sorted(result_list, key=lambda x: x["key"])

    return sorted(result_list, key=lambda x: x["key"])
    followee_dicts.sort(key=lambda d: d['display_name'])
        for group in sorted(factories.Group.create_batch(22), key=lambda g: g["name"])
        resources = sorted(pkg.resources, key=lambda r: r.url)
        data["packages"].sort(key=lambda x: x["id"])
    rv.sort(key=lambda x: x[0])
            possible_names.sort(key=lambda x: -len(x[0]))  # group long options first
CONTEXT_SETTINGS = dict(token_normalize_func=lambda x: x.lower())
        with click.progressbar(range(3), item_show_func=lambda x: str(x)) as progress:
    cli = click.Command("cli", params=[param], callback=lambda a: a)
            repr(sorted(other_aspects, key=lambda a: a.__qualname__))))
    return sorted(occurences, key=lambda x: x[1])
def _sort_bears(bears, key=lambda x: x.name.lower(), reverse=False):
def group(iterable, key=lambda x: x):

    grouping = group(bears, key=lambda bear: (bear.section, bear.file_dict))
                          dict(generate_tasks=lambda self: tuple()))
    sorted_timing_info = sorted(_timing_info[0].items(), key=lambda kv: kv[1])
                        if max(xrange(len(how_long_statistic)), key=lambda x: how_long_statistic[x]) == len(TAG) - 1:
      if len(max(re.compile("\w+").findall(payload), key=lambda word: len(word))) >= 5000:  

        long_string = max(re.compile("\w+").findall(payload), key=lambda word: len(word))
                        if max(xrange(len(how_long_statistic)), key=lambda x: how_long_statistic[x]) == len(TAG) - 1:
    api = app.add_api(spec, resolver=lambda oid: (lambda foo: 'bar'))

    api = app.add_api('swagger.yaml', resolver=lambda oid: (lambda foo: 'bar'))

        app.add_api('swagger.yaml', resolver=lambda oid: (lambda foo: 'bar'))
    other_contributors = sorted(other_contributors, key=lambda c: c["name"].lower())
                setup_time.items(), key=lambda item: item[1].total_seconds()
                for app in sorted(apps, key=lambda app: app.name.lower())
        vevents.sort(key=lambda x: self.to_datetime(x.dtstart.value))
        return self.json(sorted(calendar_list, key=lambda x: cast(str, x["name"])))
            children=sorted(children, key=lambda c: c.title),
    client.zones.sort(key=lambda zone: zone["number"])
        for ent in sorted(hass.states.async_all(DOMAIN_ZONE), key=lambda ent: ent.name)
    candidates.sort(key=lambda key: bin(key[1]).count("1"))

                color_modes.sort(key=lambda mode: bin(mode).count("1"))
            history_list = sorted(history_list, key=lambda s: s.last_updated)
            hue = min(self._supported_hs.keys(), key=lambda x: abs(x - hass_hue))
            for garage in sorted(api_data, key=lambda garage: garage.garage_name):
    return dict(sorted(unsorted.items(), key=lambda item: item[1]))
    hass_entities.sort(key=lambda entry: entry.original_name or "")
        artists = sorted(artists, key=lambda k: k[ITEM_KEY_NAME])  # type: ignore[no-any-return]

        albums = sorted(albums, key=lambda k: k[ITEM_KEY_NAME])  # type: ignore[no-any-return]
        children.sort(key=lambda x: x.title.replace("The ", "", 1), reverse=False)
        return sorted(out, key=lambda out: out[1], reverse=True)
def _dump_filter(filter_dict, desc, func=lambda x: x):
        media.children.sort(key=lambda child: (child.can_play, child.title))
    source_names = sorted(source_name_id.keys(), key=lambda v: source_name_id[v])
        return dict(sorted(all_region_codes_swaped.items(), key=lambda ele: ele[1]))
            tags.sort(key=lambda tag: tag.name)
    ORDER_OLDEST_FIRST: lambda torrents: sorted(torrents, key=lambda t: t.addedDate),

    ORDER_WORST_RATIO_FIRST: lambda torrents: sorted(torrents, key=lambda t: t.ratio),
        sorted_by_most_targeted = sorted(matched, key=lambda item: -len(item))
        return sorted(data, key=lambda app: app["name"])
        for match in sorted(matches, key=lambda x: x.weight, reverse=True):

                sorted_matches = sorted(matches, key=lambda x: x.weight, reverse=True)
                            for cc in sorted(node.command_classes, key=lambda cc: cc.name)  # type: ignore[no-any-return]
                            for cc in sorted(node.command_classes, key=lambda cc: cc.name)  # type: ignore[no-any-return]
                            for cc in sorted(node.command_classes, key=lambda cc: cc.name)  # type: ignore[no-any-return]
    return sorted(found.values(), key=lambda a: a.entity_id)
            yaml.load(content, Loader=lambda stream: SafeLineLoader(stream, secrets))
        reqs[key] = sorted(reqs[key], key=lambda name: (len(name.split(".")), name))

    for pkg, requirements in sorted(reqs.items(), key=lambda item: item[0]):
    res.sort(key=lambda item: item.file)
    return sorted(manifests, key=lambda man: man["domain"])
    for integration in sorted(integrations, key=lambda itg: itg.domain):
        hass, MockModule("test", async_setup_entry=lambda *args: mock_coro(True))
    mock_integration(hass, MockModule("comp", setup=lambda hass, config: False))

        hass, MockModule("disabled_component", setup=lambda hass, config: None)

        MockModule("disabled_component", setup=lambda hass, config: False),

        hass, MockModule("disabled_component", setup=lambda hass, config: True)
        hass, MockConfig(should_expose=lambda *_: False), State("light.kitchen", "on")

        MockConfig(should_expose=lambda *_: True, should_2fa=lambda *_: False),
        sorted(devices, key=lambda d: d["id"]),

        sorted(DEMO_DEVICES, key=lambda d: d["id"]),
    config = MockConfig(should_expose=lambda _: True, entity_config={})
    zones = sorted(json, key=lambda entry: entry["entity_id"])
        sorted(history.get_states(hass, future), key=lambda state: state.entity_id),
        set_state=AsyncMock(side_effect=lambda turn: {"ison": turn == "on"}),
    assert sorted(msg["result"], key=lambda manifest: manifest["domain"]) == [
    cluster_infos = sorted(msg["result"], key=lambda k: k[ID])
            registries.MatchRule(channel_names="on_off", models=lambda x: x == MODEL),

            registries.MatchRule(channel_names="on_off", models=lambda x: x != MODEL),
    assert [state2, state3] == sorted(states, key=lambda state: state.entity_id)
    test_thread = ThreadWithException(target=lambda *_: None)
        videos.sort(key=lambda video: video.resolution, reverse=True)
            possible_quotes.sort(key=lambda q: q[0] == escaped_string[-1])
                    modes[char] = max(items, key=lambda x: x[1])
            members.sort(key=lambda t: (t[1], t[0]))

            members.sort(key=lambda t: t[0])

        members.sort(key=lambda t: (t[1], t[0]))

        members.sort(key=lambda t: t[0])
    results.sort(key=lambda pair: pair[0])
        directories.sort(key=lambda a: a.name)
        L.sort(key=lambda item:item[1].index)
        helpSources.sort(key=lambda x: x[2])
        theme_names.sort(key=lambda x: self.theme_elements[x][1])
            drop.add_command(label=lbl, command=lambda dex=dex:text.yview(dex))
        list.sort(key=lambda a: a.lower())
        cookies.sort(key=lambda a: len(a.path), reverse=True)
        self.assertIsNone(start(is_char_in_string=lambda index: True))

        eq(start(is_char_in_string=lambda index: index > pos), pos)

        eq(start(is_char_in_string=lambda index: index >= pos), pos0)

        eq(start(is_char_in_string=lambda index: index < pos), None)

        eq(start(is_char_in_string=lambda index: index > pos), pos)

        eq(start(is_char_in_string=lambda index: index >= pos), pos0)

        eq(start(is_char_in_string=lambda index: index < pos), pos)
        texts.sort(key=lambda text: canvas.bbox(text)[1])
    def __init__(self, spec, adapter=lambda spec: spec.loader):
        L.sort(key=lambda x: x if isinstance(x, tuple) else ())
        self._test_recursive_list(REX_six, aslist=lambda x: x.items)

        self._test_recursive_tuple_and_list(REX_six, aslist=lambda x: x.items)

        self._test_recursive_dict(REX_seven, asdict=lambda x: x.table)

        self._test_recursive_tuple_and_dict(REX_seven, asdict=lambda x: x.table)

        self._test_recursive_dict_key(REX_seven, asdict=lambda x: x.table)

        self._test_recursive_tuple_and_dict_key(REX_seven, asdict=lambda x: x.table)
        data.sort(key=lambda r: r[1])
        self.assertEqual(data, sorted(copy, key=lambda x: -x))
        cf = self.fromstring(ini, optionxform=lambda opt: opt)
        (x, y), (z, t) = sorted(v.items(), key=lambda pair: pair[0].i)
        test_classes = sorted(set(test_classes), key=lambda cls: cls.__qualname__)
    >>> print(sorted(a.keys(), key=lambda x: (str(type(x)), x)))
        result.sort(key=lambda row: int(row[0]))
        values.sort(key=lambda item: item.name)

        result.sort(key=lambda item: item.name)
            fi = FileInput(inplace=1, openhook=lambda f, m: None)
        self.client.storbinary('stor', f, callback=lambda x: flag.append(None))

        self.client.storlines('stor foo', f, callback=lambda x: flag.append(None))
    for k, v in sorted(after.items(), key=lambda i: i[0]):
        L.sort(key=lambda x: (type(x).__name__, x))
            entries.sort(key=lambda entry: entry.name)
        self.assertRaises(TypeError, data.sort, key=lambda x,y: 0)

        data.sort(key=lambda t: t[0])   # sort on the random first field

        self.assertRaises(ZeroDivisionError, data.sort, key=lambda x: 1/x)

        copy2.sort(key=lambda x: x[0], reverse=True)
    >>> def f(*, x=lambda __debug__:0): pass
            t = threading.Thread(target=lambda : time.sleep(0.3))
        b = ET.TreeBuilder(pi_factory=lambda target, text: (len(target), text))
                zipfp.writepy(packagedir, filterfunc=lambda whatever: False)
        sorted_ = lambda l: sorted(l, key=lambda x: id(x))
        self.assertEqual(self.loads(s, object_pairs_hook=lambda x: x), p)
        cur = self.con.cursor(factory=lambda con: MyCursor(con))
        def foo3(bar: 'func'=lambda x: x) -> {1: 2}:
        transitions = sorted(map(zt_as_tuple, transitions), key=lambda x: x[0])
        mock_obj.mock_func = MagicMock(spec=lambda x: x)
        items = (item for item in items if filter(item, log=lambda msg: logger.log(1, msg)))
    items = sorted(analysis, key=lambda v: v.key)
            for name, short_name in sorted(ids, key=lambda x: x[1].lower()):
    table.sort(key=lambda values: -values[1])
                for literal, name in sorted(strings.items(), key=lambda x: x[1]):

                        for literal, name in sorted(strings.items(), key=lambda x: x[1]):
        for entry in sorted(os.scandir(pkgdir), key=lambda e: e.name):
    wordtail.sort(key=lambda a: a[0], reverse=True)
        laps_computers = sorted(laps_computers, key=lambda x: x[0])
        SAM = SAMHashes(self.output_filename + ".sam", bootKey, isRemote=None, perSecretCallback=lambda secret: self.logger.highlight(secret))

        LSA = LSASecrets(self.output_filename + ".security", bootKey, None, isRemote=None, perSecretCallback=lambda secretType, secret: self.logger.highlight(secret))
            SAM = SAMHashes(SAMFileName, self.bootkey, isRemote=True, perSecretCallback=lambda secret: add_sam_hash(secret, host_id))
    node = pretend.stub(iter_markers=lambda x: [supported])

    node = pretend.stub(iter_markers=lambda x: [supported])
        pretend_key = pretend.stub(public_bytes=lambda x, y: data)

        pretend_key = pretend.stub(public_bytes=lambda x, y: data)
    height_weight_pairs.sort(key=lambda x: -x[1])
            path = min(full_results, key=lambda x: x[0])[1]

    path = min(full_results, key=lambda x: x[0])[1]

        best = min(known_contractions, key=lambda x: x[0])
def _fix_sequence_arg(arg, ndim, name, conv=lambda x: x):
        new_items = sorted(new_items, key=lambda x: x["layer_height"])
        items.sort(key=lambda i: (not i["hasRemoteConnection"], i["name"]))
        result.sort(key=lambda k: k["weight"])
        for self_extruder, other_extruder in zip(sorted(self._extruder_configurations, key=lambda x: x.position), sorted(other.extruderConfigurations, key=lambda x: x.position)):
        result_tuple_list = sorted(list(self._extruders.items()), key=lambda x: int(x[0]))
        nodes = sorted(nodes, key=lambda n: n["name"])
    extensions.sort(key=lambda ext: os.path.getsize(ext.sources[0]), reverse=True)
            bufvars.sort(key=lambda entry: entry.name)
        all_members.sort(key=lambda e: e.name)
            spam_locals.sort(key=lambda e: e.attrib['name'])
        L.sort(key=lambda x: (type(x).__name__, x))
            train_list.extend(sorted(train_items[u], key=lambda k: k[3]))

        ranked_list[u] = sorted(item_scores, key=lambda t: t[1], reverse=True)
        self.token_freqs = sorted(counter.items(), key=lambda x: x[0])

        self.token_freqs.sort(key=lambda x: x[1], reverse=True)
    sorted_by_sugar = sorted(cereals, key=lambda cereal: cereal["sugars"])
    sorted_by_sugar = sorted(cereals, key=lambda cereal: cereal["sugars"])
    sorted_cereals = list(sorted(cereals, key=lambda cereal: cereal["calories"]))

    sorted_cereals = list(sorted(cereals, key=lambda cereal: cereal["protein"]))
    sorted_cereals = sorted(cereals, key=lambda cereal: cereal["calories"])
    sorted_cereals = sorted(cereals, key=lambda cereal: cereal["calories"])
    sorted_cereals = sorted(cereals, key=lambda cereal: cereal["calories"])
    sorted_cereals = sorted(cereals, key=lambda cereal: cereal["calories"])
    assets_defs = sorted(assets_defs, key=lambda ad: (sorted((ak for ak in ad.keys))))
                    sorted(assets_by_partitions_def.items(), key=lambda item: repr(item[0]))
        return ResourceDefinition(resource_fn=lambda _init_context: value, description=description)
        config_mapping = ConfigMapping(config_fn=lambda _: config, config_schema=None)
                for name, inp in sorted(self.ins.items(), key=lambda input: input[0])
    def reindex(self, print_fn=lambda _: None):
        return IOManagerDefinition(resource_fn=lambda _init_context: value, description=description)
        event_records = sorted(event_records, key=lambda x: x.storage_id, reverse=not ascending)

            for record in sorted(asset_records, key=lambda x: x.timestamp, reverse=True)

        for record in sorted(asset_records, key=lambda x: x.timestamp, reverse=True):
        return sorted(list([(k, v) for k, v in result.items()]), key=lambda x: x[0])
        return sorted([(k, v) for k, v in all_tags.items()], key=lambda x: x[0])

        return self._slice(backfills[::-1], cursor, limit, key_fn=lambda _: _.backfill_id)
        asset_keys = [AssetKey.from_db_string(row[1]) for row in sorted(rows, key=lambda x: x[1])]

        asset_keys = [AssetKey.from_db_string(row[1]) for row in sorted(rows, key=lambda x: x[1])]
    @composite_solid(config_schema=int, config_fn=lambda _: 4)
    @composite_solid(config_schema={}, config_fn=lambda _cfg: {"return_int": {"config": 35}})
        ResourceDefinition(resource_fn=lambda _: None, config_schema="wut")
    @composite_solid(config_fn=lambda _cfg: {})
    foo_system_sensor = SensorDefinition(name="foo", evaluation_fn=lambda x: x)
                dependencies=sorted(node.dependencies, key=lambda d: d.upstream_asset_key),

                depended_by=sorted(node.depended_by, key=lambda d: d.downstream_asset_key),

                dependencies=sorted(node.dependencies, key=lambda d: d.upstream_asset_key),

                depended_by=sorted(node.depended_by, key=lambda d: d.downstream_asset_key),
    AlwaysSucceedsFoo = DagsterType(name="Foo", type_check_fn=lambda _, _val: True)

    AlwaysFailsFoo = DagsterType(name="Foo", type_check_fn=lambda _, _val: False)
ReturnBoolType = DagsterType(name="ReturnBoolType", type_check_fn=lambda _, _val: True)

    FalsyType = DagsterType(name="FalsyType", type_check_fn=lambda _, _val: False)
        output_defs=[OutputDefinition(name="output2", asset_key=lambda _: AssetKey("table2"))],
    @solid(output_defs=[OutputDefinition(name="output2", asset_key=lambda _: AssetKey("table2"))])
        step_stats = sorted(instance.get_run_step_stats(result.run_id), key=lambda x: x.end_time)
    my_dagster_type = DagsterType(name="foo", type_check_fn=lambda _, _a: True)

    my_dagster_type = DagsterType(name="foo", type_check_fn=lambda _, _a: True)
        step_stats = sorted(storage.get_step_stats_for_run(result.run_id), key=lambda x: x.end_time)
    my_dagster_type = DagsterType(name="aaaa", type_check_fn=lambda _, _a: True)
def construct_structured_logger(constructor=lambda x: x):
        for definition in sorted(definitions, key=lambda d: d.name)
            for preset in sorted(self._external_pipeline.active_presets, key=lambda item: item.name)
            for pipeline in sorted(repo.get_all_external_pipelines(), key=lambda p: p.name):
        return [OrderedDict(sorted(x.items(), key=lambda x: x[0])) for x in csv.DictReader(fd)]
        result.data["assetsOrError"]["nodes"].sort(key=lambda e: e["key"]["path"][0])

            assets_live_info = sorted(assets_live_info, key=lambda res: res["assetKey"]["path"])

            assets_live_info = sorted(assets_live_info, key=lambda res: res["assetKey"]["path"])
        return [OrderedDict(sorted(x.items(), key=lambda x: x[0])) for x in csv.DictReader(fd)]
        sorted_items = sorted(partitions[0]["tagsOrError"]["results"], key=lambda item: item["key"])
    dag_roots = sorted(dag.roots, key=lambda x: x.task_id)

        task_upstream_list = sorted(task.upstream_list, key=lambda x: x.task_id)

    task_downstream_list = sorted(task.downstream_list, key=lambda x: x.task_id)
    sorted_keys = [obj["Key"] for obj in sorted(contents, key=lambda x: x["LastModified"])]
                step_results.items(), key=lambda x: priority_for_key(x[0])
        [make_readonly_value(val) for val in list1], key=lambda val: val.__hash__()

    ) == sorted([make_readonly_value(val) for val in list2], key=lambda val: val.__hash__())
            metadata_entries=sorted(metadata, key=lambda x: x.label),
        mapper1 = InvertibleMapper(fn=lambda x: x + 10, inverse_fn=lambda x: x - 10)

        mapper2 = InvertibleMapper(fn=lambda x: x * 10, inverse_fn=lambda x: x / 10)

        mapper_inv = InvertibleMapper(fn=lambda x: x + 2, inverse_fn=lambda x: x - 2)
    >>> with dask.annotate(priority=lambda k: k[1]*nblocks[1] + k[2]):
        type(max(arrays, key=lambda x: getattr(x, "__array_priority__", 0)))

            mode, count = max(chunk_frequencies.items(), key=lambda kv: kv[1])

    rec = _Recurser(recurse_if=lambda x: type(x) is list)

    elem_ndim = rec.map_reduce(arrays, f_map=lambda xi: xi.ndim, f_reduce=max)

        type(max(seq_metas, key=lambda x: getattr(x, "__array_priority__", 0)))
        i = sorted(enumerate(args), key=lambda v: (v[1].ndim, -v[0]))[-1][0]
    x = max([a, b], key=lambda x: x.__array_priority__)
        >>> b.distinct(key=lambda x: x['name']).compute()

        ...     return max((t, x), key=lambda x: x[1])
    expected = list(unique(seq, key=lambda x: x["a"]))

    assert_eq(bag.distinct(key=lambda x: x["a"]), expected)

    c = b.topk(4, key=lambda x: -x)

    c2 = b.topk(4, key=lambda x: -x, split_every=2)

    assert list(b.topk(2, key=lambda a, b: b)) == [(1, 10), (2, 9)]
        >>> res = ddf.x.reduction(count_greater, aggregate=lambda x: x.sum(),

        >>> res = ddf.x.reduction(sum_and_count, aggregate=lambda x: x.sum())
    n = g[x.columns].count().rename(columns=lambda c: (c, "-count"))

    x2 = g2.sum().rename(columns=lambda c: (c, "-x2"))

    x2 = g[g.columns[nc // 3 : 2 * nc // 3]].rename(columns=lambda c: c[0])

    n = g[g.columns[-nc // 3 :]].rename(columns=lambda c: c[0])

    n = g[x.columns].count().rename(columns=lambda c: f"{c}-count")

                dict(column=input_column, func=lambda s: s.apply(list)),
                f"- {c}\n  {e!r}" for c, e in sorted(errors, key=lambda x: str(x[0]))

        bad_dtypes = sorted(bad_dtypes, key=lambda x: str(x[0]))
            a.to_csv(fn, name_function=lambda x: x, index=False, single_file=True)
        a.to_hdf(fn, "/data_*", name_function=lambda i: "a" * (i + 1))

        a.to_hdf(fn, "/data", name_function=lambda i: "a" * (i + 1))
        df.to_parquet(fn, name_function=lambda x: "whatever.parquet", engine=engine)
    df.assign(B=lambda df: df["A"], C=lambda df: df.A + df.B)

    ddf.assign(B=lambda df: df["A"], C=lambda df: df.A + df.B)

    b = df.assign(B=lambda x: x.A.shift())

        [ddf.x], chunk=lambda x: pd.Series([x.sum()]), aggregate=lambda x: x.sum()

    res = ddf.reduction(sum_and_count, aggregate=lambda x: x.groupby(level=0).sum())
    prof_data = sorted(prof.results, key=lambda d: d.key)
    with Callback(pretask=lambda key, *args: keys.append(key)):

    with Callback(pretask=lambda key, *args: keys.append(key)):
    c = delayed(max)([[a, 10], [b, 20]], key=lambda x: x[0])[1]
    with Callback(pretask=lambda key, *args: keys.append(key)):
    first_store = min(stores, key=lambda k: o[k])
    resorted = sorted(ranked, key=lambda trip: trip[1][0])
        ps.sort(key=lambda p: p["name"])
        tables.sort(key=lambda t: (t["hidden"], t["name"]))
    pks.sort(key=lambda column: column.is_pk)
    assert EXPECTED_PLUGINS == sorted(response.json, key=lambda p: p["name"])
    databases.sort(key=lambda d: d["name"])
    assert sorted(data2["suggested_facets"], key=lambda f: f["name"]) == [
    expected.sort(key=lambda row: -row["sortable"])
        population.sort(key=lambda ind: ind.fitness, reverse=True)

        population.sort(key=lambda ind: ind.fitness, reverse=True)
    def _genpop(n, pickfrom=[], acceptfunc=lambda s: True, producesizes=False):

        sortednatural = sorted(naturalpop, key=lambda ind: ind.fitness)
        crowd.sort(key=lambda element: element[0][i])
stats = tools.Statistics(key=lambda ind: ind.fitness.values)

stats = tools.Statistics(key=lambda ind: ind.fitness.values)
stats_fit = tools.Statistics(key=lambda ind: ind.fitness.values)
        best = max(population, key=lambda ind: ind.fitness)
    # pop.sort(key=lambda x: x.fitness.values)
    # pop.sort(key=lambda x: x.fitness.values)
    price_stats = tools.Statistics(key=lambda ind: ind.fitness.values[0])

    time_stats = tools.Statistics(key=lambda ind: ind.fitness.values[1])
        sorted_swarm = sorted(swarm, key=lambda ind: ind.bestfit, reverse=True)
        sorted(data.items(), key=lambda x: x[1]["unique_id"]),
        blocked_dupes = itertools.groupby(self.bipartite_dupes, key=lambda x: x[0][0])
    labels_preds = sorted(labels_preds,key=lambda x:x[1])  #         sorted_matches = sorted(matches, key=lambda m: (m[0], m[1]))

        counts = [(*key, len(list(group))) for key, group in groupby(sorted_matches, key=lambda m: (m[0], m[1]))]

            [max(list(group), key=lambda g: g[2]) for key, group in groupby(counts, key=lambda count: count[0])],
    output_list.sort(key=lambda f: (f["pipeline_name"], f["dataset_name"], f["file_name"]))
    def forward(self, g, labels, mask=None, post_step=lambda y: y.clamp_(0., 1.)):

                smoothed_error = self.prop1(g, error, post_step=lambda x: x.clamp_(-1., 1.))
        _action_list = sorted(action_list, key=lambda x: x[1])
        sort_kv = sorted(self.wf.items(), key=lambda x:x[1], reverse=True)
    neighbors = sorted(neighbors, key=lambda x:x['mol'].GetNumAtoms(), reverse=True)
        neighbors = sorted(neighbors, key=lambda x: x['mol'].GetNumAtoms(), reverse=True)
    neighbors = sorted(neighbors, key=lambda x:x['mol'].GetNumAtoms(), reverse=True)
        neighbors = sorted(neighbors, key=lambda x: x['mol'].GetNumAtoms(), reverse=True)
    item_score = sorted(item_score.items(), key=lambda kv: kv[1])
    dataloader = GraphDataLoader(subg_iter, batch_size=1, collate_fn=lambda x: x[0])
        model.load_state_dict(th.load(f, map_location=lambda storage, loc: storage))
                  GraphConv(self.hidden1_dim, self.hidden2_dim, activation=lambda x: x, allow_zero_in_degree=True),

                  GraphConv(self.hidden1_dim, self.hidden2_dim, activation=lambda x: x, allow_zero_in_degree=True)]
            sorted_with_key = sorted(zip(frames, order), key=lambda x: x[1])
        res.sort(key=lambda rel: - self.freq(rel))

            edge_list = sorted(edge_list, key=lambda x: (x[1], x[0], x[2]))
        id_ranges.sort(key=lambda a: a[0, 0])
    a = sp.random(n, n, 3 / n, data_rvs=lambda n: np.ones(n))

    a = sp.random(n, n, 3 / n, data_rvs=lambda n: np.ones(n))
    a = sp.random(n, n, p, data_rvs=lambda n: np.ones(n))
        g.apply_nodes(func=lambda nodes: {'feat': F.ones((1, in_feats)) * 10}, v=0)

        g.apply_edges(func=lambda edges: {'feat': F.ones((1, in_feats)) * 10}, edges=0)
ntypes.sort(key=lambda e: e[1])

etypes.sort(key=lambda e: e[1])
        channels.sort(key=lambda c: c.position)

        channels.sort(key=lambda c: (c.position, c.id))
        ret.sort(key=lambda c: (c.position, c.id))

        ret.sort(key=lambda c: (c.position, c.id))

        ret.sort(key=lambda c: (c.position, c.id))
        r.sort(key=lambda c: (c.position, c.id))

        r.sort(key=lambda c: (c.position, c.id))

        r.sort(key=lambda c: (c.position, c.id))

        r.sort(key=lambda c: (c.position, c.id))

        r.sort(key=lambda c: (c.position, c.id))

            channels.sort(key=lambda c: (c._sorting_bucket, c.position, c.id))
        @app_commands.checks.cooldown(1, 5.0, key=lambda i: (i.guild_id, i.user.id))
    values = sorted(parameters, key=lambda a: a.required, reverse=True)
        keys = sorted(keys, key=lambda t: len(t), reverse=True)
            commands = sorted(commands, key=lambda c: c.name) if self.sort_commands else list(commands)

            commands = sorted(commands, key=lambda c: c.name) if self.sort_commands else list(commands)
        async def wave(ctx, to: discord.User = commands.parameter(default=lambda ctx: ctx.author)):
            table.append(class_results_to_node(label, sorted(subitems, key=lambda c: c.label)))
        ctx.voice_client.play(source, after=lambda e: print(f'Player error: {e}') if e else None)

            ctx.voice_client.play(player, after=lambda e: print(f'Player error: {e}') if e else None)

            ctx.voice_client.play(player, after=lambda e: print(f'Player error: {e}') if e else None)
            return sorted(candidates, key=lambda ac: -len(ac.name))[0]
        app_list = sorted(app_dict.values(), key=lambda x: x["name"].lower())

            app["models"].sort(key=lambda x: x["name"])
                    sorted(instances, key=lambda obj: self.get_content_type(obj).pk),
def module_to_dict(module, omittable=lambda k: k.startswith("_") or not k.isupper()):
        sorted_imports = sorted(imports, key=lambda i: i.split()[1])
            set(flatten_bases(model)), key=lambda x: model.__mro__.index(x)

        sorted_managers = sorted(self.managers, key=lambda v: v[1].creation_counter)
    def __init__(self, *, coerce=lambda val: val, empty_value="", **kwargs):

    def __init__(self, *, coerce=lambda val: val, **kwargs):
        sorted_files = sorted(watched_files, key=lambda p: p.parent)

        for directory, group in itertools.groupby(sorted_files, key=lambda p: p.parent):
        class_tests = shuffler.shuffle(class_tests, key=lambda test: test.id())
    result.sort(key=lambda k: k[1], reverse=True)
            app["models"].sort(key=lambda x: x["name"], reverse=True)
        sorted_publishers = sorted(publishers, key=lambda x: x.name)
        return MagicMock(side_effect=lambda execute, *args: execute(*args))
@condition(last_modified_func=lambda r: LAST_MODIFIED)

@condition(etag_func=lambda r: ETAG)

@condition(etag_func=lambda r: ETAG.strip('"'))

@condition(etag_func=lambda r: WEAK_ETAG)

@condition(etag_func=lambda r: None)
        data = sorted(json.loads(data), key=lambda x: x["pk"])

        self.assertQuerysetEqual(Absolute.objects.all(), [1], transform=lambda o: o.pk)
        widget = CheckboxInput(check_test=lambda value: value.startswith("hello"))
        pl.sort(key=lambda x: (mid - x) ** 2)

        ul.sort(key=lambda x: (mid - x) ** 2)

        pl.sort(key=lambda x: (mid - x) ** 2)

        ul.sort(key=lambda x: (mid - x) ** 2)
        return sorted(self.houses.all(), key=lambda house: -house.rooms.count())[0]
        with mock.patch("builtins.input", side_effect=lambda _: "no"):
        hooks = sorted(hooks, key=lambda hook: hook[1])
        top_plugins_pks = [p[0].pk for p in sorted(top_plugins, key=lambda pair: pair[1].position)]

        top_plugins_pks = [p[0].pk for p in sorted(top_plugins, key=lambda pair: pair[1].position)]
        wizards = sorted(wizards, key=lambda e: getattr(e, 'weight'))

        wizards = sorted(wizards, key=lambda e: getattr(e, 'weight'))
        for result in sorted(results, key=lambda x: x.item.name):
            self._entries.items(), key=lambda e: getattr(e[1], 'weight'))]
        fields.sort(key=lambda x: x[1]._creation_counter)
                sorted(models, key=lambda k: k.tid), key=lambda k: k.tid):

                sorted(item, key=lambda x: x.created_time))
        self._errors.sort(key=lambda error: error.line_num)

        self._errors.sort(key=lambda error: error.line_num)
def split_buffer(stream, splitter=None, decoder=lambda a: a):
    return sorted(zip(prediction_model_factories, average_metric_scores), key=lambda x: x[1])[0][0]
    sample = interventional_samples(causal_model, dict(X2=lambda x: np.array(10)), observed_data).to_numpy()

    sample = interventional_samples(causal_model, dict(X2=lambda x: x + 10), observed_data).to_numpy()

    samples = interventional_samples(causal_model, dict(X2=lambda x: np.array(10)), num_samples_to_draw=10)

    samples = interventional_samples(causal_model, dict(X2=lambda x: x + 10), num_samples_to_draw=10)

                                                  dict(X0=lambda x: 0, X1=lambda x: '0'),

        interventional_samples(causal_model, dict(X0=lambda x: 10))

        interventional_samples(causal_model, dict(X0=lambda x: 10), observed_data=observed_data, num_samples_to_draw=100)

    sample = counterfactual_samples(causal_model, dict(X2=lambda x: 2), observed_data=observed_samples)

    sample = counterfactual_samples(causal_model, dict(X2=lambda x: 2), noise_data=noise_samples)

        counterfactual_samples(causal_model, dict(X0=lambda x: 10))

                               dict(X0=lambda x: 10),
    for page, mod, prio in sorted(pages, key=lambda t: -t[2]):
        ret_list.sort(key=lambda f: f["path"])
@mock.patch.object(ObjectDB, "path_to_oid", side_effect=lambda x: x)
        polys_list = [[p for p, _ in sorted(polys, key=lambda x: abs(optimal_num_chars - x[1]))]
    sep_list = sorted(sep_list, key=lambda x: x[0])

        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)

        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)

        horizontal_list = sorted(horizontal_list, key=lambda item: item[4])

            boxes = sorted(boxes, key=lambda item: item[0])

                    x_min = min(mbox, key=lambda x: x[0])[0]

                    x_max = max(mbox, key=lambda x: x[1])[1]

                    y_min = min(mbox, key=lambda x: x[2])[2]

                    y_max = max(mbox, key=lambda x: x[3])[3]

        image_list = sorted(image_list, key=lambda item: item[0][0][1]) # sort by vertical position
        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)

        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)[:beamWidth]

    sep_list = sorted(sep_list, key=lambda x: x[0])
    return jsn.dumps(obj, default=lambda o: None)
    l = sorted(l, key=lambda x: int(x.split('_')[1]))  # sort by forkpoint

    chains = sorted(chains, key=lambda x: x.get_chainwork(), reverse=True)
        self.show_xpub_dialog(xpub=xpub, run_next=lambda x: self.run('choose_keystore'))

        self.confirm_seed_dialog(run_next=f, seed=seed if self.config.get('debug_seed') else '', test=lambda x: x==seed)

            self.line_dialog(run_next=f, title=title, message=message, default='', test=lambda x: x==passphrase)
        addr = sorted(list(addr_to_ts), key=lambda a: addr_to_ts[a], reverse=True)[0]
    bkts = sorted(bkts, key=lambda bkt: bkt.value, reverse=True)

        winner = min(scored_candidates, key=lambda x: x.penalty)
    def verify_message_for_address(self, sig65: bytes, message: bytes, algo=lambda x: sha256d(msg_magic(x))) -> bool:
json_loads = lambda x: json.loads(x, parse_float=lambda x: str(Decimal(x)))
    sig = privkey.sign_message(msg, is_compressed=False, algo=lambda x:sha256(x).digest())
    start_node = attr.ib(type=bytes, kw_only=True, repr=lambda val: val.hex())

    end_node = attr.ib(type=bytes, kw_only=True, repr=lambda val: val.hex())

    short_channel_id = attr.ib(type=ShortChannelID, kw_only=True, repr=lambda val: str(val))

    node_features = attr.ib(type=int, kw_only=True, repr=lambda val: str(int(val)))  # note: for end node!

    short_channel_id = attr.ib(default=ShortChannelID(8), repr=lambda val: str(val))
    rated_configs.sort(key=lambda x: x.rating)
    offered_htlcs.sort(key=lambda htlc: htlc.cltv_expiry)

    received_htlcs.sort(key=lambda htlc: htlc.cltv_expiry)

    htlcs.sort(key=lambda x: x.htlc.cltv_expiry)

    payment_hash = attr.ib(type=bytes, kw_only=True, converter=hex_to_bytes, repr=lambda val: val.hex())
        out.sort(key=lambda x: (x.get('timestamp') or float("inf")))

            channels = sorted(channels, key=lambda chan: -chan.available_to_spend(REMOTE))
    srv_records = sorted(srv_records, key=lambda x: (x.priority, -x.weight))
        out.sort(key=lambda x:x.time)

        s = sorted(s, key=lambda o: o.value)

        for k in sorted(self.get_keystores(), key=lambda ks: ks.ready_to_sign(), reverse=True):

        out.sort(key=lambda x: x.time)

        out.sort(key=lambda x: x.time)
            item.bind(on_release=lambda option: dp.select(option.key))
            self.confirm_dialog(message=_('Wallet creation failed'), run_next=lambda x: self.app.on_wizard_aborted())
            ActionButtonOption(text=_('Backup'), func=lambda btn: self.export_backup()),

            ActionButtonOption(text=_('Close channel'), func=lambda btn: self.close(), enabled=ChanCloseOption.COOP_CLOSE in self.chan.get_close_options()),

            ActionButtonOption(text=_('Force-close'), func=lambda btn: self.force_close(), enabled=ChanCloseOption.LOCAL_FCLOSE in self.chan.get_close_options()),

            ActionButtonOption(text=_('Delete'), func=lambda btn: self.remove_channel(), enabled=self.can_be_deleted),

                options.append(ActionButtonOption(text=_("Freeze") + "\n(for sending)", func=lambda btn: self.freeze_for_sending()))

                options.append(ActionButtonOption(text=_("Unfreeze") + "\n(for sending)", func=lambda btn: self.freeze_for_sending()))

                options.append(ActionButtonOption(text=_("Freeze") + "\n(for receiving)", func=lambda btn: self.freeze_for_receiving()))

                options.append(ActionButtonOption(text=_("Unfreeze") + "\n(for receiving)", func=lambda btn: self.freeze_for_receiving()))
            ActionButtonOption(text=_('Sign'), func=lambda btn: self.do_sign(), enabled=self.can_sign),

            ActionButtonOption(text=_('Broadcast'), func=lambda btn: self.do_broadcast(), enabled=self.can_broadcast),

            ActionButtonOption(text=_('Bump fee'), func=lambda btn: self.do_rbf(), enabled=self.can_rbf),

            ActionButtonOption(text=_('Child pays\nfor parent'), func=lambda btn: self.do_cpfp(), enabled=(not self.can_rbf and self.can_cpfp)),

            ActionButtonOption(text=_('Cancel') + '\n(double-spend)', func=lambda btn: self.do_dscancel(), enabled=self.can_dscancel),

            ActionButtonOption(text=_('Remove'), func=lambda btn: self.remove_local_tx(), enabled=self.can_remove_tx),
        resolution = sorted(candidate_resolutions, key=lambda r: r.width() * r.height(), reverse=not is_ideal)[0]
        wizard.request_password(run_next=lambda pw, encrypt: self.on_password(wizard, pw, encrypt, k1, k2))
        for server, header in sorted(results.items(), key=lambda x: x[1].get('height')):
	return sorted(matching_vouchers, key=lambda x: x[0], reverse=True) if matching_vouchers else []
		for i, gle in enumerate(sorted(gl_entries, key=lambda gle: gle.account)):
		self.shipping_rules_conditions = sorted(self.conditions, key=lambda d: flt(d.from_value))
	return sorted(journal_entries + payment_entries, key=lambda k: k[2] or getdate(nowdate()))
		row.payment_terms = sorted(row.payment_terms, key=lambda x: x["due_date"])
		po_data = sorted(po_data, key=lambda i: i["rm_item_code"])
		sorted_failed_import_log = sorted(failed_import_log, key=lambda row: row["doc"]["creation"])
		return sorted(accounts, key=lambda account: int(account["Id"]))
		result = sorted(result, key=lambda x: x.get("ranking"), reverse=True)
	valid_shifts.sort(key=lambda x: x["actual_start"])
	employee_data = sorted(employee_data, key=lambda k: k["employee_name"])
	sorted_pledges = dict(sorted(current_pledges.items(), key=lambda item: item[1], reverse=True))
		for parameter, employees in groupby(employee_details, key=lambda d: d[group_by]):

		year_list.sort(key=lambda d: d.year, reverse=True)
		sub_assembly_items_store.sort(key=lambda d: d.bom_level, reverse=True)  # sort by bom level
				for item in sorted(item_dict.values(), key=lambda d: d["idx"] or float("inf")):
	tasks.sort(key=lambda x: x["delay"], reverse=True)
		self.data.sort(key=lambda x: x["per_util"], reverse=True)
			d.credit_limit for d in sorted(self.credit_limits, key=lambda k: k.company)
	loop_data = sorted(data, key=lambda k: k["indent"], reverse=True)
		for item, value in (sorted(item_wise_sales_map.items(), key=lambda i: i[1], reverse=True))
		result = sorted(report[1], key=lambda k: k["entity"])

		result = sorted(report[1], key=lambda k: k["entity"])
		return min(out, key=lambda x: x[1])[0]  # find min by sort_key
		notifications = sorted(notifications.get("open_count_doctype", {}).items(), key=lambda a: a[1])
	sorted_warehouse_map = sorted(warehouses, key=lambda i: i["balance"], reverse=True)
	capacity_data = sorted(capacity_data, key=lambda i: (i[sort_by] * asc_desc))
		return sorted(entries_to_fix, key=lambda k: k["timestamp"])
	batches_dates.sort(key=lambda tup: tup[1])
		taxes = sorted(taxes_with_validity, key=lambda i: i.valid_from, reverse=True)
	for customer, rows in groupby(sales_orders, key=lambda so: so["customer"]):
	vacant_rules = sorted(vacant_rules, key=lambda i: (i["priority"], -i["free_space"]))
		expected_sle.sort(key=lambda x: x[1])

		sle.sort(key=lambda x: x[1])

		expected_gl_entries.sort(key=lambda x: x[0])

		gl_entries.sort(key=lambda x: x[0])
	data = sorted(data, key=lambda i: i[-1], reverse=True)
	lr_list = sorted(item_groups_dict, key=lambda x: int(x[0]))
	data = sorted(data, key=lambda i: i[-1], reverse=True)
	data.sort(key=lambda row: row[6], reverse=True)
    for field in sorted(embedded_fields, key=lambda a: a.count(".")):

    for field in sorted(embedded_fields, key=lambda a: a.count(".")):
d3 = dict(sorted(DIAL_CODES, key=lambda x:x[1]))  # <3>
        res.sort(key=lambda item: (-item[0], item[1]))
        res.sort(key=lambda item: (-item[0], item[1]))
        res.sort(key=lambda item: (-item[0], item[1]))
        result_sort = sorted(result, key=lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8]))

        result_sort = sorted(result, key=lambda x: (x[0]))
    new_data = sorted(origin_dict.items(), key=lambda item: item[1], reverse=True)

    new_data = sorted(origin_dict.items(), key=lambda item: item[1][0], reverse=True)

    new_data = sorted(origin_dict.items(), key=lambda item: time.mktime(time.strptime(item[0], "%Y-%m-%d")), reverse=False)
    for item in sorted(data['updated'], key=lambda x: x['chapterUid']):

        for start, text in sorted(contents[c], key=lambda e: e[0]):
    sort_list = sorted(dict_common_in.items(), key=lambda item: item[1], reverse=True)
            tunnel_sock = Mock(name="tunnel_sock", recv=lambda n: data)
            finalized[sent] = sorted(finalized[sent], key=lambda r: r['score'], reverse=True)
            sorted_hyps = sorted(hypotheses.beams, key=lambda x: x[0])
        for _, s in sorted(vocab.items(), key=lambda x: x[0]):
            seg_group = sorted(_seg_group, key=lambda x: float(x["offset"]))
    sequence2length.sort(key=lambda x: x[1])
        data = [v for k, v in sorted(data, key=lambda x: x[0])]
            seg_group = sorted(_seg_group, key=lambda x: x["offset"])
    sequence2length.sort(key=lambda x: x[1])
    sequence2length.sort(key=lambda x: x[1])
        for id, src_tokens, hypos in sorted(results, key=lambda x: x[0]):
            self.remapping.sort(key=lambda i: lengths[i])
    a = sorted(d.items(), key=lambda i: i[0])
        outputs = [hypos for _, hypos in sorted(results, key=lambda x: x[0])]
                sampled_indices.sort(key=lambda i: self.num_tokens(i))
        self.longest_dataset_key = max(datasets, key=lambda k: len(datasets[k]))
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))
        nodes.sort(key=lambda n: n.id)
        res = [hypos for _, hypos in sorted(res, key=lambda x: x[0])]
        for id_, src_tokens, hypos, info in sorted(results, key=lambda x: x[0]):
        for ws in sorted(word_stats.values(), key=lambda x: x.count, reverse=True):
        networks_to_exclude.sort(key=lambda x: x.prefixlen)
        return sorted(self._images.values(), key=lambda item: item.modified)
    dataset = sorted(dataset, key=lambda r: r[1])
        resp.media = sorted(self._items.values(), key=lambda item: item['itemid'])
    mock = MagicMock(side_effect=lambda *a, **k: add_route(router, *a, **k))
        for login in sorted(contributors, key=lambda s: s.lower()):
                    for k, g in itertools.groupby(rowise, key=lambda x: x[1])
            k: list(group) for k, group in itertools.groupby(rows, key=lambda r: r[0])
@pytest.mark.parametrize("full_feature_names", [True, False], ids=lambda v: str(v))
@pytest.mark.parametrize("pass_as_path", [True, False], ids=lambda v: str(v))
@pytest.mark.parametrize("full_feature_names", [True, False], ids=lambda v: str(v))

@pytest.mark.parametrize("full_feature_names", [True, False], ids=lambda v: str(v))

@pytest.mark.parametrize("full_feature_names", [True, False], ids=lambda v: str(v))

@pytest.mark.parametrize("full_feature_names", [True, False], ids=lambda v: str(v))
@pytest.mark.parametrize("infer_features", [True, False], ids=lambda v: str(v))

@pytest.mark.parametrize("infer_features", [True, False], ids=lambda v: str(v))
@pytest.mark.parametrize("full_feature_names", [True, False], ids=lambda v: str(v))

@pytest.mark.parametrize("full_feature_names", [True, False], ids=lambda v: str(v))

@pytest.mark.parametrize("full_feature_names", [True, False], ids=lambda v: str(v))
        self.seed_features = sorted(seed_features or [], key=lambda f: f.unique_name())

        new_features.sort(key=lambda f: f.get_depth())

            tuple(sorted(s, key=lambda x: x.get_name().lower()))
    def wait_for_property(self, name, cond=lambda val: val, level_sensitive=True):
    def wait_for_property(self, name, cond=lambda val: val, level_sensitive=True):

    def prepare_and_wait_for_property(self, name, cond=lambda val: val, level_sensitive=True):

    def wait_for_event(self, *event_types, cond=lambda evt: True):

    def prepare_and_wait_for_event(self, *event_types, cond=lambda evt: True):
            songs = sorted(songs, key=lambda song: score(s, repr_song(song)),
        rules = sorted(rules, key=lambda rule: sorted(rule.methods))  # type: ignore
        # for i, e in enumerate(sorted(events, key=lambda event: event['uuid'])):
	messages = sorted(messages, key=lambda x: x[0])

	app_messages.sort(key=lambda x: x[1])
		sorted_counts = sorted(counts, key=lambda k: k["count"])
		sorted_obj = dict(sorted(obj.items(), key=lambda kv: str(kv[0])))
		tablecolumns.sort(key=lambda a: int(a.idx))
		self.doctypes = sorted(list(set(doctypes)), key=lambda x: -1 if x[0] == self.doctype else 1)
	languages.sort(key=lambda a: a["code"])
		get_user_permissions().get(doctype, []), key=lambda x: x.get("is_default"), reverse=True
		"doctypes": sorted(doctypes_list, key=lambda d: d["label"]),

		"roles": sorted(roles_list, key=lambda d: d["label"]),
			values = sorted(values, key=lambda x: relevance_sorter(x, txt, as_dict))

	return sorted(filtered_mentions, key=lambda d: d["value"])
		user_icons.sort(key=lambda a: a.idx)
	return sorted(activity_list, key=lambda k: k["time"], reverse=True)
	files.sort(key=lambda x: x[1], reverse=True)
	for parent, rows in itertools.groupby(res, key=lambda row: row["parenttype"]):
	files.sort(key=lambda item: item.client_modified, reverse=True)
		custom_fields = sorted(self.get_custom_fields(), key=lambda df: df.idx)
			"top_reviewer": max(user_points, key=lambda x: x["given_points"]),
	app_change_log = sorted(app_change_log, key=lambda d: d[0], reverse=True)
	results = sorted(results, key=lambda x: x.relevance, reverse=True)
	longest_match = max(sequence, key=lambda seq: len(seq.get("token", "")))
	patterns_desc = sorted(patterns, key=lambda x: len(x), reverse=True)
	return sorted((comments + communications), key=lambda comment: comment["creation"], reverse=True)

					_children = sorted(children, key=lambda x: os.path.basename(x.route))
    strategy_objs = sorted(strategy_objs, key=lambda x: x['name'])
        data = sorted(data, key=lambda x: x[0])

                    data = sorted(data, key=lambda x: x[0])
        locks = sorted(locks, key=lambda l: l.lock_end_time, reverse=True)
    tabular_data = sorted(tabular_data, key=lambda k: k['profit_total_abs'], reverse=True)

        tabular_data = sorted(tabular_data, key=lambda k: k['profit_total_abs'], reverse=True)

        best_trade = max(strat_results['trades'], key=lambda x: x['profit_ratio'])

        worst_trade = min(strat_results['trades'], key=lambda x: x['profit_ratio'])
        sorted_tickers = sorted(filtered_tickers, reverse=True, key=lambda t: t[self._sort_key])
            trade = sorted(trades, key=lambda t: t.close_date)[-1]  # type: ignore
    strategies = sorted(strategies, key=lambda x: x['name'])

    pair_interval = sorted(pair_interval, key=lambda x: x[0])
    inf_dataframe.rename(columns=lambda column: formatter(column=column, **fmt_args),
    sleep_mock = mocker.patch('time.sleep', side_effect=lambda _: None)

    price_mock = MagicMock(side_effect=lambda p, s: int(s))
    ght_mock = MagicMock(side_effect=lambda pair, *args, **kwargs: (pair, trades_history))
    ind_mock = MagicMock(side_effect=lambda x, meta: x)

    entry_mock = MagicMock(side_effect=lambda x, meta: x)

    exit_mock = MagicMock(side_effect=lambda x, meta: x)

    ind_mock = MagicMock(side_effect=lambda x, meta: x)

    entry_mock = MagicMock(side_effect=lambda x, meta: x)

    exit_mock = MagicMock(side_effect=lambda x, meta: x)
    @informative('30m', 'ETH/{stake}', fmt=lambda column, **kwargs: column + '_from_callable')
        best = process.extractOne(query, events, processor=lambda event: event[0])
    return heapq.nlargest(limit, sl, key=lambda i: i[1]) if limit is not None else \

        sorted(sl, key=lambda i: i[1], reverse=True)

    return heapq.nlargest(limit, best_list, key=lambda i: i[1]) if limit is not None else \

        sorted(best_list, key=lambda i: i[1], reverse=True)

        return max(best_list, key=lambda i: i[1])

            filtered = sorted(filtered, key=lambda x: x[0])

            filter_sort = sorted(filtered, key=lambda x: len(x[0]), reverse=True)
    commits_sorted = dict(sorted(commits.items(), key=lambda item: -item[1]))
        threads = sorted(gdb.selected_inferior().threads(), key=lambda t: t.num)

        self.loaded_commands = sorted(self.loaded_commands, key=lambda x: x[1]._cmdline_)
sims = sorted(enumerate(sims), key=lambda item: -item[1])
for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):
for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):
sims = sorted(enumerate(sims), key=lambda item: -item[1])
            good_ids.sort(key=lambda x: self.num_docs if x in keep_ids else self.dfs.get(x, 0), reverse=True)

                for tokenid, freq in sorted(self.dfs.items(), key=lambda item: -item[1]):

            in sorted(self.cfs.items(), key=lambda x: (-x[1], x[0]))[:n]
    sims = sorted(enumerate(sims), key=lambda item: -item[1])
        ok = frozenset(word for word, freq in sorted(ok, key=lambda x: -x[1])[:keep_n])

                    words_df = ["%s(%i)" % item for item in sorted(words_df, key=lambda x: -x[1])]
            temp = sorted(temp, key=lambda x: x[0], reverse=True)

        temp = sorted(temp, key=lambda x: x[0], reverse=True)
        return sorted(scored_topics, key=lambda tup: tup[1], reverse=True)
        min_distance_per_topic_sorted = sorted(min_distance_per_topic, key=lambda distance: distance[0])
            store_order_vocab_keys = sorted(self.key_to_index.keys(), key=lambda k: -self.get_vecattr(k, sort_attr))
        return sorted(scored_topics, key=lambda tup: tup[1], reverse=True)
        weights = sorted(result[topic], key=lambda x: -abs(x[0]))
            sorted_vocab = sorted(self.raw_vocab.keys(), key=lambda word: self.raw_vocab[word], reverse=True)
    >>> sorted(model.raw_vocab, key=lambda w: len(w), reverse=True)[:5]
        return sorted(result.items(), key=lambda x: (-x[1], x[0]))[:topn]
    return heapq.nlargest(n, itertools.chain(*iterable), key=lambda item: abs(item[1]))
        d = HashDictionary(self.texts, id_range=2, myhash=lambda key: len(key))
        most_common_word = max(model.wv.key_to_index, key=lambda word: model.wv.get_vecattr(word, 'count'))[0]
        model = tfidfmodel.TfidfModel(corpus, wlocal=lambda x: x, wglobal=lambda x, y: x * x, smartirs='nnc')

        model = tfidfmodel.TfidfModel(corpus, wlocal=lambda x: x * x, wglobal=lambda x, y: x, smartirs='nnc')
    def __init__(self, importing, extra_all=lambda mod_name: ()):
        self.client.storbinary('stor', f, callback=lambda x: flag.append(None))

        self.client.storlines('stor foo', f, callback=lambda x: flag.append(None))
        self.client.storbinary('stor', f, callback=lambda x: flag.append(None))

        self.client.storlines('stor foo', f, callback=lambda x: flag.append(None))
        self.client.storbinary('stor', f, callback=lambda x: flag.append(None))

        self.client.storlines('stor foo', f, callback=lambda x: flag.append(None))
        self.client.storbinary('stor', f, callback=lambda x: flag.append(None))

        self.client.storlines('stor foo', f, callback=lambda x: flag.append(None))
            t = threading.Thread(target=lambda : time.sleep(0.3))
            t = threading.Thread(target=lambda : time.sleep(0.3))
        self.client.storbinary('stor', f, callback=lambda x: flag.append(None))

        self.client.storlines('stor foo', f, callback=lambda x: flag.append(None))
            t = threading.Thread(target=lambda : time.sleep(0.3))
            t = threading.Thread(target=lambda : time.sleep(0.3))
        self.client.storbinary('stor', f, callback=lambda x: flag.append(None))

        self.client.storlines('stor foo', f, callback=lambda x: flag.append(None))
        self.client.storbinary('stor', f, callback=lambda x: flag.append(None))

        self.client.storlines('stor foo', f, callback=lambda x: flag.append(None))
            t = threading.Thread(target=lambda : time.sleep(0.3))
            t = threading.Thread(target=lambda : time.sleep(0.3))
images.sort(key=lambda x: os.stat(os.path.join(rundir1, x)).st_mtime)
        return sorted(self.entries.values(), key=lambda e: (e.path, e.stage))
        self.assertEqual(next(start.traverse(branch_first=1, prune=lambda i, d: i == p0)), p1)

        self.assertEqual(next(start.traverse(branch_first=1, predicate=lambda i, d: i == p1)), p1)
        for blob in tree.traverse(predicate=lambda e, d: e.type == "blob", branch_first=False):

        for blob in tree.traverse(predicate=lambda item, d: item.type == "blob"):
            stats.sort(key=lambda process: process['name'] if process['name'] is not None else '~', reverse=False)
    themax = max(tree, key=lambda d: d['weight'])
            len(max(self.stats['containers'], key=lambda x: len(x['name']))['name']),
                sorted(gmail_ids.items(), key=lambda t: t[0]))
  for param in sorted(params.iteritems(), key=lambda x: x[0]):
  for param in sorted(params.iteritems(), key=lambda x: x[0]):
  for param in sorted(params.iteritems(), key=lambda x: x[0]):
            sorted(api_directory.items(), key=lambda x: x[0])
    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)
        extra_args = sorted(extra_args.items(), key=lambda f: f[1])
        fields_with_names = sorted(fields_with_names, key=lambda f: f[1])
        hello = String(resolver=lambda *_: "World")

        hello = Dynamic(lambda: String(resolver=lambda *_: "World"))

        hellos = Dynamic(lambda: List(String, resolver=lambda *_: ["Worlds"]))

        hello_field = Dynamic(lambda: Field(String, resolver=lambda *_: "Field World"))
        s = graphene.String(resolver=lambda *_: "S")
        x[0] for x in sorted(qualified_content_types, key=lambda x: x[1], reverse=True)

                    and max(responses, key=lambda response: response[1])[1]
    known_settings = sorted(guncfg.KNOWN_SETTINGS, key=lambda s: s.section)
    urlset[:] = sorted([url for url in urlset], key=lambda url: url[0].text)
        rows = sorted(rows, key=lambda x: int(x[2]), reverse=False)

        rows = sorted(rows, key=lambda x: x[0], reverse=True)

        rows = sorted(rows, key=lambda x: (x[1], int(x[2])), reverse=False)

        rows = sorted(rows, key=lambda x: int(x[2]), reverse=False)

        rows = [rows[0]] + sorted(rows[1:], key=lambda x: int(x[2]), reverse=True)
        workers = sorted(workers, key=lambda w: w[1].age)
notebooks = sorted(notebooks, key=lambda x: x[8])
notebooks = sorted(notebooks, key=lambda x: natural_keys(x))
notebooks = sorted(notebooks, key=lambda x: x[8])
notebooks = sorted(notebooks, key=lambda x: x[8])
notebooks = sorted(notebooks, key=lambda x: x[8])
notebooks = sorted(notebooks, key=lambda x: natural_keys(x))
notebooks = sorted(notebooks, key=lambda x: natural_keys(x))
notebooks = sorted(notebooks, key=lambda x: natural_keys(x))
notebooks = sorted(notebooks, key=lambda x: natural_keys(x))
notebooks = sorted(notebooks, key=lambda x: natural_keys(x))
notebooks = sorted(notebooks, key=lambda x: natural_keys(x))
notebooks = sorted(notebooks, key=lambda x: natural_keys(x))
        return sorted(candidate_docs, key=lambda x: x.score if x.score is not None else 0.0, reverse=True)[0:top_k]
        sorted_documents = sorted(documents, key=lambda doc: vector_ids.index(doc.meta["vector_id"]))
        sorted_docs = sorted(scores_map.items(), key=lambda d: d[1], reverse=True)
            OrderedDict(sorted(query_idx_scores, key=lambda tup: tup[1], reverse=True))
        sorted_matches = sorted(matches, key=lambda candidate: candidate.score, reverse=True)

        group_sorted_matches = sorted(matches, key=lambda candidate: candidate.context_id)

        grouped_matches = groupby(group_sorted_matches, key=lambda candidate: candidate.context_id)

            sorted_group = sorted(group, key=lambda candidate: candidate.score, reverse=True)
    docs.sort(key=lambda d: d.id)

    docs = sorted(docs, key=lambda d: d.id)

    docs = sorted(docs, key=lambda d: d.id)
        checks.sort(key=lambda check: check.created)
    pairs.sort(key=lambda pair: pair[0].lower())

        projects.sort(key=lambda p: (p.overall_status() != "down", p.name))

    events.sort(key=lambda el: el.created, reverse=True)

    sorted_tags = sorted(tags, key=lambda s: s.lower())
    keys.sort(key=lambda item: (item.count("-"), item))
    @mock.patch("os.path.abspath", side_effect=lambda f: f)

    @mock.patch("os.path.abspath", side_effect=lambda f: f)

    @mock.patch("os.path.abspath", side_effect=lambda f: f)

    @mock.patch("os.path.abspath", side_effect=lambda f: f)
        headers = sorted(lines[1:], key=lambda h: h.split(':')[0])
    shown_arguments.sort(key=lambda argument: argument.aliases, reverse=True)
    s.sort(key=lambda s: s["string"])

    groups.sort(key=lambda s: s["string"])
    @hug.call(on_invalid=lambda data: "error")

    @hug.get(transform=lambda data: "Goodbye {0}!".format(data))
def _sym(wanted, f=lambda x: x):
def _get_code_from_file(run_name, fname=None, hy_src_check=lambda x: x.endswith(".hy")):
        module_data.sort(key=lambda x: id(x))
                contributions = sorted(contributions, key=lambda r: -r[1])
    fmin(fn=lambda x: 1, space=space, algo=rand.suggest, max_evals=50)

    fmin(fn=lambda x: 1, space=space, algo=anneal.suggest, max_evals=50)
        for label, score in sorted(best_targets.items(), key=lambda x: x[::-1]):

                    for k, v in sorted(events.items(), key=lambda x: (-x[1], x[0]))
                    sorted({type(e) for e in elems}, key=lambda t: t.__name__)
    return tuple(sorted(uniques, key=lambda e: e.__name__))

    for module_name in sorted(sys.modules, key=lambda n: tuple(n.split("."))):
        _categories = sorted(cm.keys(), key=lambda c: len(cm[c]))
                children.sort(key=lambda j: self.data[j].score)
        report_lines.sort(key=lambda line: (line.startswith(LIB_DIR), line))
            self.interesting_examples.values(), key=lambda d: sort_key(d.buffer)
        self.front = SortedList(key=lambda d: sort_key(d.buffer))
        results.sort(key=lambda t: (t[1] - t[0], t[1]))

        for u, v in sorted(regions_to_delete, key=lambda x: x[1] - x[0], reverse=True):
def minimal(definition, condition=lambda x: True, settings=None, timeout_after=10):

def find_any(definition, condition=lambda _: True, settings=None):

def assert_no_examples(strategy, condition=lambda _: True):
    recursive(base=booleans(), extend=lambda x: lists(x, max_size=3), max_leaves=10),
        j = chooser.choose(range(10), condition=lambda j: j > i)

        chooser.choose(range(10), condition=lambda j: False)

                chooser.choose(range(3), condition=lambda x: x > 0),
    a = attr.ib(converter=lambda x: x)
        ds.builds(non_callable, target=lambda x: x).example()
@given(functions(like=lambda a: None, returns=booleans()))
@pytest.mark.parametrize("t", BUILTIN_TYPES, ids=lambda t: t.__name__)

@pytest.mark.parametrize("t", BUILTIN_TYPES, ids=lambda t: t.__name__)

@pytest.mark.parametrize("t", BUILTIN_TYPES, ids=lambda t: t.__name__)
    s = integers().map(pack=lambda t: "foo")
    @given(lists(integers(), unique=True, unique_by=lambda x: x))
def with_docstring(a, b, c, d=int, e=lambda x: f"xx{x}xx") -> None:
    text(), lambda x: any(ord(c) > 127 for c in x), condition=lambda x: len(x) <= 3

    lists(sampled_from(range(5))), distorted_value, condition=lambda x: len(x) >= 3
def iter_values(strategy, unique_by=lambda s: s):
sorted_by_value = sorted(tags.items(), key=lambda kv: kv[1], reverse=True)
            sorted(reels, key=lambda m: m["taken_at"], reverse=True)
    return sorted(comments, key=lambda k: k["created_at_utc"], reverse=False)
    ProgressBar(persist=True).attach(trainer, output_transform=lambda out: {"batch loss": out})
        ProgressBar().attach(trainer, output_transform=lambda x: {"batch loss": x})
    ProgressBar(persist=True).attach(trainer, output_transform=lambda out: {"batch loss": out})
    ProgressBar(persist=True).attach(trainer, output_transform=lambda out: {"batch loss": out})
    RunningAverage(output_transform=lambda x: x).attach(trainer, "loss")
    RunningAverage(output_transform=lambda x: x[0]).attach(trainer, "batch_loss")

    RunningAverage(output_transform=lambda x: x[1]).attach(trainer, "global_rate")
    RunningAverage(alpha=alpha, output_transform=lambda x: x["errD"]).attach(trainer, "errD")

    RunningAverage(alpha=alpha, output_transform=lambda x: x["errG"]).attach(trainer, "errG")

    RunningAverage(alpha=alpha, output_transform=lambda x: x["D_x"]).attach(trainer, "D_x")

    RunningAverage(alpha=alpha, output_transform=lambda x: x["D_G_z1"]).attach(trainer, "D_G_z1")

    RunningAverage(alpha=alpha, output_transform=lambda x: x["D_G_z2"]).attach(trainer, "D_G_z2")
            RunningAverage(output_transform=lambda x: x).attach(trainer, 'loss')

            pbar.attach(trainer, output_transform=lambda x: {'loss': x})
            self._saved.sort(key=lambda it: it[0])
            pbar.attach(trainer, output_transform=lambda x: {"loss": x})
            img_mean = Average(output_transform=lambda output: output['mean'])

            img_mean2 = Average(output_transform=lambda output: output['mean^2'])
            wps_metric = Frequency(output_transform=lambda x: x['ntokens'])

            wps_metric = Frequency(output_transform=lambda x: x['ntokens'])
            metric = RunningAverage(output_transform=lambda x: x.item())
    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len))
        return max(scores, key=lambda x: x.recall())
    handler = DummyOutputHandler("tag", metric_names=None, output_transform=lambda x: x)

    handler = DummyOutputHandler("tag", metric_names=None, output_transform=lambda x: {"loss": x})

    handler = DummyOutputHandler("tag", metric_names=["a", "b"], output_transform=lambda x: {"loss": x})

        DummyWeightsScalarHandler(model, reduction=lambda x: x)
    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("another_tag", output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", metric_names=["a", "b"], output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)
    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("another_tag", output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", metric_names=["a", "b"], output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)
        setup_common_training_handlers(trainer, to_save={}, output_path="abc", save_handler=lambda c, f, m: None)

    _test_setup_common_training_handlers(dirname, device="cpu", output_transform=lambda loss: [loss])

    _test_setup_common_training_handlers(dirname, device="cpu", output_transform=lambda loss: {"batch_loss": loss})
    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("another_tag", output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", metric_names=["a", "b"], output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)
    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("another_tag", output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", metric_names=["a", "b"], output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)

        WeightsScalarHandler(model, reduction=lambda x: x)
    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("another_tag", output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", metric_names=["a", "b"], output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)
    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("another_tag", output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", output_transform=lambda x: x, sync=False)

    wrapper = OutputHandler("another_tag", output_transform=lambda x: {"loss": x}, sync=True)

    wrapper = OutputHandler("tag", metric_names=["a", "b"], output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)
    RunningAverage(alpha=0.5, output_transform=lambda x: x).attach(trainer, "batchloss")

    RunningAverage(alpha=0.5, output_transform=lambda x: x[0]).attach(trainer, "batchloss")

    RunningAverage(alpha=0.5, output_transform=lambda x: x[1]).attach(trainer, "another batchloss")

    RunningAverage(alpha=0.5, output_transform=lambda x: x).attach(trainer, "batchloss")

    pbar.attach(engine, output_transform=lambda x: {"a": x})

    pbar.attach(engine, output_transform=lambda x: x)

    pbar.attach(engine, output_transform=lambda x: "red")

    pbar.attach(engine, output_transform=lambda x: x)

        pbar.attach(engine, output_transform=lambda x: x)

    pbar.attach(engine, output_transform=lambda x: x)
    roc_curve_metric = RocCurve(output_transform=lambda x: (x[1], x[2]))

    roc_curve_metric = RocCurve(output_transform=lambda x: (torch.sigmoid(x[1]), x[2]))
    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("tag", output_transform=lambda x: x)

    wrapper = OutputHandler("another_tag", output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", metric_names=["a", "b"], output_transform=lambda x: {"loss": x})

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)

    wrapper = OutputHandler("tag", output_transform=lambda x: {"loss": x}, global_step_transform=global_step_transform)

        WeightsScalarHandler(model, reduction=lambda x: x)

    output_handler = OutputHandler(tag="training", output_transform=lambda x: {"loss": x})

    output_handler = OutputHandler(tag="training", output_transform=lambda x: {"loss": x})

        output_handler = OutputHandler(tag="training", output_transform=lambda x: {"loss": x})
    precision_recall_curve_metric = PrecisionRecallCurve(output_transform=lambda x: (x[1], x[2]))

    precision_recall_curve_metric = PrecisionRecallCurve(output_transform=lambda x: (torch.sigmoid(x[1]), x[2]))
        Events.ITERATION_STARTED(event_filter=lambda x: x)
    mock_fn_1 = create_autospec(spec=lambda x: None)

    mock_fn_2 = create_autospec(spec=lambda x: None)

    handler = create_autospec(spec=lambda x: None)

    handler = create_autospec(spec=lambda x: None)

    handler = create_autospec(spec=lambda x: None)

    handler = create_autospec(spec=lambda x: None)

    handler = create_autospec(spec=lambda x: None)

    handler = create_autospec(spec=lambda x: None)

        handler = create_autospec(spec=lambda e, x1, x2, x3, a, b: None)
        EarlyStopping(patience=-1, score_function=lambda engine: 0, trainer=trainer)

        EarlyStopping(patience=2, min_delta=-0.1, score_function=lambda engine: 0, trainer=trainer)

        EarlyStopping(patience=2, score_function=lambda engine: 0, trainer=None)

    h = EarlyStopping(patience=2, min_delta=0.1, score_function=lambda _: next(scores), trainer=trainer)
        Checkpoint(to_save, lambda x: x, score_function=lambda e: 123, score_name="acc", global_step_transform=123)

    checkpointer = Checkpoint(to_save, lambda x: x, score_function=lambda e: {"1": 1}, score_name="acc")

    checkpointer = Checkpoint(to_save, save_handler=save_handler, score_function=lambda e: e.state.score)

    assert _test(to_save, score_function=lambda e: e.state.score, dirname=dirname) == "model_0.9999.pt"

        _test(to_save, score_function=lambda e: e.state.score, score_name="acc", dirname=dirname)

    assert _test(to_save, "best", score_function=lambda e: e.state.score, dirname=dirname) == "best_model_0.9999.pt"

    res = _test(to_save, "best", score_function=lambda e: e.state.score, score_name="acc", dirname=dirname)

        _test(to_save, score_function=lambda e: e.state.score, filename_pattern=pattern, dirname=dirname)

    res = _test(to_save, "best", score_function=lambda e: e.state.score, filename_pattern=pattern, dirname=dirname)
    eos = EpochOutputStore(output_transform=lambda x: x[0])
        lr_finder._log_lr_and_loss(dummy_engine, output_transform=lambda x: x, smooth_f=0, diverge_th=1)

        lr_finder._log_lr_and_loss(dummy_engine, output_transform=lambda x: x, smooth_f=0, diverge_th=1)

        lr_finder._log_lr_and_loss(dummy_engine, output_transform=lambda x: x, smooth_f=0, diverge_th=1)

    lr_finder._log_lr_and_loss(dummy_engine, output_transform=lambda x: x, smooth_f=0, diverge_th=1)

    lr_finder._log_lr_and_loss(dummy_engine, output_transform=lambda x: x, smooth_f=0, diverge_th=1)

    lr_finder._log_lr_and_loss(engine, output_transform=lambda x: x, smooth_f=0.1, diverge_th=10.0)
        Fbeta(1.0, precision=p, output_transform=lambda x: x)

        Fbeta(1.0, recall=r, output_transform=lambda x: x)
        custom_var_mean = metric_cls(output_transform=lambda output: output[1])

        custom_var_mean = metric_cls(output_transform=lambda output: output[1])

    img_mean = Average(output_transform=lambda output: output["mean"])

    img_mean2 = Average(output_transform=lambda output: output["mean^2"])

        custom_var_mean = metric_cls(output_transform=lambda output: output[1], device=metric_device)

        custom_var_mean = metric_cls(output_transform=lambda output: output[1], device=metric_device)
    wps_metric = Frequency(output_transform=lambda x: x["ntokens"])
    def __init__(self, loss_fn, true_output, output_transform=lambda x: x):
    def __init__(self, true_output, output_transform=lambda x: x):
        FID(num_features=1, feature_extractor=lambda x: x)
        RunningAverage(Accuracy(), output_transform=lambda x: x[0])

    acc_metric = RunningAverage(Accuracy(output_transform=lambda x: [x[1], x[2]]), alpha=alpha)

    avg_output = RunningAverage(output_transform=lambda x: x[0], alpha=alpha)

    acc_metric = RunningAverage(Accuracy(output_transform=lambda x: [x[1], x[2]]), alpha=alpha, epoch_bound=False)

    avg_output = RunningAverage(output_transform=lambda x: x[0], alpha=alpha, epoch_bound=False)

    m = RunningAverage(output_transform=lambda x: x)

    avg_output = RunningAverage(output_transform=lambda x: x, alpha=alpha, epoch_bound=False, device=metric_device)

            Accuracy(output_transform=lambda x: [x[0], x[1]], device=metric_device), alpha=alpha, epoch_bound=False

        avg = RunningAverage(output_transform=lambda x: x, device=metric_device)
        DummyInceptionMetric(num_features=1000, feature_extractor=lambda x: x)
    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - len(candidates)), ref_len))
        categories.sort(key=lambda x: x['id'])
            order.sort(key=lambda x: self.image_aspect_ratio(x))
        result.sort(key=lambda x: x[2], reverse=True)
        for method in sorted(methods, key=lambda x: str(x)):
    shadowcredentials.add_argument('--export-type', action='store', required=False, choices=["PEM", " PFX"], type=lambda choice: choice.upper(), default="PFX",
        return OrderedDict(sorted(list(properties.items()), key=lambda x:x[1]['order']))

        sorted_props = sorted(list(properties.keys()), key=lambda k: properties[k]['order'])

        sorted_props = sorted(list(properties.keys()), key=lambda k: properties[k]['order'])
    all_followers = sorted(set(all_followers), key=lambda x: all_followers.index(x))

    all_following = sorted(set(all_following), key=lambda x: all_following.index(x))
                for x in sorted(self.tasks.values(), key=lambda x: x.name)
    with patch("invoke.config.expanduser", side_effect=lambda x: x):
        return sorted(funcs), sorted(classes, key=lambda x: x.name)
        self.chain.sort(key=lambda x: x[0])
        _filtered_matches = sorted(filtered_matches, key=lambda x: completions_sorting_key(x[0]))
        self._transformers.sort(key=lambda x: x.priority)

        self._checkers.sort(key=lambda x: x.priority)
            return real_unwrap(func, stop=lambda obj: _is_mocked(obj) or _stop(func))
    unique_authors = sorted(set(all_authors), key=lambda s: s.lower())
    return printer.pformat(dict(sorted(value.items(), key=lambda item: item[1])))  # type: ignore
      ans = max(tracers, key=lambda x: x._trace.level)

      ans = max(tracers, key=lambda x: x._trace.level)
    for naxis, raxes in sorted(axis_resources.items(), key=lambda x: str(x[0])):

    for naxis, vaxis in sorted(vmap_axes.items(), key=lambda x: x[1].uid):

  for name, dim in sorted(axes.items(), key=lambda x: x[1], reverse=True):

  for name, dim in sorted(axes.items(), key=lambda x: x[1]):
  entries, treedef = tree_flatten(axis_resources, is_leaf=lambda x: x is None)
        unique_limitations.values(), key=lambda pair: unique_hash(*pair)):
        unique_limitations.values(), key=lambda pair: unique_hash(*pair)):
    def tf_fn(x_str, compute_tf_fn=lambda x: x):
      dyn_args_flat, _ = tree_util.tree_flatten(dyn_args, is_leaf=lambda arg: isinstance(arg, BCOO))
    for ty, axes in sorted(axes_by_type.items(), key=lambda x: x[0].value):

    for name, axis in sorted(aval_axes.items(), key=lambda x: x[1]):
      tree_broadcast(lin_tree, ct_lin, is_leaf=lambda x: x is None),
      ans = max(tracers, key=lambda x: x._trace.level)

      ans = max(tracers, key=lambda x: x._trace.level)
def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):

  def gen(shape, dtype, post=lambda x: x):

  devices = sorted(api.devices(), key=lambda d: d.id)
      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

    @partial(new_checkpoint, policy=lambda *_, **__: False)

    @partial(new_checkpoint, policy=lambda *_, **__: False)

    @partial(new_checkpoint, policy=lambda *_, **__: False)

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

          ('_policy', partial(api.remat, policy=lambda *_, **__: False)),

          ('_new', partial(new_checkpoint, policy=lambda *_, **__: False)),

    @partial(remat, policy=lambda *_, **__: False)
    sorted_by_device = sorted(by_device, key=lambda x: x[0])

      hcb.call(lambda x: x, 3., result_shape=lambda x: x)
    y = lax.fori_loop(lower=0, upper=0, body_fun=lambda x, i: x + i, init_val=1.)

      y = lax.fori_loop(lower=0, upper=0, body_fun=lambda x, i: x + i, init_val=1.)
                    sorted(enumerate(dn.out_spec), key=lambda x: x[1])]
def rand_sparse(rng, nse=0.5, post=lambda x: x, rand_method=jtu.rand_default):
    leaves = leaf_fn(x, is_leaf=lambda t: False)

    leaves = leaf_fn(x, is_leaf=lambda t: isinstance(t, tuple))

    leaves = leaf_fn(x, is_leaf=lambda t: isinstance(t, list))

    leaves = leaf_fn(x, is_leaf=lambda t: True)

    leaves = leaf_fn(y, is_leaf=lambda t: isinstance(t, tuple))

    treedef = structure_fn(x, is_leaf=lambda t: False)

    treedef = structure_fn(x, is_leaf=lambda t: isinstance(t, tuple))

    treedef = structure_fn(x, is_leaf=lambda t: isinstance(t, list))

    treedef = structure_fn(x, is_leaf=lambda t: True)

    treedef = structure_fn(y, is_leaf=lambda t: isinstance(t, tuple))
    self.assert_wolfe(s, phi=lambda sp: f(x + p * sp),
            da = sorted(da, key=lambda ma: ma.scores['relevance'].value, reverse=True)
    merged_list.sort(key=lambda x: version.Version(x['version']))
    return sorted(_schema['properties'].items(), key=lambda k: k[0])
            result.sort(key=lambda x: version.Version(x['version']))
            matches = sorted(results[0].docs[0].matches, key=lambda match: match.id)
            sorted(filtered_client_resps, key=lambda msg: msg.docs[0].text)
        return iter(sorted(self.extensions.values(), key=lambda x: x.priority))
    f"({'|'.join(re.escape(x) for x in sorted(operators, key=lambda x: -len(x)))})"
        e = Environment(finalize=lambda v: "" if v is None else v)

        e = Environment(finalize=lambda v: "" if v is None else v)

        e = Environment(finalize=lambda v: type(v).__name__)
        env = Environment(autoescape=lambda x: False)
        self.entries = sorted(self.entries, key=lambda entry: entry.date)
        infos = sorted(infos, key=lambda i: [int(i) for i in i.key.split(':')])
        values = [v for k, v in sorted(values, key=lambda x: len(x[0]))]
        values = [v for k, v in sorted(values, key=lambda x: len(x[0]))]
    counters = sorted(counters.items(), key=lambda x: x[1])
    results = sorted(results.items(), key=lambda x: x[1], reverse=True)
            queryset = sorted(queryset, key=lambda asset: asset.hostname)
        organizations.sort(key=lambda x: x.name)
        nodes = sorted(nodes, key=lambda x: x.value)
        queryset = sorted(queryset, key=lambda x: x[order_by], reverse=reverse)
            merged_commands.sort(key=lambda command: command.timestamp)

            merged_commands.sort(key=lambda command: command.timestamp, reverse=True)
        return sorted(queryset, key=lambda command: command.timestamp, reverse=True)
        roles = sorted(list(self.all()), key=lambda r: r.scope)
    users = sorted(r.json(), key=lambda d: d['name'])

    before_servers = sorted(db.query(orm.Server), key=lambda s: s.url)

    after_servers = sorted(db.query(orm.Server), key=lambda s: s.url)

    before_servers = sorted(db.query(orm.Server), key=lambda s: s.url)
                    _order_ = [name for (name, value) in sorted(members.items(), key=lambda item: item[1])]
    cluster = create_cluster(mocker, topics={'t0', 't1', 't2'}, topic_partitions_lambda=lambda t: partitions[t])

    cluster = create_cluster(mocker, topics={'t1', 't2'}, topic_partitions_lambda=lambda t: partitions[t])

    cluster = create_cluster(mocker, topics=all_topics, topic_partitions_lambda=lambda t: partitions[t])

    cluster = create_cluster(mocker, topics=all_topics, topic_partitions_lambda=lambda t: partitions[t])
                sorted_dict = sorted(obj.items(), key=lambda pair: str(pair[0]))  # 2
        _CONTEXT_CLASS = Validator("CONTEXT_CLASS", default=lambda *_: MyContext)
  tf_blocks = sorted(tf_blocks, key=lambda x: int(x.split('_')[1]))
    result.sort(key=lambda x: x[2], reverse=True)
        callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001)
    layers_for_depth.sort(key=lambda x: layer_indices[x])
  return sorted(feature_columns, key=lambda x: x.name)
    ld = keras.layers.Lambda(lambda x: x[0], output_shape=lambda x: x[0])

        lambda x: x + 1, output_shape=(1, 1), mask=lambda i, m: m)
  return sorted(zip(keys, values), key=lambda x: x[1])
          counts.items(), key=lambda item: item[1], reverse=True)
    wrapper = wrapper_cls(cell, residual_fn=lambda i, o: i + i + o)

    wrapper = wrapper_cls(cell, dropout_state_filter_visitor=lambda s: True)
      sorted(var_dict.items(), key=lambda t: t[0]))
    v = tf.compat.v1.get_variable("foo", initializer=lambda x=True: [2])
    for (name, g), v in sorted(self._weights.items(), key=lambda i: i[0][0]):
  metric_layers.sort(key=lambda layer: metrics_names.index(layer.metric_name))
    wcounts.sort(key=lambda x: x[1], reverse=True)
  for layer in sorted(model.layers, key=lambda x: x.name):
        os.walk(subpath, followlinks=follow_links), key=lambda x: x[0])
          sorted(optimizer.variables(), key=lambda v: v.name))

          self.evaluate(sorted(optimizer.variables(), key=lambda v: v.name)))
  for root, _, files in sorted(walk, key=lambda x: x[0]):
def expand_dirs(items, exclude=lambda x: x.endswith('.so')):
        directories = sorted((df for df in self.files.values() if df.ftype is FileType.directory), key=lambda x: len(x.name), reverse=True)
    for option in sorted(defn.iter_all_options(), key=lambda a: natural_keys(a.name)):
    for k in sorted(groups, key=lambda x: x.lower()):

        for f in sorted(groups[k], key=lambda x: x['full_name'].lower()):
    m.sort(key=lambda x: extract_summary_line(sys.modules[x].__doc__).upper())
        for k in sorted(self.gdict, key=lambda n: n.lower()):
        onwin_button.bind(on_press=lambda j: self.set_display_type('normal'))

        popup_button.bind(on_press=lambda j: self.set_display_type('popup'))
        self.rv.data = sorted(self.rv.data, key=lambda x: x['name.text'])
        animation.bind(on_complete=lambda *x: self.reset_animation(item))
    P = min(points, key=lambda p: p.y)
        return max(self.points, key=lambda pt: pt.x).x

        return min(self.points, key=lambda pt: pt.x).x

        return max(self.points, key=lambda pt: pt.y).y

        return min(self.points, key=lambda pt: pt.y).y
        files.sort(key=lambda x: x[1])
            tasklist = sorted(db, key=lambda n: n.priority)

        b = max(results, key=lambda r: results[r]['score'])
        self.bind(_kheight=lambda *args: self.update_viewport())

        self.bind(softinput_mode=lambda *dt: self.update_viewport(),

        self.bind(show_cursor=lambda *dt: self._set_cursor_state(dt[1]))
        image.bind(on_load=lambda *args, **kwargs: event.set())

        image.bind(on_load=lambda *args, **kwargs: event.set())
    scope='session', params=(True, False), ids=lambda v: 'loop=' + str(v))
    items = sorted(items, key=lambda x: x[0])

        for group, items in sorted(grouped.items(), key=lambda x: x[0]):
    infos.sort(key=lambda x: x['source'])
        btn.bind(on_release=lambda btn: dropdown.select(btn.text))

    dropdown.bind(on_select=lambda instance, x: setattr(mainbutton, 'text', x))

    dropdown.bind(on_select=lambda instance, x: setattr(mainbutton, 'text', x))

        dp.bind(on_select=lambda instance, x: setattr(button, 'text', x))

            item.bind(on_release=lambda btn: dp.select(btn.text))
            v.bind(selection=lambda *x: pprint("selection: %s" % x[1:]))

            v.bind(path=lambda *x: pprint("path: %s" % x[1:]))
            ani.bind(on_complete=lambda *_args: self.dispatch('on_open'))
                image.bind(on_load=lambda *a: set_size(image, image_size))
        anchor = max(points[:-1], key=lambda p: p.distance(touch.pos))
            item.bind(on_release=lambda option: dp.select(option.text))
        self.interface.bind(on_close=lambda j: self.dispatch('on_close'))

        self.close_button.bind(on_release=lambda j: self.dispatch('on_close'))
        anim.bind(on_complete=lambda *args: parent.remove_widget(self))
    whitening_models = torch.hub.load_state_dict_from_url(urls[kernel_type], map_location=lambda storage, loc: storage)
            pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=lambda storage, loc: storage)
        cb = ModelCheckpoint(tmp_path, 'test_monitor', filename_fcn=lambda x: "model.pt")
    for shape in sorted(label_file.shapes, key=lambda x: x["label"]):
    for shape in sorted(data["shapes"], key=lambda x: x["label"]):
    rules.sort(key=lambda r: len(r.expansion))
        terminals.sort(key=lambda x: (-x.priority, -x.pattern.max_width, -len(x.pattern.value), x.name))
        exps.sort(key=lambda x: (-x.max_width, -x.min_width, -len(x.value)))
                    p.sort(key=lambda r: r[0], reverse=True)
                considered_rules = list(sorted(to_scan, key=lambda key: key.rule.origin.name))
                for peer in sorted(batch, key=lambda peer: self.scores.get(peer, 0), reverse=True):
        peers.sort(key=lambda peer: distance(peer.node_id))
            self.active = OrderedDict(sorted(self.active.items(), key=lambda item: item[1]))

        not_yet_yielded.sort(key=lambda peer: self.distance(peer.node_id))
            peers.sort(key=lambda c: Distance(sort_distance_to)(c.node_id))

        contacts.sort(key=lambda c: distance(c.node_id))

            contacts.sort(key=lambda c: distance(c.node_id))
    return execute_command(conf, method, kwargs, callback=lambda data: data)
            streams.sort(key=lambda s: getattr(s, sort_by) or "")
    for blob in sorted(decoded['blobs'], key=lambda x: int(x['blob_num']), reverse=True):

        for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):
def constrain_single_or_list(constraints, column, value, convert=lambda x: x, negate=False):
        for txid, height in sorted(to_request, key=lambda x: x[1]):
        self.on_payment.listen(None, on_error=lambda e: logging.warning(e.args[0]))
        ordered = sorted(zip(request_ids, results), key=lambda t: t[0])
                candidates.sort(key=lambda sorting_node: distance(sorting_node.protocol.node_id))
    @mock.patch('os.urandom', side_effect=lambda i: b'd'*i)

    @mock.patch('os.urandom', side_effect=lambda i: b'f'*i)

    @mock.patch('os.urandom', side_effect=lambda i: b'd'*i)
    pairs.sort(key=lambda pair: pair[1])
                    sorted(unannotated, key=lambda n: cd.get(n).counter)
                lst.sort(key=lambda entry: entry.name)

        lst.sort(key=lambda tp_order: tp_order[1])
            sorted(constants, key=lambda descriptor: descriptor._index))
        self.errors.sort(key=lambda e: e.order)
    return sorted(outrows, key=lambda row: tuple(str(x) for x in row))
            installations.values(), key=lambda x: x.name.lower()):
        reqs.sort(key=lambda req: req.name.lower())

        reqs.sort(key=lambda req: req.name.lower())
                max(all_candidates, key=lambda c: c.version).version
            matches.sort(key=lambda x: -x[0])
        result = sorted(result, key=lambda t: t[0], reverse=True)
        directories.sort(key=lambda a: a.name)
def load(fin, translate=lambda t, x, v: v, object_pairs_hook=dict):

def loads(s, filename='<string>', translate=lambda t, x, v: v, object_pairs_hook=dict):
            paddedChunks.sort(key=lambda x: x[0])
        macro.patch.sort(key=lambda x: x[2],reverse=True)
            f.sort(key=lambda x: x[1].__code__.co_firstlineno)

            s.sort(key=lambda x: len(x[1]), reverse=True)
        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links
def unique(list_, key=lambda x: x):
    _whenRunning            = attrib(default=lambda **_: None)

    _reactorExited          = attrib(default=lambda **_: None)
        primesKeys = sorted(self.primes.keys(), key=lambda i: abs(i - bits))
        result = checkers.readAuthorizedKeyFile(fileobj, parseKey=lambda x: x)
        self.calls.sort(key=lambda a: a.getTime())
    def test_extractField(self, flattenFirst=lambda x: x):
        E.sort(key=lambda o: o[0])

        expect.sort(key=lambda o: o[0])
        self.servers.sort(key=lambda record: (record.priority, record.weight))
    pdop = property(fget=lambda self: self._getDOP('pdop'),

    hdop = property(fget=lambda self: self._getDOP('hdop'),

    vdop = property(fget=lambda self: self._getDOP('vdop'),
    optList.sort(key=lambda o: o.get('short', None) or o.get('long', None))
    def test_moveToSizeCache(self, hook=lambda : None):
            names, sorted(names, key=lambda name : name.split(".")[:4]),
    def __init__(self, quiescentCallback=lambda c: None):
        reg = sorted(comp.registeredUtilities(), key=lambda r: r.name)

        reg = sorted(comp.registeredAdapters(), key=lambda r: r.name)
        intervals.sort(key=lambda x: x.start)
        intervals.sort(key=lambda x:x.start)
    #     check.sort(key=lambda x : x[0])

        intervals.sort(key=lambda x: x.start)
    #     return min(path, key=lambda x: abs(target - x))

        return min((kid_min, root.val), key=lambda x: abs(target - x))
    #     A.sort(key=lambda x: x % 2)
        return sorted(letter_logs, key=lambda x: x.split(' ')[1:] + x.split(' ')[0]) + digit_logs
        A.sort(key=lambda x: abs(x))
    #     return sorted(points, key=lambda x: x[0] ** 2 + x[1] ** 2)[:K]

        return heapq.nsmallest(K, points, key=lambda x: x[0] ** 2 + x[1] ** 2)
            photos_with_timestamp = sorted(photos_with_timestamp, key=lambda x: x[0])
    return json.dumps(configs, default=lambda x: x.__dict__)
        counts = sorted(counts, key=lambda k: k["month"])
    data.sort(key=lambda x: len(x[1]), reverse=True)
        checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)
            largest_paths = sorted((x for x in path_sizes if x[-1] > 0.01), key=lambda x: x[1], reverse=True)[:25]
            statuses = sorted(statuses, key=lambda x: x["timestamp"])

            statuses = sorted(statuses, key=lambda x: x["timestamp"])
    state_paths_cleaned = apply_to_collection(state, dtype=(Path, BasePayload), function=lambda x: x.to_dict())

    state_diff_cleaned = apply_to_collection(state_paths_cleaned, dtype=type(NotPresent), function=lambda x: None)
        >>> trainer = Trainer(callbacks=[LambdaCallback(setup=lambda *args: print('setup'))])
        report.sort(key=lambda x: x[4], reverse=True)

        report.sort(key=lambda x: x[1], reverse=True)
    p1 = ModelPruning("l1_unstructured", amount=0.5, apply_pruning=lambda e: not e % 2, **pruning_kwargs)

    p2 = ModelPruning("random_unstructured", amount=0.25, apply_pruning=lambda e: e % 2, **pruning_kwargs)
@mock.patch.object(seed_utils, attribute="_select_seed_randomly", new=lambda *_: 123)

@mock.patch.object(seed_utils, attribute="_select_seed_randomly", new=lambda *_: 123)
        rst.sort(key=lambda x: x[2])
    stack = sorted(stack, key=lambda name: nodes[name].get_order())
    services.sort(key=lambda item: item[0])
        self.rules = sorted(rules, key=lambda rule: rule.match_score, reverse=True)
        sorted_matches = sorted(matches, key=lambda x: len(x[0]), reverse=True)
    return sorted(versions, key=lambda k: str(k.get("Version")))

        {"Aliases": sorted(region.lambdas.get(arn).aliases.values(), key=lambda x: x["Name"])}
        return FuncThread(func=_run_follow, on_stop=lambda *_: tailer.close())
        executions = sorted(response["executions"], key=lambda x: x["startDate"])

        events = sorted(result["events"], key=lambda event: event["timestamp"])
    streams = sorted(streams, key=lambda x: x["creationTime"], reverse=True)
        worker_nodes_by_id = sorted(self._worker_nodes, key=lambda w: w.id)

        self._worker_nodes = sorted(self._worker_nodes, key=lambda worker: (worker._index_within_host, worker.id))
            workers = sorted(workers, key=lambda w: w.client_id)
    logger.add(writer, format=lambda _: "{message}\n")

    logger.add(writer, format=lambda _: "{message}\n{exception}")
    logger.configure(patcher=lambda record: record["extra"].update(a=1, b=2))

    logger.configure(extra={"a": 1}, patcher=lambda r: r["extra"].update(b=2))

    logger.configure(patcher=lambda r: r.update(a=123))

    logger.configure(patcher=lambda r: None)
        >>> @logger.catch(onerror=lambda _: sys.exit(1))
    caster = dict(num=int, val=float, date=lambda d: datetime.strptime(d, "%Y-%m-%d %H:%M:%S"))
    logger.configure(patcher=lambda r: r["extra"].update(a=-1))

    logger.configure(patcher=lambda r: r["extra"].update(a=123, b=678))
    logger.add(writer, format=lambda x: "{message} {extra[trap]}", colorize=True, catch=False)

    logger.add(writer, format=lambda x: "{message} {extra[trap]}", colorize=False, catch=False)

    logger.add(writer, format=lambda r: "<red>{message}</red>", colorize=colorize)

    logger.add(writer, format=lambda _: "{time} \n")
    logger.add(print, filter=lambda r: True)

    logger.add(print, format=lambda r: "{message}")
        bool2str = [k for k, v in sorted(str2bool.items(), key=lambda item: item[1])]
        feature_df = feature_df.rename(columns=lambda c: c[len(of_name) + 1 :])
        files.sort(key=lambda x: int(filter_numeric(os.path.basename(x).split(".")[0])))
        tasks = sorted(tasks, key=lambda x: str(x))
        ordered_tasks = sorted(weighted_tasks, key=lambda pair: pair[0])
        params.sort(key=lambda t: t[1]._counter)
        workers.sort(key=lambda worker: worker['started'], reverse=True)
    def download(self, path, chunksize=None, chunk_callback=lambda _: False):
        for key, values in groupby(inputs, key=lambda x: self.internal_serialize(x[0])):
        tasks = sorted(tasks, key=lambda x: x.id)

        tasks = sorted(tasks, key=lambda x: x.id)

        tasks = sorted(tasks, key=lambda x: x.id)
            sorted(combined_overrides['containerOverrides'], key=lambda x: x['name']),

            sorted(combined_overrides['containerOverrides'], key=lambda x: x['name']),
all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)
  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)
  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)
  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)
  src_vars = list(sorted(src_vars, key=lambda v: v.name))

  dst_vars = list(sorted(dst_vars, key=lambda v: v.name))
    mine = sorted(mine, key=lambda v: v.name)

    theirs = sorted(theirs, key=lambda v: v.name)
d_sorted = sorted(w2i.keys(), key=lambda w: -d_avg[w2i[w]])

h_sorted = sorted(w2i.keys(), key=lambda w: -h_avg[w2i[w]])
    h = DenseLayer(M_in, 2 * M, f=lambda x: x)

    h = DenseLayer(M_in, D, f=lambda x: x)
        address = address.replace(sorted(zeros, key=lambda _: len(_))[-1], ":", 1)
    h = DenseLayer(M_in, 2 * M, f=lambda x: x)
            results = sorted(results, key=lambda _: _[1], reverse=True)
    directories = sorted(directories, key=lambda _: -1 if any(__ in _ for __ in ("suspicious", "malicious")) else int("custom" in _))

        filenames = sorted(filenames, key=lambda _: "history" in _)
            self.submobjects.sort(key=lambda m: point_to_num_func(m.get_center()))
        self.submobjects.sort(key=lambda m: m.get_tex())
    indexed_files.sort(key=lambda p: p[0])
    rings_sorted.sort(key=lambda x: area[x], reverse=True)
        for c_st in sorted(state.children, key=lambda k: len(states[k].children)):
            for mnemonic, count in sorted(ctx.items(), key=lambda x: x[1], reverse=True):
        return sorted(result, key=lambda m: m.start)
    def invoke(self, name="main", argv_generator=lambda s: []):

                self.invoke(name=name, argv_generator=lambda s: args)
    for pc, freq in sorted(list(db.items()), key=lambda x: -x[1]):
        fields.sort(key=lambda pair: pair[1]._creation_index)
            foo = fields.Int(validate=lambda n: n != 42)

            bar = fields.Int(validate=lambda n: n == 1)

            foo = fields.Int(required=True, validate=lambda n: n == 3)
        field = fields.Function(lambda x: None, deserialize=lambda val: val.upper())

        field = fields.Function(deserialize=lambda val: val.upper())

            serialize=lambda val: val.lower(), deserialize=lambda val: val.upper()

        field = fields.String(validate=lambda s: s.lower() == "valid")

        field = fields.String(validate=lambda s: None)

        field2 = fields.String(validate=lambda s: False)

        field = fields.String(validate=lambda s: False)

    age = fields.Integer(validate=lambda n: n > 0)

            foo = fields.Field(required=True, validate=lambda f: False)

        field = fields.Integer(validate=lambda x: 18 <= x <= 24)

        field = fields.String(validate=lambda n: len(n) == 3)

            lambda d: d.name.upper(), validate=lambda n: len(n) == 3

            y = fields.Integer(validate=lambda n: n > 0)

            lamb = fields.Raw(validate=lambda x: x is False)
        int_field = fields.Integer(validate=lambda x: True)
        foo = MyField(validate=lambda x: False)

        foo = fields.Str(validate=lambda x: len(x) > 3)

        bar = fields.Int(validate=lambda x: x > 3)
        field = fields.Function(serialize=lambda obj: obj.name.upper())

            serialize=lambda val: val.lower(), deserialize=lambda val: val.upper()

        field = fields.Function(serialize=lambda obj: obj.name.upper())

        field = fields.Function(deserialize=lambda obj: None)

        field = fields.Field(validate=lambda x: False)
wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data),
    managers.sort(key=lambda m: m.num)
            x, relposx = min(xpos, key=lambda v: abs(v[0] - x1))

            y, relposy = min(ypos, key=lambda v: abs(v[0] - y1))
def _rc_params_in_file(fname, transform=lambda x: x, fail_on_error=False):
            with cbook._setattr_cm(FigureManagerBase, show=lambda self: None):
    unique_authors = sorted(set(all_authors), key=lambda s: s.lower())
    for record in heapq.nlargest(num_largest, data, key=lambda rec: rec.size):

    for record in heapq.nlargest(num_largest, data, key=lambda rec: rec.n_allocations):
        for record in sorted(allocations, key=lambda alloc: alloc.size, reverse=True)[
        assert sorted(memory_snapshots, key=lambda r: r.time) == memory_snapshots

        assert sorted(memory_snapshots, key=lambda r: r.time) == memory_snapshots
            self.__class__.graph = property(fget=lambda _, flow=flow: flow._graph_info)

            setattr(self.__class__, k, property(fget=lambda _, v=v: v))
                    property(fget=lambda _, val=value: val, fset=_set_cls_var),
            return iter(sorted(children, reverse=True, key=lambda x: x.created_at))

                for m in sorted(origin_task.metadata, key=lambda m: m.created_at)

            m.name: m.value for m in sorted(self.metadata, key=lambda m: m.created_at)
            possible_names.sort(key=lambda x: -len(x[0]))  # group long options first
    rv.sort(key=lambda x: x[0])
        for test in sorted(iter_tests(), key=lambda x: x.PRIORITY)
        return list(sorted(steps, key=lambda x: x.prio))
        neighbors = sorted(((dist, target) for (dist, target) in zip(distances, self.y)), key=lambda x: x[0])
            pipeline.compute_batch(postprocessor=lambda df: df, pass_output_id=True)
            .assign(range_v1_v2=lambda x: x["v1"] - x["v2"])[["range_v1_v2"]]
        *create_test_dfs(data), loc=-1, value=lambda df: type(df)(df[df.columns[0]])
    eval_insert(modin_df, pandas_df, col="Bad Column", value=lambda df: df)
            suite.addTests(sorted(test_cases(all_tests), key=lambda x: x.__module__))
        full_result["writeErrors"].sort(key=lambda error: error["index"])
            return max(secondaries.server_descriptions, key=lambda sd: sd.last_write_date)
        servers = sorted(self._server_descriptions.values(), key=lambda sd: sd.address)
                sorted_expected_documents = sorted(expected_documents, key=lambda doc: doc["_id"])
        assert [x.name for x in query_result] == sorted(names, key=lambda x: x.lower())

        assert [x.name for x in query_result] == sorted(names, key=lambda x: x.lower())
        base_list.sort(key=lambda i: str(i))
        with trace_calls(collector, max_typed_dict_size=0, code_filter=lambda code: code.co_name == 'simple_add'):
                for stub in sorted(self.attribute_stubs, key=lambda stub: stub.name)

        for func_stub in sorted(self.function_stubs.values(), key=lambda s: s.name):

        for class_stub in sorted(self.class_stubs.values(), key=lambda s: s.name):
        cv = types.String(transformer=lambda value: value.lower())
        self.clips = sorted(self.clips, key=lambda clip: clip.layer)
    indexed_slices = sorted(enumerate(slices), key=lambda slice: slice[1][1].start)
        list.__init__(self, sorted(lst, key=lambda e: e.max_distance))

        starts_ends = sorted(dict_starts.items(), key=lambda k: k[0])
            search_result.sort(key=lambda ele: ele[1], reverse=True)
    return sorted(services, key=lambda p: p.get('name'))
    skills = sorted(skills, key=lambda p: basename(p['path']))
            sorted(expected_message_data, key=lambda d: sorted(d.items())))
                         sorted(result_list, key=lambda d: sorted(d.items())))

                         sorted(result_list, key=lambda d: sorted(d.items())))

                         sorted(result_list, key=lambda d: sorted(d.items())))
    simplified_operator_list.sort(key=lambda item: item[1][0])
                all_ids = sorted(ascc | viable, key=lambda id: graph[id].xmeta.data_mtime)

        return sorted(ascc, key=lambda id: -graph[id].order)
            a = sorted(errors[i0:i], key=lambda x: (x.line, x.column))
    for n, mem in sorted(memuse.items(), key=lambda x: -x[1]):
        sorted_locals = OrderedDict(sorted(type_map.items(), key=lambda t: t[0]))
        targets = sorted(get_all_leaf_targets(tree), key=lambda x: (x[1].line, x[0]))
        rows.sort(key=lambda x: x[0])

        rows.sort(key=lambda x: x[0])

        output_files = sorted(self.files, key=lambda x: x.module)

        output_files = sorted(self.files, key=lambda x: x.module)
    patches_by_priority = sorted(patches, key=lambda x: x[0])
        return list(sorted(self.signatures, key=lambda x: 1 if args_kwargs(x) else 0))
    items = sorted(module.__dict__.items(), key=lambda x: x[0])

    items = sorted(obj_dict.items(), key=lambda x: method_name_sort_key(x[0]))
                new_items.sort(key=lambda lit: lit.value)
        kw_only = sorted(self.kwonly.values(), key=lambda a: (has_default(a), get_name(a)))
    root = max(comparison_methods, key=lambda k: (comparison_methods[k] is None, k))
            all_attrs.sort(key=lambda a: a.kw_only)
    for trigger, targets in sorted(all_deps.items(), key=lambda x: x[0]):
        for id, nodes in sorted(todo.items(), key=lambda x: x[0]):
        for name, node in sorted(names.items(), key=lambda x: x[0]):

                for expr in sorted(type_map, key=lambda n: (n.line, short_type(n),
        lines.append('exits: %s' % sorted(self.exits, key=lambda e: e.label))
    for name, (slot, generator) in sorted(table.items(), key=lambda x: slot_key(x[0])):
        return sorted(concrete, key=lambda c: (len(c.children or []), c.name))
        s = sorted(attrs, key=lambda x: slot_key(x))
    failures = sorted(failures, key=lambda e: extract_line(e[2]))
                     for reg in sorted(decref, key=lambda r: ordering[r])

                     for reg in sorted(incref, key=lambda r: ordering[r])
        sen_counts = sorted(sen_counts, key=lambda kv: kv[1], reverse=True)

                list(vocab.items()), key=lambda kv: kv[1], reverse=True)
        vocab_sorted = sorted(word_count.items(), key=lambda kv: kv[1], reverse=True)
    child_prefixes.sort(key=lambda p: p.prefix)

    vlans.sort(key=lambda v: v.vid if type(v) == VLAN else v['vid'])
        for prefix, viewset, basename in sorted(self.registry, key=lambda x: x[0]):
            field_info['choices'].sort(key=lambda item: item['display_name'])
    def __init__(self, label, choices, default=None, description='', coerce=lambda x: x):
        z = max(unnumbered_nodes, key=lambda node: weight[node])
        subgraph_hash_counts.extend(sorted(counter.items(), key=lambda x: x[0]))
        mincomp = min(strongcomp, key=lambda ns: min(ordering[n] for n in ns))
    >>> list(nx.lexicographical_topological_sort(DG, key=lambda x: -x))

        maxu = max(us, key=lambda x: x[0]) if us else (0, v)

    v = max(dist, key=lambda x: dist[x][0])
    u = max(subg, key=lambda u: len(cand & adj[u]))

                        u = max(subg, key=lambda u: len(cand & adj[u]))

        u = max(subg, key=lambda u: len(cand & adj[u]))

        nodes = sorted(self.G.nodes(), key=lambda v: self.G.degree(v), reverse=True)
        yield from sorted(other, key=lambda t: t[4] + t[1].ls + t[3].ls)
            (u, v, min(G[u][v], key=lambda k: G[u][v][k][weight])) for u, v in edges
        (u, v) = max(pairwise(best_GG), key=lambda x: dist[x[0]][x[1]])

        min_in_edge = min(G.in_edges(n, data=True), key=lambda x: x[2][weight])

            min_weight = min(G.in_edges(n, data=weight), key=lambda x: x[2])[2]

        next_node = min(nodeset, key=lambda n: nbrdict[n].get(weight, 1))
        n = max(G.nodes, key=lambda x: vote_rank[x][0])
    >>> print(sorted(soc.items(), key=lambda x: x[1])[0][0])  # pick first id
            node = max(saturation, key=lambda v: (saturation[v], G.degree(v)))
        mu: sorted(mapped, key=lambda u: (G.degree(u), u))
        ismags = iso.ISMAGS(g2, g1, node_match=lambda x, y: x == y)

        ismags = iso.ISMAGS(g2, g1, edge_match=lambda x, y: x == y)
            start_sgn = min(candidates, key=lambda n: min(candidates[n], key=len))

                next_sgn = min(nodes, key=lambda n: min(candidates[n], key=len))
        [label for label, _ in sorted(label_to_id.items(), key=lambda x: x[1])]
        lb = nx.local_bridges(G, weight=lambda u, v, d: 2)
        assert list(nx.lexicographical_topological_sort(G, key=lambda x: x)) == [

        assert list(nx.lexicographical_topological_sort(G, key=lambda x: -x)) == [

    #     left, right = sorted(children, key=lambda v: B.node[v]['label'])
                tuple(sorted(edge_path, key=lambda x: (None in x, x))),
        node_labels = sorted(node_labels, key=lambda n: sorted(G.nodes[n]["group"])[0])
    edges = sorted(edges, key=lambda x: (x[2], x[1], x[0]))
    >>> D = nx.gn_graph(10, kernel=lambda x: x ** 1.5)  # A_k = k^1.5
                    min(cc, key=lambda n: graph.degree[n])
core = sorted(resp, key=lambda user: user["login"].lower())

emeritus = sorted(resp, key=lambda user: user["login"].lower())
        func = rl.agents.TorchAgentFunction(agents[archi], runner, reward_postprocessing=lambda x: 1 - x)
    handlederrordf = df.select(error=lambda x: isinstance(x, str) and x, loss=lambda x: not np.isnan(x))

        sorted_optimizers = sorted(optim_vals, key=lambda x: optim_vals[x]["loss"][-1], reverse=True)
        self.archive = sorted(self.archive, key=lambda trace: -len(trace[0]))
            order = sorted(range(len(price)), key=lambda x: price[x])  # pylint: disable=cell-var-from-loop
    model.Constraint1 = pyomo.Constraint(rule=lambda m: m.x[0] >= 1)

    model.Constraint2 = pyomo.Constraint(rule=lambda m: m.x[1] >= 0.8)

    abstract_model.constraints = pyomo.Constraint(abstract_model.F, rule=lambda m, i: m.x[i] >= m.Xmin[i])
    model.Constraint1 = pyomo.Constraint(rule=lambda m: m.x[0] >= 1)

    model.Constraint2 = pyomo.Constraint(rule=lambda m: m.x[1] >= 0.8)

    abs_model.constraints = pyomo.Constraint(abs_model.F, rule=lambda m, i: m.x[i] >= m.Xmin[i])

    model.constraint1 = pyomo.Constraint(rule=lambda m: m.x >= 2)
                uid, worst = max(self.population.items(), key=lambda p: base._loss(p[1]))
                best = min(self.archive.values(), key=lambda mv, n=name: mv.get_estimation(n))  # type: ignore
        return np.frombuffer(min(my_keys, key=lambda x: archive.bytesdict[x].pessimistic_confidence_bound))
    first_k_individuals = sorted(items, key=lambda indiv: archive[indiv[0]].get_estimation("average"))[:k]
                sorted(items, key=lambda indiv: archive[indiv[0]].get_estimation("pessimistic")), axis=0

        # return hull_center(np.concatenate(sorted(items, key=lambda indiv: archive[indiv[0]].get_estimation("pessimistic")), axis=0), k)

    first_k_individuals = sorted(items, key=lambda indiv: archive[indiv[0]].get_estimation("pessimistic"))[:k]
            uid, worst = max(self.population.items(), key=lambda p: base._loss(p[1]))
        winners = sorted(x, key=lambda x_: np.linalg.norm(x_.value - np.array((1.0, 1.0, 1.0))))
    for param in sorted(optimizer.pareto_front(), key=lambda p: p.losses[0]):
            front = sorted(front, key=lambda x: x.losses[i])

            front = sorted(front, key=lambda x: x.loss)  # type: ignore

        return sorted(candidates, key=lambda elem: elem._meta["crowding_distance"], reverse=True)
    candidates.sort(key=lambda x: rank_result[x.uid][0] if x.uid in rank_result else float("inf"))
        return sorted(node_list, key=lambda node: node.coordinates[dimension_index])
    summaries.sort(key=lambda summary: summary[0])
    sites_available = sorted(sites_available, key=lambda _: _['name'])
    def _apply_filter(self, fn=lambda ngram, freq: False):

        return sorted(self._score_ngrams(score_fn), key=lambda t: (-t[1], t[0]))
    top_elt[0].extend(sorted(packages, key=lambda package: package.get("id")))

    top_elt[1].extend(sorted(collections, key=lambda collection: collection.get("id")))
        binditems = sorted(bindings.items(), key=lambda v: v[0].name)
    def __init__(self, tokens, context_func=None, filter=None, key=lambda x: x):

    def __init__(self, tokens, key=lambda x: x):

                self.tokens, filter=lambda x: x.isalpha(), key=lambda s: s.lower()
        key for key, value in sorted(fd.items(), key=lambda item: item[1], reverse=True)
        luinfo["lexemes"].sort(key=lambda x: x.order)
    def _read_instance_block(self, stream, instance_filter=lambda inst: True):
    def _read_instance_block(self, stream, instance_filter=lambda inst: True):
            tags = sorted(tags, key=lambda v: -sum(self._confusion[self._indices[v]]))
        for node in sorted(self.nodes.values(), key=lambda v: v["address"]):
    for (parser, t) in sorted(times_items, key=lambda a: a[1]):
        stanford_jar = max(jars, key=lambda model_name: re.match(self._JAR, model_name))
        parses.sort(reverse=True, key=lambda tree: tree.prob())

        queue.sort(key=lambda edge: edge.prob())

        queue.sort(key=lambda edge: edge.length())
            + sorted(event_vars, key=lambda v: int([v[2:], -1][len(v[2:]) == 0]))

            + sorted(func_vars, key=lambda v: (v[0], int([v[1:], -1][len(v[1:]) == 0])))

            + sorted(ind_vars, key=lambda v: (v[0], int([v[1:], -1][len(v[1:]) == 0])))
        for i, e in enumerate(sorted(get_indiv_vars(self), key=lambda e: e.variable)):
        best_label = max(self.classes, key=lambda label: (scores[label], label))

            tag, mode = max(tag_freqs.items(), key=lambda item: item[1])
        self.src_phrases[src_phrase].sort(key=lambda e: e.log_prob, reverse=True)
            n_match, n_all = max(hyp_counts, key=lambda hc: hc[0] / hc[1])
        self.items.sort(key=lambda h: h.score(), reverse=True)
            a.sort(key=lambda n: min(n.leaves()) if isinstance(n, Tree) else n)

            levels[n].sort(key=lambda n: max(tree[n].leaves()) - min(tree[n].leaves()))
    return json.dumps(graph, default=lambda obj: obj.__dict__)
    return list(sorted(queue.entries, key=lambda x: -x[0]))
    return json.dumps(graph, default=lambda obj: obj.__dict__)
    return list(sorted(queue.entries, key=lambda x: -x[0]))
    return json.dumps(graph, default=lambda obj: obj.__dict__)
    return json.dumps(graph, default=lambda obj: obj.__dict__)
        min_gm_kernels = sorted(dist_list, key=lambda x: x[0])[:num_prune]
        op_names = [k for k, _ in sorted(self.weights_numel.items(), key=lambda item: item[1]) if k in config['op_names']]
        self.finished = sorted(self.finished, key=lambda x: x.score, reverse=reverse)
                sorted_perf = sorted(this_round_perf.items(), key=lambda kv: kv[1][1], reverse=True)  # reverse

                sorted_perf = sorted(this_round_perf.items(), key=lambda kv: kv[1][1])
            candidate = max(sample, key=lambda x: x.result)
                    this_round_perf.items(), key=lambda kv: kv[1][1])
            [item[1] for item in sorted(pbounds.items(), key=lambda x: x[0])]
            vals_new.append(min(bound['_value'], key=lambda x: abs(x - vals[i])))
            vals_new.append(min(vals_bounds[i], key=lambda x: abs(x - vals[i])))
            return max(self.history, key=lambda x: x["metric_value"])[

        return min(self.history, key=lambda x: x["metric_value"])["model_id"]
            all_models = sorted(all_models, key=lambda m: cast(float, m.metric), reverse=optimize_mode == 'maximize')
            self.kernel_size_candidates = sorted(candidates, key=lambda t: t[0], reverse=True)
            parent = max(samples, key=lambda sample: sample.y)

            parent = min(samples, key=lambda sample: sample.y)
        sorted(intermediate_results, key=lambda x: x['timestamp'])
def field_map(path, python_to_api=lambda x: x, api_to_python=lambda x: x):
    return sorted(uncompiled_modules, key=lambda module: module.getFullName())

    return sorted(result, key=lambda module: module.getFullName())

    return sorted(result, key=lambda module: module.getFullName())

    return sorted(done_modules, key=lambda module: (module.getFullName(), module.kind))
    for module in sorted(modules, key=lambda x: x.getFullName()):
        manifestFileInfos.sort(key=lambda t: t[0].st_atime, reverse=True)

        objectInfos.sort(key=lambda t: t[0].st_atime)
                         sorted(operators, key=lambda x: -len(x))))
                         sorted(operators, key=lambda x: -len(x))))
                spawn = env.subst(spawn, raw=1, conv=lambda x: x)

    def __init__(self, actfunc, strfunc, convert=lambda x: x):
def _concat(prefix, list, suffix, env, f=lambda x: x, target=None, source=None):
l = sorted(CPP_to_Python_Ops_Dict.keys(), key=lambda a: len(a), reverse=True)
    for n in sorted(StatsNodes, key=lambda a: str(a)):

    def will_not_build(self, nodes, node_func=lambda n: None):
        for node in sorted(self.children(), key=lambda t: t.name):

        return sorted(result, key=lambda a: str(a))
            #TODO 2.4: self.sources[n].sort(key=lambda a: a.lower())

            self.sources[n] = sorted(self.sources[n], key=lambda a: a.lower())

        for kind in sorted(categories.keys(), key=lambda a: a.lower()):

        sorteditems = sorted(hierarchy.items(), key=lambda a: a[0].lower())
            options = sorted(self.options, key=lambda x: x.key)
                spawn = env.subst(spawn, raw=1, conv=lambda x: x)

    def __init__(self, actfunc, strfunc, convert=lambda x: x):
l = sorted(list(CPP_to_Python_Ops_Dict.keys()), key=lambda a: len(a), reverse=True)
def _concat(prefix, list, suffix, env, f=lambda x: x, target=None, source=None):
    for n in sorted(StatsNodes, key=lambda a: str(a)):

    def will_not_build(self, nodes, node_func=lambda n: None):
    for n in sorted(node.children(), key=lambda t: t.name):
        return sorted(result, key=lambda a: str(a))
            self.sources[n].sort(key=lambda a: a.lower())

        for kind in sorted(list(categories.keys()), key=lambda a: a.lower()):

        sorteditems = sorted(hierarchy.items(), key=lambda a: a[0].lower())
            options = sorted(self.options, key=lambda x: x.key)
                spawn = env.subst(spawn, raw=1, conv=lambda x: x)

        cmd_list = env.subst_list(self.cmd_list, SUBST_SIG, conv=lambda x: x)

    def __init__(self, actfunc, strfunc, convert=lambda x: x):
l = sorted(list(CPP_to_Python_Ops_Dict.keys()), key=lambda a: len(a), reverse=True)
def _concat(prefix, items_iter, suffix, env, f=lambda x: x, target=None, source=None, affect_signature=True):
    for n in sorted(StatsNodes, key=lambda a: str(a)):

    def will_not_build(self, nodes, node_func=lambda n: None):
        return sorted(result, key=lambda a: str(a))
    for n in sorted(node.children(), key=lambda t: t.name):
            self.sources[n].sort(key=lambda a: a.lower())

        for kind in sorted(categories.keys(), key=lambda a: a.lower()):

        sorteditems = sorted(hierarchy.items(), key=lambda a: a[0].lower())
    pch_subst = env.get('PCH', False) and env.subst('$PCH',target=target, source=source, conv=lambda x:x)
            options = sorted(self.options, key=lambda x: x.key)
                    inst = min(instances, key=lambda i: i.pos)
    for variable in sorted(temp_variables, key=lambda variable: variable.getName()):
        candidates = sorted(candidates, key=lambda c: (c.search_order, c.priority))
    p.sort(key=lambda x: x[1], reverse=True)
    # new_data = sorted(new_data, key=lambda d: d["module-name"].lower())
def defaultValueTest4(no_default, funced_defaulted=lambda x: x ** 2):
def defaultValueTest4(_no_default, funced_defaulted=lambda x: x ** 2):

def defaultValueTest4a(_no_default, funced_defaulted=lambda x: x ** 2):

def defaultValueTest4b(_no_default, funced_defaulted=lambda x: x ** 3):
        for state in sorted(runner.finished, key=lambda x: x.pc_initial):
        ordered = sorted(candidates.keys(), key=lambda x: genericity[x])
        for loop in sorted(loops.values(), key=lambda loop: len(loop.body)):
                symbols = sorted(sec.iter_symbols(), key=lambda sym: sym.name)
            for k, s in sorted(sized_loops, key=lambda tup: tup[1], reverse=True):
        def __init__(self, flag_name, apply=lambda x: x):
            argtypes = tuple(recur_tuplize(args, func=lambda x: x.type))
        for name, infos in sorted(fields, key=lambda x: (x[1]['offset'], x[0])):

        ordered = sorted(self.fields.items(), key=lambda x: x[1].offset)
        self.types = tuple(sorted(set(types), key=lambda x: x.name))
        candidates.sort(key=lambda i: i[0])
    strideperm.sort(key=lambda x: x[1])
    strideperm.sort(key=lambda x: x[1])
def make_optional_return_case(jit=lambda x: x):

def make_growing_tuple_case(jit=lambda x: x):
    expr_var_unique = sorted(set(expr_var_list), key=lambda var: var.name)
        for fn, fname in sorted(map(format_fname, fninfos), key=lambda x: x[1]):
    tests = sorted(tests, key=lambda case: case.id())
        for pf in sorted(self.initial_parfors, key=lambda x: x.loc.line):

        for loop, s in sorted(sized_loops, key=lambda tup: tup[1]):
def make_type_change_self(jit=lambda x: x):

def make_mutual2(jit=lambda x: x):

def make_type_change_mutual(jit=lambda x: x):

def make_four_level(jit=lambda x: x):

def make_inner_error(jit=lambda x: x):

def make_raise_mutual(jit=lambda x: x):

def make_optional_return_case(jit=lambda x: x):

def make_growing_tuple_case(jit=lambda x: x):
        for (k, line_no) in zip(sorted(line2dbg, key=lambda x: int(x[1:])),
        def factory(decor=lambda x: x):
        zip_sorted = sorted(zip(orig, orig_values), key=lambda x: x[0])
            lst.sort(key=lambda x: -x)

            lst.sort(key=lambda arr: np.sum(arr))
        field_data = sorted(field_data, key=lambda f: f[0])
            path = min(full_results, key=lambda x: x[0])[1]

    path = min(full_results, key=lambda x: x[0])[1]

        best = min(known_contractions, key=lambda x: x[0])
    allfields.sort(key=lambda x: x[2])
             dict(__array__=lambda *x: np.array(100.0, dtype=np.float64)))()
    check_may_share_memory_easy_fuzz(get_max_work=lambda a, b: max(a.size, b.size)//2,
    T = property(fget=lambda self: self.transpose())
    files.sort(key=lambda name: (name.endswith('.pxi') or
    jobs.sort(key=lambda job: job.length, reverse=True)
        str(p) for p in unique_everseen(shim_paths, key=lambda p: str.casefold(str(p)))
        map(lambda x: x.as_dict(), users), key=lambda x: sv(x.get("name"))
                    sorted(self.plugins.values(), key=lambda x: str(x).lower()),
               return dict(some_key=lambda x: x.upper()),        # getter preprocessors

                      dict(some_other_key=lambda x: x.lower())   # setter preprocessors
                    sorted(self._update_log, key=lambda x: x["datetime"]),
        plugins = sorted(self._get_plugins(), key=lambda x: x["name"].lower())
            data.sort(key=lambda x: x["time"])
        roles = sorted(current_user.permissions, key=lambda x: x.key)
            for route in sorted(cache_data.keys(), key=lambda x: (x.count("/"), x)):

                    sorted(cache_data[route], key=lambda x: x.get("_count", 0))
        hash_update(repr(sorted(printer.get_sd_files(), key=lambda x: sv(x["name"]))))
    return sorted(data, key=lambda item: item.get('subdomain'))
    ranges = sorted(ranges, key=lambda x: x[0])

        self.file_info["files"].sort(key=lambda k: k["basename"])

        self.file_info["dirs"].sort(key=lambda k: k["basename"])
                    bridges.sort(key=lambda s: s.split()[1])
        results.sort(key=lambda x: -x["ac_info"]["ac_time"])
            resp["data"].sort(key=lambda x: int(x["test_case"]))
    arg_dict = dict(sorted(arg_dict.items(), key=lambda item: item[0]))

    mount_dict = dict(sorted(mount_dict.items(), key=lambda item: item[0]))

    media_dict = dict(sorted(media_dict.items(), key=lambda item: item[0]))

    cache_data = dict(sorted(cachedata.items(), key=lambda item: item[0].source_filename))
        aliases = dict(sorted(aliases.items(), key=lambda item: item[1]))
                sorted(metric_data.items(), key=lambda t: t[1][0], reverse=True)
            sorted(companies_per_country.items(), key=lambda t: t[1], reverse=True)
            sorted(companies_per_sector.items(), key=lambda t: t[1], reverse=True)
@bot.message_handler(func=lambda m: m.text[0] == "/")
        sectors = dict(sorted(sectors.items(), key=lambda x: x[1], reverse=True))
                    sorted(weights.items(), key=lambda x: x[1], reverse=True)
        dict(sorted(d_ats_reg.items(), key=lambda item: item[1], reverse=True)).keys()
        biggest = max(options, key=lambda x: x["strike"])

        smallest = min(options, key=lambda x: x["strike"])
                sorted(metric_data.items(), key=lambda t: t[1][0], reverse=True)

            sorted(companies_per_sector.items(), key=lambda t: t[1], reverse=True)

            sorted(companies_per_industry.items(), key=lambda t: t[1], reverse=True)

            sorted(companies_per_industry.items(), key=lambda t: t[1], reverse=True)

            sorted(companies_per_country.items(), key=lambda t: t[1], reverse=True)

            sorted(companies_per_country.items(), key=lambda t: t[1], reverse=True)
        runs = sorted(beam_job_runs, key=lambda j: j.job_id)
        sorted(blog_post_summaries, key=lambda k: k.last_updated, reverse=True)
        messages.sort(key=lambda m: m.html)

        sent_email_models.sort(key=lambda m: m.html_body)

        messages.sort(key=lambda m: m.html)

        sent_email_models.sort(key=lambda m: m.html_body)
        questions.sort(key=lambda question: question.last_updated)

        questions.sort(key=lambda question: question.last_updated)
        all_users = sorted(self._users_by_uid.values(), key=lambda u: u.uid)
    updated_list = sorted(updated_list, key=lambda s: s.lower())

            list(set(all_developer_names)), key=lambda s: s.lower())
        sorted_wins = [k for k, _ in sorted(wins.items(), key=lambda x: x[1])]
            list(set(expected_developer_names)), key=lambda s: s.lower())
        sorted(distributions.items(), key=lambda name_and_distribution: name_and_distribution[0])
        population.sort(key=lambda x: cast(float, x.values[i]))

    population.sort(key=lambda x: manhattan_distances[x.number])
                    best_trial = min(trials, key=lambda t: cast(float, t.value))

                    best_trial = max(trials, key=lambda t: cast(float, t.value))
        population.sort(key=lambda x: cast(float, x.values[i]))

    population.sort(key=lambda x: manhattan_distances[x.number])
            search_space = OrderedDict(sorted(search_space.items(), key=lambda x: x[0]))
        first_trial = min(past_trials, key=lambda t: t.number)
            trials = list(sorted(trials.values(), key=lambda t: t.number))
            best_trial = max(all_trials, key=lambda t: cast(float, t.value))

            best_trial = min(all_trials, key=lambda t: cast(float, t.value))
                best_trial = max(all_trials, key=lambda t: cast(float, t.value))

                best_trial = min(all_trials, key=lambda t: cast(float, t.value))
    pareto_front.sort(key=lambda trial: trial.number)
                ticktext = list(sorted(vocab.keys(), key=lambda x: vocab[x]))
            vocab_item_sorted = sorted(vocab.items(), key=lambda x: x[1])
    param_importance_without_inf = evaluator.evaluate(study, target=lambda t: t.values[target_idx])

    param_importance_with_inf = evaluator.evaluate(study, target=lambda t: t.values[target_idx])
    param_importance_without_inf = evaluator.evaluate(study, target=lambda t: t.values[target_idx])

    param_importance_with_inf = evaluator.evaluate(study, target=lambda t: t.values[target_idx])
    param_importance_without_inf = evaluator.evaluate(study, target=lambda t: t.values[target_idx])

    param_importance_with_inf = evaluator.evaluate(study, target=lambda t: t.values[target_idx])
    with patch("optuna.Study.get_trials", new=Mock(side_effect=lambda deepcopy: _create_trials())):
        new=mock.Mock(side_effect=lambda study, trial: trial.number % n_brackets),
    with patch("optuna.Study.get_trials", new=Mock(side_effect=lambda deepcopy: _create_trials())):
        sampler = NSGAIISampler(constraints_func=lambda _: [0])

        sampler = NSGAIISampler(constraints_func=lambda _: [0])

        sampler = NSGAIISampler(constraints_func=lambda _: [0])

        sampler = NSGAIISampler(constraints_func=lambda _: [0])

        NSGAIISampler(constraints_func=lambda _: [0])
    sampler = TPESampler(gamma=lambda _: 1, seed=0)

    sampler = TPESampler(weights=lambda n: np.zeros(n), seed=0)

    sampler = TPESampler(constraints_func=lambda trial: trial.user_attrs["constraint"], seed=0)
        figure = plot_edf(study0, target=lambda t: t.params["x"])

    figure = plot_edf(study, target=lambda t: t.values[objective])
    plot_contour(study, target=lambda t: t.values[0])

    plot_contour(study, target=lambda t: t.values[0])

        figure = plot_contour(study, params=params, target=lambda t: t.params["param_d"])

    contour = plot_contour(study, target=lambda t: t.number).data[0]

    contour = plot_contour(study, target=lambda t: t.number).data[0]
        optuna.samplers.TPESampler(constraints_func=lambda _: (0,))

        sampler = TPESampler(gamma=lambda _: 5, n_startup_trials=5, seed=0, multivariate=True)

    sampler = TPESampler(gamma=lambda _: 5, n_startup_trials=5, seed=0)

    sampler = TPESampler(n_startup_trials=0, seed=2, constraints_func=lambda _: (0,))

    sampler = TPESampler(constraints_func=lambda trial: trial.user_attrs["constraint"])

    sampler = TPESampler(constraints_func=lambda trial: trial.user_attrs["constraint"])
        figure = plot_optimization_history(study, target=lambda t: t.number)

        figure = plot_optimization_history(studies, target=lambda t: t.number)

        figure = plot_optimization_history(studies, target=lambda t: t.number, error_bar=True)
        figure = plot_slice(study, params=["param_a"], target=lambda t: t.params["param_b"])
        figure = plot_edf(study0, target=lambda t: t.params["x"])

    figure = plot_edf(study, target=lambda t: t.values[objective])
    line = plot_parallel_coordinate(study, target=lambda t: t.number).data[0]["line"]

    line = plot_parallel_coordinate(study, target=lambda t: t.number).data[0]["line"]

    figure = plot_parallel_coordinate(study, target=lambda t: t.values[objective])
    plot_contour(study, target=lambda t: t.values[0])

    plot_contour(study, target=lambda t: t.values[0])

        figure = plot_contour(study, params=params, target=lambda t: t.params["param_d"])

    plot_contour(study, target=lambda t: t.values[objective])
        figure = plot_optimization_history(study, target=lambda t: t.number)

        figure = plot_optimization_history(studies, target=lambda t: t.number)

        figure = plot_optimization_history(studies, target=lambda t: t.number, error_bar=True)
    plot_parallel_coordinate(study, target=lambda t: t.values[objective])
        figure = plot_slice(study, params=["param_a"], target=lambda t: t.params["param_b"])
trial_with_highest_accuracy = max(study.best_trials, key=lambda t: t.values[1])
    plot_param_importances(study, evaluator=evaluator, target=lambda t: t.values[target_idx])

    plot_param_importances(study, evaluator=evaluator, target=lambda t: t.values[target_idx])
        all_ep.sort(key=lambda x: x.name)
        for format_ in sorted(cls.registry.values(), key=lambda x: x.PRIORITY):
            ContinuousVariable("x", compute_value=lambda *_: 42)
        scores = [i[0] for i in sorted(chain(best, worst), key=lambda i: i[1])]
        self.assertRaises(ValueError, Validation, preprocessor=lambda x: x)

        self.assertRaises(ValueError, Validation, callback=lambda x: x)
                for l, group in groupby(labels_attrs, key=lambda x: x[0])]
        items = unique_everseen(items, key=lambda t: t[0])

    hspec = sorted(opts.rowspec, key=lambda t: t[0].start)
        writers.sort(key=lambda writer: cls.builtin_order.index(writer)
            return sorted(selected_attrs, key=lambda attr: domain_hints[attr][1])
        na3 = a3.copy(compute_value=lambda *_: 3)

        na4 = a4.copy(compute_value=lambda *_: 4)

        nc1 = c1.copy(compute_value=lambda *_: 5)

        ma3 = a3.copy(compute_value=lambda x: 6)

        ma4 = a4.copy(compute_value=lambda x: 7)
    @patch("os.path.exists", new=lambda _: True)

    @patch("os.path.exists", new=lambda _: True)
    @patch("os.path.exists", new=lambda x: x == "old.tab")
    return sorted(usable, key=lambda cls: order.get(cls.name, 99))
                [ContinuousVariable(name, compute_value=lambda _: None)
        selection = sorted(selection, key=lambda c: c.value.first)
    g = groupby(enumerate(indices), key=lambda t: t[1] - t[0])
                    to_add = sorted(to_add, key=lambda x: x.name)
                         sorted(attrs + metas, key=lambda x: x.name) +

                         sorted(classes, key=lambda x: x.name))
        results = sorted(zip(weights, domain.attributes), key=lambda x: (-x[0], x[1].name))
        attrs = sorted(zip(weights, attrs), key=lambda x: (-x[0], x[1].name))
                sorted_rules = sorted(rules[:-1], key=lambda rule: rule.attr_name)

    def __init__(self, weight_adjustment=lambda x: x):
                    + sorted(fonts, key=lambda s: s.replace(".", "")))
    most_recent = max(method_infos, key=lambda x: x["metadata"]["timestamp"])
    first_pipeline_runs.sort(key=lambda x: x["pipeline_run_index"])

    pipeline_runs.sort(key=lambda x: x["pipeline_run_index"])
            data["entries"].sort(key=lambda x: -x.get("stargazers_count", -1))
        self.assertEqual(orjson.dumps(Custom(), default=lambda x: None), b"null")

            orjson.dumps(ref, default=lambda x: str(x)),
    uvk = ((u, v, min(G[u][v], key=lambda k: G[u][v][k]["length"])) for u, v in node_pairs)
        sort_addresses = sorted(address.items(), key=lambda p: p[1], reverse=True)

            ssort = sorted(hashtag_counter.items(), key=lambda value: value[1], reverse=True)

            ssort = sorted(users, key=lambda value: value['counter'], reverse=True)

            ssort = sorted(users, key=lambda value: value['counter'], reverse=True)

            ssort = sorted(users, key=lambda value: value['counter'], reverse=True)
        data = min(G.get_edge_data(u, v).values(), key=lambda d: d["length"])
        data = min(G.get_edge_data(u, v).values(), key=lambda x: x[minimize_key])

        k_min, _ = min(G.get_edge_data(u, v).items(), key=lambda x: x[1][weight])
        word_freq_sorted = sorted(word_freq, key=lambda x: (-x[1], x[0]))
    dictionary = sorted(word_freq, key=lambda x: (-x[1], x[0]))
            tensor_nodes.sort(key=lambda node: node.node.original_desc_id())

            tensor_nodes.sort(key=lambda node: node.node.original_desc_id())
        return sorted(self._records.values(), key=lambda r: r.step)
                    rank_params.sort(key=lambda x: np.prod(x.shape))
        sorted_varsize = sorted(varsize_count.items(), key=lambda x: x[0])
                    rank_params.sort(key=lambda x: x._numel())
        for param in sorted(list(self._unslice_params), key=lambda p: p.name):
        self._trainable_params.sort(key=lambda x: x._numel())
        self._trainable_params.sort(key=lambda x: np.prod(x.shape))
        for param in sorted(list(self._unslice_params), key=lambda p: p.name):
    insert_idx_and_kwargs.sort(key=lambda element: element[0], reverse=True)
        sorted_checkpoints = sorted(sorted_checkpoints, key=lambda x: x[1])
        params_grads = sorted(params_grads, key=lambda x: x[0].name)

        params_grads = sorted(params_grads, key=lambda x: x[0].name)

        params_grads = sorted(params_grads, key=lambda x: x[0].name)
def _accumulate(iterable, fn=lambda x, y: x + y):
                scheduler = fluid.dygraph.LambdaDecay(0.5, lr_lambda=lambda x: 0.95**x)
        tmp_list.sort(key=lambda k: k[1])
    match_sorted = sorted(match_pair, key=lambda tup: tup[2], reverse=True)
    params1.sort(key=lambda x: x.name)

    params2.sort(key=lambda x: x.name)
            sorted_list = sorted(pos_list, key=lambda pos: pos[0], reverse=True)
            v0 = sorted(list(six.iteritems(v)), key=lambda x: x[0])

            v1 = sorted(list(six.iteritems(b.__dict__[k])), key=lambda x: x[0])
        p_g = sorted(p_g, key=lambda x: x[0].name)

        p_g_clip = sorted(p_g_clip, key=lambda x: x[0].name)
        prof = profiler.Profiler(on_trace_ready=lambda prof: None)

        prof = profiler.Profiler(on_trace_ready=lambda prof: None)
        arr_list.sort(key=lambda x: x[0])
        np_tuple.sort(key=lambda x: x[1])

        np_tuple.sort(key=lambda x: x[1])
        np_tuple.sort(key=lambda x: x[1])

        np_tuple.sort(key=lambda x: x[1])
                new_cache = sorted(b_src, key=lambda k: len(k[0]))

            new_cache = sorted(b_src, key=lambda k: len(k[0]))
        o_params = sorted(o_block.all_parameters(), key=lambda p: p.name)

        c_params = sorted(c_block.all_parameters(), key=lambda p: p.name)

        o_vars = sorted(o_block.vars.values(), key=lambda v: v.name)

        c_vars = sorted(c_block.vars.values(), key=lambda v: v.name)

        origin_vars = sorted(origin_vars, key=lambda v: v.name)

        converted_vars = sorted(converted_vars, key=lambda v: v.name)
                    sorted(arr.reshape((-1, 6)), key=lambda i: [i[0], i[1]]))
            scheduler = paddle.optimizer.lr.LambdaDecay(learning_rate=0.5, lr_lambda=lambda x:0.95**x, verbose=True)

                scheduler = paddle.optimizer.lr.LambdaDecay(learning_rate=0.5, lr_lambda=lambda x:0.95**x, verbose=True)

            scheduler = paddle.optimizer.lr.MultiplicativeDecay(learning_rate=0.5, lr_lambda=lambda x:0.95, verbose=True)
        params_grads = sorted(params_grads, key=lambda x: x[0].name)
            src_ranges.sort(key=lambda x: x[0])
            word_freq_sorted = sorted(word_freq, key=lambda x: (-x[1], x[0]))
        dictionary = sorted(word_freq, key=lambda x: (-x[1], x[0]))
        case_mem_1_sort = sorted(case_mem_1.items(), key=lambda x: x[1])
            mem_list.sort(key=lambda tmp: (tmp.get('time', 0)))
            mem_list.sort(key=lambda tmp: (tmp.get('time', 0)))
@pytest.fixture(params=[0, 1, "index", "columns"], ids=lambda x: f"axis={repr(x)}")

@pytest.fixture(params=[1, "columns"], ids=lambda x: f"axis={repr(x)}")

@pytest.fixture(params=tm.NULL_OBJECTS, ids=lambda x: type(x).__name__)

@pytest.fixture(params=tm.NP_NAT_OBJECTS, ids=lambda x: type(x).__name__)

@pytest.fixture(params=[Index, Series, DataFrame, pd.array], ids=lambda x: x.__name__)
        pair[0] for pair in sorted(zip(columns, order), key=lambda t: t[1])
        >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)

        >>> df.sort_index(key=lambda x: x.str.lower())
        >>> df.sort_values(by='col4', key=lambda col: col.str.lower())
        >>> s.sort_values(key=lambda x: x.str.lower())

        >>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))

        >>> s.sort_index(key=lambda x : x.str.lower())
    def __init__(self, env, engine, parser, preparser=lambda x: x) -> None:
        for cell in sorted(cells, key=lambda cell: (cell.row, cell.col)):
        self.value_labels.sort(key=lambda x: x[0])
        result = result.rename(columns=lambda x: f"{record_prefix}{x}")
@pytest.mark.parametrize("name1,dtype1", list(dtypes.items()), ids=lambda x: str(x))

@pytest.mark.parametrize("name2,dtype2", list(dtypes.items()), ids=lambda x: str(x))

@pytest.mark.parametrize("name,dtype", list(dtypes.items()), ids=lambda x: str(x))

@pytest.mark.parametrize("func", get_is_dtype_funcs(), ids=lambda x: x.__name__)
@pytest.fixture(params=["python", "pandas"], ids=lambda x: x)
        result = df.assign(C=lambda x: x.B / x.A)

        result = df.assign(A=lambda x: x.A + x.B)

        result = df.assign(C=[7, 8, 9], D=df.A, E=lambda x: x.B)

        result = df.assign(C=df.A, D=lambda x: x["A"] + x["C"])

        result = df.assign(C=lambda df: df.A, D=lambda df: df["A"] + df["C"])
    other = frame_with_period_index.rename(columns=lambda key: f"{key}{key}")
        df = tm.makeCustomDataframe(30, 3, data_gen_f=lambda x, y: np.random.random())
        result = df.sort_index(level=list("ac"), key=lambda x: x)

        result = df.sort_index(level=list("ac"), key=lambda x: -x)

        result = df.sort_index(key=lambda x: x.str.lower())

        result = df.sort_index(key=lambda x: x.str.lower(), ascending=False)

        result = df.sort_index(key=lambda x: -x)

        result = df.sort_index(key=lambda x: 2 * x)

        result = df.sort_index(level="a", key=lambda x: x.str.lower())

            df.sort_index(key=lambda x: x[:1])
        df.update(other, filter_func=lambda x: x > 2)
        result = df.sort_values(0, key=lambda x: x + 5)

        result = df.sort_values(0, key=lambda x: -x, ascending=False)

        result = df.sort_values("a", key=lambda x: -x)

        result = df.sort_values(by=["a", "b"], key=lambda x: -x)

        result = df.sort_values(by=["a", "b"], key=lambda x: -x, ascending=False)

        result = df.sort_values([0, 1], key=lambda col: col.str.lower())

        expected = df.sort_values(1, key=lambda col: col.str.lower(), ascending=False)

            df.sort_values("A", key=lambda x: x[:1])

        result = df.sort_values(0, key=lambda col: col.str.lower())

        result = df.sort_values(1, key=lambda col: -col)

        result = df.sort_values(0, key=lambda col: col.str.lower(), axis=1)

        result = df.sort_values(1, key=lambda col: -col, axis=1)
    (pd.DataFrame, frame_data, operator.methodcaller("rename", index=lambda x: x)),

@pytest.fixture(params=_all_methods, ids=lambda x: idfn(x[-1]))
    exp = res[sorted(res.index, key=lambda x: float(x.split()[0]))]
    def _check_groupby(df, result, keys, field, f=lambda x: x.sum()):

    df = DataFrame(data).assign(time=lambda x: x.time.dt.tz_localize(tz))

    expected = DataFrame(expected).assign(time=lambda x: x.time.dt.tz_localize(tz))
        levels.sort(key=lambda x: x[1])
        result = gr.agg(a=lambda x: 0, b=lambda x: 1)

            height_max_2=pd.NamedAgg(column="height", aggfunc=lambda x: np.max(x)),

            weight_min=pd.NamedAgg(column="weight", aggfunc=lambda x: np.min(x)),
    by1 = sorted(tuples, key=lambda x: (x[1], x[0]))

    by1 = sorted(tuples, key=lambda x: (x[1], x[0]))
        result = values.sort_values(key=lambda x: x.map(sort_order))
    sblocks = sorted(blocks, key=lambda b: b.mgr_locs[0])
        res = s.to_string(float_format=lambda x: f"{x:2.1f}", max_rows=2)
    result = df.to_html(float_format=lambda x: f"{x:,.0f}")
        expected = expected.rename(columns=lambda x: "county_" + x)
        result = parser.read_csv(bad_sio, on_bad_lines=lambda x: x)

    result = parser.read_csv(bad_sio, on_bad_lines=lambda x: None)

    result = parser.read_csv(bad_sio, on_bad_lines=lambda x: ["99", "99"])
@pytest.fixture(params=["python", "python-fwf"], ids=lambda val: val)
    result = parser.read_csv(StringIO(data), skiprows=lambda x: x % 2 == 0, **kwargs)

        parser.read_csv(StringIO(data), skiprows=lambda x: True)

        parser.read_csv(StringIO(data), skiprows=lambda x: 1 / 0)
        StringIO(data), date_parser=lambda x: datetime.strptime(x, "%Y%m%d")
        gen = _gen_two_subplots(f=lambda **kwargs: None, fig=fig, ax="test")
        ordered_color_label_tuples = sorted(color_label_tuples, key=lambda x: x[1])
            for patch in sorted(ax.patches, key=lambda patch: patch.get_bbox().xmax)
            .assign(C=lambda df: df.B.cumsum())

            .assign(D=lambda df: df.C * 1.1)
        result = self.data.pivot_table("D", index=lambda x: x // 5, columns=self.data.C)
    group = group.rename(columns=lambda x: x.replace(suffix, ""))
@pytest.fixture(params=get_series(), ids=lambda x: x.dtype.name)

@pytest.fixture(params=get_series(), ids=lambda x: x.dtype.name)

@pytest.fixture(params=get_series_na(), ids=lambda x: x.dtype.name)
        result = series.sort_values(axis=0, key=lambda x: x.str.lower())

        result = series.sort_values(axis=0, key=lambda x: x + 5)

        result = series.sort_values(axis=0, key=lambda x: -x, ascending=False)
        result = s.sort_index(level="C", key=lambda x: -x)

        result = s.sort_index(level="C", key=lambda x: x)  # nothing happens

        result = s.sort_index(level=["A", "C"], key=lambda x: -x)

        result = s.sort_index(level=["A", "C"], key=lambda x: x)  # nothing happens

        result = series.sort_index(key=lambda x: x.str.lower())

        result = series.sort_index(key=lambda x: x.str.lower(), ascending=False)

        result = series.sort_index(key=lambda x: -x)

        result = series.sort_index(key=lambda x: 2 * x)

        index_sorted_series = series.sort_index(kind=sort_kind, key=lambda x: -x)

            s.sort_index(key=lambda x: x[:1])

        result = s.sort_index(key=lambda x: x.month)

        result = s.sort_index(key=lambda x: x.day)

        result = s.sort_index(key=lambda x: x.year)

        result = s.sort_index(key=lambda x: x.month_name())
    >> mkdf(5,3,data_gen_f=lambda r,c:randint(1,100))
    alerts.sort(key=lambda alert: str(alert.alert_type))
    selected_cols = sorted(selcols, key=lambda i: df_cols_dict[i])
        summary["category_alias_char_counts"].items(), key=lambda x: -len(x[1])

            summary["script_char_counts"].items(), key=lambda x: -len(x[1])
        dct = dict(sorted(dct.items(), key=lambda x: x[0].casefold()))

        dct = dict(sorted(dct.items(), key=lambda x: x[0].casefold(), reverse=True))
for n in sorted(g.nodes, key=lambda n: n.centrality, reverse=True):
@app.route("/language/paid", limit=True, key=lambda data: data.get("key"))
def confusion_matrix(classify=lambda document: False, documents=[(None, False)]):

def test(classify=lambda document: False, documents=[], average=None):

def accuracy(classify=lambda document: False, documents=[], average=None):

def precision(classify=lambda document: False, documents=[], average=None):

def recall(classify=lambda document: False, documents=[], average=None):

def F1(classify=lambda document: False, documents=[], average=None):

def F(classify=lambda document: False, documents=[], beta=1, average=None):

def sensitivity(classify=lambda document: False, documents=[]):

def specificity(classify=lambda document: False, documents=[]):

def intertextuality(texts=[], n=5, weight=lambda ngram: 1.0, **kwargs):

def cooccurrence(iterable, window=(-1, -1), term1=lambda x: True, term2=lambda x: True, normalize=lambda x: x, matrix=None, update=None):
            p = [n.id for n in reversed(sorted(p, key=lambda n: n.centrality))]

    p = [n.id for n in reversed(sorted(p, key=lambda n: getattr(n, centrality)))]

        return sorted(concepts, key=lambda candidate: self.similarity(concept, candidate, k), reverse=True)
    def route(self, path, limit=False, time=None, key=lambda data: data.get("key"), reset=100000):
    def flatten(self, depth=1, traversable=lambda node, edge: True, _visited=None):

    def fringe(self, depth=0, traversable=lambda node, edge: True):

def depth_first_search(node, visit=lambda node: False, traversable=lambda node, edge: True, _visited=None):

def breadth_first_search(node, visit=lambda node: False, traversable=lambda node, edge: True):
def variations(iterable, optional=lambda x: False):

    # For example: variations(["A?", "B?", "C"], optional=lambda s: s.endswith("?"))

#c = Classifier(parents=lambda term: term.endswith("ness") and ["quality"] or [])

    def __init__(self, parents=lambda term: [], children=lambda term: [], value=lambda term: None):

            c = Classifier(parents=lambda term: term.endswith("ness") and ["quality"] or [])

# Classifier(parents=lambda word: word.endswith("ness") and ["quality"] or [])

# Classifier(parents=lambda word, chunk=None: chunk=="VP" and [ACTION] or [])

        v = variations(self.sequence, optional=lambda constraint: constraint.optional)
def _escape(value, quote=lambda string: "'%s'" % string.replace("'", "\\'")):

def find(match=lambda item: False, list=[]):

def cmp(field, value, comparison="=", escape=lambda v: _escape(v), table=""):

def parse_xml(database, xml, table=None, field=lambda s: s.replace(".", "-")):

    def save(self, path, separator=",", encoder=lambda v: v, headers=False, password=None, **kwargs):

    def load(cls, path, separator=",", decoder=lambda v: v, headers=False, preprocess=None, password=None, **kwargs):

    def group(self, j, function=FIRST, key=lambda v: v):

    def map(self, function=lambda item: item):

    def map(self, function=lambda value: value):

    def filter(self, function=lambda value: True):
    def __init__(self, function=lambda x: x, items=[]):
    m = sorted(m, key=lambda x: x[1])

    m = sorted(m, key=lambda x: x[0], reverse=True)

        def avg(assessments, weighted=lambda w: 1):

    return max(p.items(), key=lambda kv: (kv[1], int(kv[0] == "en")))
    return sorted(a, key=lambda tag: tag.lower() != x and tag or "")
            graph.adjacency(self.g, heuristic=lambda id1, id2: 0.1),

            graph.dijkstra_shortest_paths(g, "a", heuristic=lambda id1, id2: id1 == "d" and id2 == "a" and 1 or 0)
        self.assertEqual(db.order(v, cmp=lambda a, b: a - b), [1, 2, 0])

        self.assertEqual(db.order(v, key=lambda i: i), [1, 2, 0])

        v5 = v1.group(0, function=db.CONCATENATE, key=lambda j: j > 0)
def stream(url, delimiter="\n", parse=lambda data: data, **kwargs):

    def __init__(self, socket, delimiter="\n", format=lambda s: s, **kwargs):

        results.sort(key=lambda r: r.score, reverse=True)

    def traverse(self, visit=lambda node: None):
        v = sorted(v.values(), key=lambda item: len(item))
    return sorted(list(iterable), key=lambda x: random())

def bin(iterable, key=lambda x: x, value=lambda x: x):

    # bin([["a", 1], ["a", 2], ["b", 3]], key=lambda x: x[0]) =>

def words(string, filter=lambda w: w.strip("'").isalnum(), punctuation=PUNCTUATION, **kwargs):

        count = count.__class__(heapq.nsmallest(top, list(count.items()), key=lambda kv: (-kv[1], kv[0])))

        count = dict(heapq.nsmallest(top, list(count.items()), key=lambda kv: (-kv[1], kv[0])))

        v = heapq.nsmallest(top, v, key=lambda v: (-v[0], v[1]))

        v = heapq.nsmallest(top, v, key=lambda v: (-v[0], v[1]))

        v = heapq.nsmallest(top, v, key=lambda v: (-v[0], v[1]))

        for i, v in enumerate(sorted(vectors, key=lambda x: random())):

    def traverse(self, visit=lambda cluster: None):

def sequence(i=0, f=lambda i: i + 1):

    def __init__(self, classify=lambda document: True, documents=[]):

        c = min(H3.keys(), key=lambda k: abs(k - o))
        v = vector.words(s, filter=lambda w: w.isalpha())

        v = vector.words(s, filter=lambda w: True)

        v4 = vector.stem(s, stemmer=lambda w: "wolf*")
        v = search.variations([1], optional=lambda item: item == 1)

        v = search.variations([1, 2], optional=lambda item: item in (1, 2))

        v = search.variations([1, 2, 3, 4], optional=lambda item: item in (1, 2))

        c1 = search.Classifier(parents=lambda word, chunk=None: word.endswith("ness") and ["quality"] or [])

        c2 = search.Classifier(parents=lambda word, chunk=None: chunk == "VP" and ["action"] or [])
        v3 = p.find_tags(["Schrdinger", "cat", "1.0"], map=lambda token, tag: (token, tag + "!"))
        self._objs = csort(self._objs, key=lambda obj: -obj.y1)

        self._objs = csort(self._objs, key=lambda obj: -obj.x1)

            textboxes.sort(key=lambda box: box.index)
    return sorted(objs, key=lambda obj: (key(obj), idxs[obj]))
            hlist = sorted(hlist, key=lambda x:x[1][0])

            hlist = sorted(hlist, key=lambda x:x[1][0])

            hlist = sorted(hlist, key=lambda x:x[1][0])

                headers = sorted(headers.items(), key=lambda x: x[1][1])

            for (k, (start, end, type)) in sorted(result.items(), key=lambda x: x[1]):

            for (k, (start, end, type)) in sorted(result.items(), key=lambda x: x[1]):

            for (k, v) in sorted(result.items(), key=lambda x: x[1]):

        result = sorted(result, key=lambda x: len(x[1][0]))

            for (k, v) in sorted(result.items(), key=lambda x: len(x[0]) if not x[0].startswith("add") else int(x[0].split("_")[1])):
                columns = sorted(accum, key=lambda obj: obj.get_sort_key(ctx))

            for col in sorted(defaults, key=lambda obj: obj.get_sort_key(ctx)):
        self.assertEqual(sorted(table.all(), key=lambda row: row['id']), [

        self.assertEqual(sorted(table.all(), key=lambda row: row['id']), [
            data['tweets'].sort(key=lambda t: t['content'])
        self.entry = sorted(self.entry, key=lambda x: x["color_depth"])

        # self.entry = sorted(self.entry, key=lambda x: x['width'])

        self.entry = sorted(self.entry, key=lambda x: x["square"])
            x = round_aspect(y * aspect, key=lambda n: abs(aspect - n / y))
qt_versions.sort(key=lambda qt_version: qt_version[1] in sys.modules, reverse=True)
        self.errors.sort(key=lambda e: e.order)
    for installation in sorted(installations.values(), key=lambda x: x.name.lower()):
            return sorted(results, key=lambda result: -result['confidence'])
        result = sorted(result, key=lambda t: t[0], reverse=True)
        directories.sort(key=lambda a: a.name)
        for classname, data in sorted(LEXERS.items(), key=lambda x: x[0]):

        for module, lexers in sorted(modules.items(), key=lambda x: x[0]):

        for classname, data in sorted(FORMATTERS.items(), key=lambda x: x[0]):
                fatals.sort(key=lambda e: -e.loc)

                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parserElement))))

                fatals.sort(key=lambda e: -e.loc)

                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parserElement))))
    return sorted(resolved, key=lambda diag: diag.index)
        self.pools = RecentlyUsedContainer(num_pools, dispose_func=lambda p: p.close())
    return sorted(authors, key=lambda x: x.lower())
                ", ".join(sorted(dependency_strings, key=lambda s: s.lower())) or "-",
    specs = sorted(specs, key=lambda x: x.version)  # type: ignore
        self.errors.sort(key=lambda e: e.order)
    for installation in sorted(installations.values(), key=lambda x: x.name.lower()):
            return sorted(results, key=lambda result: -result['confidence'])
        result = sorted(result, key=lambda t: t[0], reverse=True)
        for classname, data in sorted(LEXERS.items(), key=lambda x: x[0]):

        for module, lexers in sorted(modules.items(), key=lambda x: x[0]):

        for classname, data in sorted(FORMATTERS.items(), key=lambda x: x[0]):
                fatals.sort(key=lambda e: -e.loc)

                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parserElement))))

                fatals.sort(key=lambda e: -e.loc)

                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parserElement))))
    return sorted(resolved, key=lambda diag: diag.index)
        self.pools = RecentlyUsedContainer(num_pools, dispose_func=lambda p: p.close())
        cycles = sorted(cycles, key=lambda xs: xs[1].key)
                    sorted(unannotated, key=lambda n: cd.get(n).counter)
    languages = sorted(languages, key=lambda x: x[1], reverse=True)

    return sorted(merge, key=lambda x: x[1], reverse=True)

    return sorted(results, key=lambda x: x[1], reverse=True)
    rv.sort(key=lambda x: x[0])
            possible_names.sort(key=lambda x: -len(x[0]))  # group long options first
        result = sorted(result, key=lambda t: t[0], reverse=True)
        directories.sort(key=lambda a: a.name)
    def __init__(self, spec, adapter=lambda spec: spec.loader):
    return sorted(resolved, key=lambda diag: diag.index)
                fatals.sort(key=lambda e: -e.loc)

                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parserElement))))

                fatals.sort(key=lambda e: -e.loc)

                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parserElement))))
        ireqs = sorted(ireqs, key=lambda ireq: ireq.editable)
            ireq = sorted(self.find_all_matches(), key=lambda k: k.version)
    specs = sorted(specs, key=lambda x: x._spec[1])
        self.pools = RecentlyUsedContainer(num_pools, dispose_func=lambda p: p.close())
    help_parser.set_defaults(func=lambda args: p.print_help())
#             sortList.sort(key=lambda x: (int(x.split(config.blank)[1].strip()), x))
            tasks.sort(key=lambda t: (t.due, t.priority), reverse=descending)
            ll.sort(key=lambda item: "}" if item == "NoneType" else item)

            ll.sort(key=lambda item: "}" if item == "NoneType" else item)
    cookies.sort(key=lambda r: r["name"])

    cookies.sort(key=lambda r: r["name"])
    cookies.sort(key=lambda r: r["name"])
        all_nodes.sort(key=lambda x: x[1])

        all_nodes.sort(key=lambda x: x[1])
            self._children = sorted(self._children, key=lambda node: node.plotly_name)
    return sorted(enumerate(x), key=lambda t: t[1])[0][0]

    return sorted(enumerate(x), key=lambda t: t[1], reverse=True)[0][0]
        default_colors = OrderedDict(sorted(d.items(), key=lambda t: t[0]))
    si = sorted(range(len(rows)), key=lambda i: rows[i])
        return dict(sorted(tups, key=lambda t: t[1]))
    def fun(x, f=lambda x: x, mul=1, add=0):
        chosen = max(links, key=lambda link: self._sort_key(package, link))
        groups = itertools.groupby(operations, key=lambda o: -o.priority)
        r for r in sorted(package.requires, key=lambda r: r.name) if not r.is_optional()

        r for r in sorted(package.requires, key=lambda r: r.name) if r.is_optional()

        r for r in sorted(package.requires, key=lambda r: r.name) if not r.is_optional()

        r for r in sorted(package.requires, key=lambda r: r.name) if r.is_optional()

        r for r in sorted(package.requires, key=lambda r: r.name) if not r.is_optional()

        r for r in sorted(package.requires, key=lambda r: r.name) if r.is_optional()

        r for r in sorted(package.requires, key=lambda r: r.name) if not r.is_optional()

        r for r in sorted(package.requires, key=lambda r: r.name) if r.is_optional()

        r for r in sorted(package.requires, key=lambda r: r.name) if not r.is_optional()

        r for r in sorted(package.requires, key=lambda r: r.name) if r.is_optional()

        r for r in sorted(package.requires, key=lambda r: r.name) if not r.is_optional()

        r for r in sorted(package.requires, key=lambda r: r.name) if r.is_optional()
            sorted(package.extras[name], key=lambda r: r.name) == expected_extras[name]
    assert sorted(all_extra, key=lambda r: r.name) == [

    assert sorted(required, key=lambda dep: dep.name) == expected
        for t in sorted(data, key=lambda x: x["name"]):

        return sorted(cls.all(), key=lambda x: x.rate, reverse=True)

        return min(range(len(cls.STATIC_DATA)), key=lambda i: abs(cls.STATIC_DATA[i] - cp_multiplier)) * 0.5 + 1

            attacks = sorted(by_type[t], key=lambda m: m.dps, reverse=True)

        cls.BY_DPS = sorted(ret.values(), key=lambda m: m.dps, reverse=True)

        movesets = sorted(movesets, key=lambda m: m.dps_attack)

        movesets = sorted(movesets, key=lambda m: m.dps_defense)

        return sorted(movesets, key=lambda m: m.dps, reverse=True)

        moves = sorted(moves, key=lambda m: m.dps, reverse=True)
        closest = min(times, key=lambda x: x['diff'])
        poke_list.sort(key=lambda p: p.cp, reverse=True)
        available_clusters.sort(key=lambda c: self.get_cluster_key(c), reverse=True)
            pokemons.sort(key=lambda x: (x.pokemon_id, x.cp, x.iv), reverse=True)

            pokemons.sort(key=lambda x: (x.pokemon_id, x.iv, x.cp), reverse=True)
            pokes.sort(key=lambda p: p.cp, reverse=True)
        eligible_eggs.sort(key=lambda egg: egg["km"], reverse=sorting)

            self.used_incubators.sort(key=lambda x: x.get("km"))
        pokemon_list.sort(key=lambda x: x['dist'])

            pokemon_list.sort(key=lambda x: x['priority'], reverse=True)

            pokemon_list.sort(key=lambda x: x['is_vip'], reverse=True)
        pokemons.sort(key=lambda p: p.hp)
        pokemons_ordered = sorted(possible_pokemons, key=lambda x: get_poke_info(self.order_by, x), reverse=True)
            current_owned.sort(key=lambda p: p.cp)

            current_owned.sort(key=lambda p: p.iv)
        pokemons_ordered = sorted(self.pokemons, key=lambda x: get_poke_info(self.order_by, x), reverse=True)
            pokemons.sort(key=lambda p: p["distance"])

                    pokemons.sort(key=lambda p: p["distance"])

                    worth_pokemons.sort(key=lambda p: p["distance"])

                    possible_targets.sort(key=lambda p: p["distance"])

                possible_targets.sort(key=lambda p: p["distance"])

        pokemons.sort(key=lambda p: p["distance"])

            worth_pokemons.sort(key=lambda p: p["distance"])

            worth_pokemons.sort(key=lambda p: inventory.candies().get(p["pokemon_id"]).quantity)

        closest = min(points, key=lambda p: great_circle(self.bot.position, p).meters)
            group = sorted(group, key=lambda x: x.cp, reverse=True)
        keep.sort(key=lambda p: p.__score__[0], reverse=True)

                    best.sort(key=lambda p: p.__score__[0], reverse=True)

        crap.sort(key=lambda p: (p.iv, p.cp), reverse=True)

            try_upgrade.sort(key=lambda p: (p.cp), reverse=True)
        max_clique = max(list(find_cliques(graph)), key=lambda l: (len(l), sum(x[2] for x in l)))
        pkmns = sorted(inventory.pokemons().all(), key=lambda p: getattr(p, order), reverse=True)[:num]
		segments_priority = sorted((segment for segment in segments if segment['priority'] is not None), key=lambda segment: segment['priority'], reverse=True)
		new_pwd = new_module('pwd', getpwuid=lambda uid: struct_passwd(pw_name='def@DOMAIN.COM'))

		with replace_attr(self.module, 'datetime', Args(strptime=lambda timezone, fmt: Args(tzinfo=timezone), now=lambda tz:Args(strftime=lambda fmt: fmt + (tz if tz else '')))):

		with replace_attr(self.module, 'datetime', Args(strptime=lambda timezone, fmt: Args(tzinfo=timezone), now=lambda tz: time)):

		with replace_module_module(self.module, 'psutil', cpu_percent=lambda **kwargs: 52.3):

			with replace_attr(self.vim, 'guess', get_dummy_guess(status=lambda file: 'M')):

			with replace_attr(self.vim, 'guess', get_dummy_guess(status=lambda file: None)):

				with replace_attr(self.vim, 'guess', get_dummy_guess(status=lambda file: 'M')):
            manifest = sorted(manifest, key=lambda hook: hook['id'])
        before = sorted(before, key=lambda x: top_keys.index(x[0]))
    task_run = min(task_runs, key=lambda task_run: task_run.state_start_time)
                callback_factory(on_failure, check=lambda s: s.is_failed())
                callback_factory(on_failure, check=lambda s: s.is_failed())
    return sorted(params, key=lambda p: p.slug)

    return sorted(tasks, key=lambda t: t.slug)

    return list(sorted(tasks, key=lambda t: t.slug))
    even_filter = FilterTask(filter_func=lambda x: x % 2 == 0)
        assert sorted(taskdef["tags"], key=lambda x: x["key"]) == [
        t = Task(on_failure=lambda *args: None)

            Task(state_handlers=lambda *a: 1)
        Runner(state_handlers=lambda *a: 1)
    names = [name for name, time in sorted(times, key=lambda x: x[1])]

        flow_handler = MagicMock(side_effect=lambda t, o, n: None)
        result = LocalResult(dir=tmpdir, location=lambda **kwargs: "special_val")

        result = LocalResult(dir=tmpdir, location=lambda **kwargs: kwargs["key"])
    task = Task(name="test", task_run_name=lambda **kwargs: "name")

    @prefect.task(name="hey", task_run_name=lambda **kwargs: kwargs["config"])
        f = Flow(name="test", on_failure=lambda *args: None)

            Flow(name="test", state_handlers=lambda *a: 1)
        my_task = Task(target=lambda **kwargs: "testcall", result=result)

        my_task = Task(target=lambda **kwargs: "{task_name}", result=result)

            @prefect.task(target=lambda **kwargs: str(kwargs["task_run_count"]))

            @prefect.task(target=lambda **kwargs: str(kwargs["x"]))

        task_handler = MagicMock(side_effect=lambda t, o, n: n)

        task_handler = MagicMock(side_effect=lambda t, o, n: n)

        task_handler = MagicMock(side_effect=lambda t, o, n: None)

        task_handler = MagicMock(side_effect=lambda t, o, n: n)

        task_handler = MagicMock(side_effect=lambda t, o, n: n)

        task_runner_handler = MagicMock(side_effect=lambda t, o, n: n)

        task_runner_handler = MagicMock(side_effect=lambda t, o, n: n)

        task_runner_handler = MagicMock(side_effect=lambda t, o, n: n)

        task_runner_handler = MagicMock(side_effect=lambda t, o, n: n)

        task_runner_handler = MagicMock(side_effect=lambda t, o, n: n)

    task_runner_handler = MagicMock(side_effect=lambda t, o, n: n)

    task_handler = MagicMock(side_effect=lambda t, o, n: n)

        task = Task(name="test", task_run_name=lambda **kwargs: "name")
    d1, d2, d3 = sorted(deserialized.tasks, key=lambda t: t.name)
        task = FilterTask(filter_func=lambda r: r != 5)
        f = FunctionTask(fn=lambda x: x + 1)
            secret = PrefectSecret(name="test", result=lambda x: None)
        d = DotDict(chris=10, attr="string", other=lambda x: {})

        d = DotDict(chris=10, attr="string", other=lambda x: {})
def _test_timer_with_threads(timer, sleep, spawn, join=lambda x: x.join()):
        paths.sort(key=lambda x: int(re.search(r"[0-9]+", x).group()))
    rawlist.sort(key=lambda x: x[1])  # sort by family
    processes = sorted(procs, key=lambda p: p._total, reverse=True)
        pid = sorted(table.items(), key=lambda x: x[1])[-1][0]
    nic_names.sort(key=lambda x: sum(pnic_after[x]), reverse=True)
            tid = sorted(threads, key=lambda x: x.id)[1].id
    procs.sort(key=lambda p: p._uss)
        maps = sorted(pinfo['memory_maps'], key=lambda x: x.rss, reverse=True)
    st.sort(key=lambda x: x[:3] in ('run', 'sle'), reverse=1)
            for url in sorted(urls, key=lambda x: os.path.basename(x)):
    for methname, ads in sorted(d.items(), key=lambda x: (x[1], x[0])):
        for wheel in sorted(wheels, key=lambda x: x.name):
    timings.sort(key=lambda x: x[1])
        for a, cmd in sorted(aliases, key=lambda a: a[0]):
        objects = sorted(objects, key=lambda x: x['LOGGER'].data)
        conf['delays'] = sorted(args.delays_list, key=lambda x: x[0])
        devices = sorted(devices, key=lambda (_, weight): weight)

        non_devices = sorted(non_devices, key=lambda (_, weight): weight)
                    for f in sorted(dirs, key=lambda x: to_str(x.get(T_NAME)), reverse=args.reverse):

                    for f in sorted(files, key=lambda x: to_str(x.get(T_NAME)), reverse=args.reverse):

                    for f in sorted(r[T_FILES], key=lambda x: x.get(args.sort), reverse=args.reverse):
        self.decoding_table = sorted(self.decoding_table, key=lambda x: x[1], reverse=True)

            best = sorted(results, key=lambda x: len(x.encoded))[0]

                for idx, key in enumerate(sorted(table, key=lambda x:table[x], reverse=True)):
        for l in sorted(self,key=lambda x:x.__name__):
            sorted(orphan, key=lambda x: x['start'])

    events = sorted(events, key=lambda x: x['start'], reverse=True)
            data = sorted(data['creds'], key=lambda d: d.get('cid', d.get('uid')), reverse=True)

            data = sorted(data['creds'], key=lambda d: d.get('credtype'), reverse=True)
        pos_annotations = sorted([a for a in src.get("annotations", []) if a.get("offset") == closest], key=lambda a: a["start"])
    sorted_commands.sort(key=lambda x: x.__name__)
parser.add_argument("count", nargs="?", type=lambda n:max(int(n, 0),1), default=10, help="Number of chunks to visualize.")
    first_page = min(pages, key=lambda page: page.vaddr)
    for atom in sorted(atoms, key=lambda a: a.start):

        self.queues = { sz: SortedList(key=lambda atom: atom.integer) for sz in SPECIFIER.keys() }

        best_size = min(active_sizes, key=lambda sz: self.queues[sz][self.positions[sz]].compute_padding(self.numbwritten))
        self.mappings = sorted(self.mappings, key=lambda m: m.start)
    def read(self, path, filesize=0, callback=lambda *a: True):
    processes = sorted(processes, key=lambda p: p.create_time(), reverse=True)
        dt = sorted(dt, key=lambda x: -x['score'])

        # gt = sorted(gt, key=lambda x: x['_ignore'])

        gtind = [ind for (ind, g) in sorted(enumerate(gt), key=lambda (ind, g): g['_ignore']) ]

        dt = sorted(dt, key=lambda x: -x['score'])[0:maxDet]
        self.__bars[instrument].sort(key=lambda b: b.getDateTime())
        return sorted(ret, key=lambda t: t.getId())
        self.__values.sort(key=lambda x: x[0])
m = Foo(callback=lambda x: x)
    m = Model(callback=lambda x: x)
    assert m.json(encoder=lambda v: '__default__') == '{"x": "__default__"}'
        parse_obj_as(int, 'a', type_name=lambda type_: type_.__name__)
    index.sort(key=lambda i: ratio[i], reverse=True)
        top_lanes = sorted(line_counter.items(), key=lambda item: item[1])[::-1][:2]
        top_lanes = sorted(line_counter.items(), key=lambda item: item[1])[::-1][:2]
        top_lanes = sorted(line_counter.items(), key=lambda item: item[1])[::-1][:2]
                a.dep_nodes.sort(key=lambda x: x.abspath())
    arr.sort(key=lambda x: x['installationVersion'])
            graph_nodes.sort(key=lambda item: item.identifier)
    py_files.sort(key=lambda v: v.filename)

    extensions.sort(key=lambda v: v.filename)
        for m in sorted(self.iter_graph(), key=lambda n: n.identifier):
PYTHONPATH_PREFIXES.sort(key=lambda p: len(p.parts), reverse=True)
    frame_b, frame_a = sorted(frame.children, key=lambda f: f.time(), reverse=True)
        y = DensityDist("y", logp=lambda *args: x)
            IntervalTransform(args_fn=lambda *args: (-0.5, 0.5))
    tracker = pm.callbacks.Tracker(bad=lambda t: t)  # bad signature
            for t in sorted(params.items(), key=lambda t: t[0])
results.sort(key=lambda x: x[1], reverse=True)
                fontlist[list].sort(key=lambda x: x['fname'])
        curve_types.sort(key=lambda ct: ct[1])
    _: Msg[Request] = await func1(check=lambda msg: (msg.body.id == 12345))
foo2 = Foo2(a=lambda a: a)

foo3 = Foo3(a=lambda a: a)
    neutra_model = poutine.reparam(model, config=lambda _: neutra)

    neutra_model = poutine.reparam(model, config=lambda _: neutra)
debug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)

        poutine.block(model, expose_fn=lambda msg: msg["name"].startswith("probs_"))
debug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)

        handlers.block(model, expose_fn=lambda msg: msg["name"].startswith("probs_"))
    def __init__(self, fn=None, hide_fn=lambda msg: True):

            with block(hide_fn=lambda msg: msg["type"] == "sample"):

            with block(), trace() as tr, block(hide_fn=lambda m: m["type"] != "param"):
    edges.sort(key=lambda uv: (uv[1], uv[0]))
            for f in sorted(self._plates[d], key=lambda f: f.dim):

                p.size for p in sorted(self._plates[d], key=lambda p: p.dim)

    perm = sorted(range(-x.dim(), 0), key=lambda d: (d in event_dims, d))
            name, count = min(num_pending.items(), key=lambda kv: (kv[1], kv[0]))
        model = poutine.reparam(model, config=lambda _: neutra)
    def __init__(self, model, guide, median=lambda *args, **kwargs: {}):
    def log_prob_sum(self, site_filter=lambda name, site: True):

    def compute_log_prob(self, site_filter=lambda name, site: True):
@pytest.fixture(params=[1, 2, 3], ids=lambda x: "dim=" + str(x))
@pytest.fixture(params=[1, 2, 3], ids=lambda x: "dim=" + str(x))
DISTRIBUTIONS.sort(key=lambda d: d.__name__)
        sorted(trace_prob_evaluator._log_probs.keys(), key=lambda x: (len(x), x))
        with poutine.escape(escape_fn=lambda msg: msg["name"] == "internal2"):
    titles = sorted(titles, key=lambda x: x[0].lower())
psg_classes = sorted(list(set([i[1] for i in psg_classes])), key=lambda x : x.__name__) # filtering of anything that starts with _ (methods, classes, etc)
    tup.sort(key=lambda x: x[position])
		for message in sorted(warning_list + info_list, key=lambda x: x['message_time']):
    result_items.sort(key=lambda key_value: vars_order.index(key_value[0]))
    for updatetime, task in sorted(updatetime_tasks, key=lambda x: x[0]):
    open_issues.sort(key=lambda x: x["number"])
        return sorted(items, key=lambda item: item.path.stat().st_mtime, reverse=True)  # type: ignore[no-any-return]
            return real_unwrap(func, stop=lambda obj: _is_mocked(obj) or _stop(func))
    entries.sort(key=lambda entry: entry.name)

    for entry in visit(source, recurse=lambda entry: not entry.is_symlink()):
    dlist.sort(key=lambda x: x.duration, reverse=True)  # type: ignore[no-any-return]
                items[:] = sorted(items, key=lambda item: item.nodeid)
        entry.name for entry in visit(str(tmp_path), recurse=lambda entry: False)
        @pytest.fixture(params=['foo', 'bar'], ids=lambda p: p.upper())
@pytest.fixture(params=[(0, 0), (1, 1)], ids=lambda x: str(x[0]))

@pytest.mark.parametrize("func", [str, int], ids=lambda x: str(x.__name__))
    return sorted((Letter(c, f) for c, f in chars.items()), key=lambda l: l.freq)

        response.sort(key=lambda l: l.freq)
    >>> h = Heap(key=lambda x: -x)  # Min heap
    return sorted(array, key=lambda x: x[column])
    index.sort(key=lambda i: ratio[i], reverse=True)
    r = list(sorted(zip(vl, wt), key=lambda x: x[0] / x[1], reverse=True))
    nodes.sort(key=lambda node: node.key)
        population_score = sorted(population_score, key=lambda x: x[1], reverse=True)
    E.sort(reverse=True, key=lambda x: x[2])
        for k, v in sorted(frequency_table.items(), key=lambda v: v[1][0], reverse=True)
        edges.sort(key=lambda e: e[2])
    edges = sorted(edges, key=lambda edge: edge[2])
        edges.sort(key=lambda x: x[2])
                i2 = min(tmp_error_dict, key=lambda index: tmp_error_dict[index])

                i2 = max(tmp_error_dict, key=lambda index: tmp_error_dict[index])
    sorted_points = sorted(points, key=lambda point: angle_comparer(point, minx, miny))
    neighborhood_of_solution.sort(key=lambda x: x[indexOfLastItemInTheList])
    all_things.sort(key=lambda x: x.value, reverse=True)
def select_sort(origin_items, comp=lambda x, y: x < y):

def bubble_sort(origin_items, *, comp=lambda x, y: x > y):

def merge_sort(items, comp=lambda x, y: x <= y):

def merge(items1, items2, comp=lambda x, y: x <= y):

def quick_sort(origin_items, comp=lambda x, y: x <= y):

    # print(bubble_sort(items2, comp=lambda p1, p2: p1.age > p2.age))

    # print(select_sort(items2, comp=lambda p1, p2: p1.name < p2.name))

    # print(merge_sort(items2, comp=lambda p1, p2: p1.age <= p2.age))

    print(quick_sort(items2, comp=lambda p1, p2: p1.age <= p2.age))

    # print(bubble_sort(items3, comp=lambda x, y: len(x) > len(y)))
        self.cards.sort(key=lambda card: (card.suite, card.face))
cheap = heapq.nsmallest(3, portfolio, key=lambda s: s['price'])

expensive = heapq.nlargest(3, portfolio, key=lambda s: s['price'])
rows.sort(key=lambda r: r['date'])

for date, items in groupby(rows, key=lambda r: r['date']):
    print(list(dedupe(a, key=lambda a: (a['x'],a['y']))))
        pool.run(fib, (n,), callback=lambda r: self.respond(r, addr))
    locks = sorted(locks, key=lambda x: id(x))   
        main_py_dirs = sorted(main_py_dirs, key=lambda j: len(split(j)))
    busy.sort(key=lambda x: x.play_time)
    hanzi_pairs = sorted(parse(in_fp).items(), key=lambda x: x[0])
        hanzi_pairs = sorted(new_dict.items(), key=lambda x: x[0])
_convert_table.sort(key=lambda x: len(x[0]), reverse=True)

_initial_table.sort(key=lambda x: len(x[0]), reverse=True)

_tone_table.sort(key=lambda x: len(x[0]), reverse=True)

_except_table.sort(key=lambda x: len(x[0]), reverse=True)
    assert slug(hans, style=TONE2, separator=' ', errors=lambda x: None) == ret
    pics = sorted(pics, key=lambda x:int(x))
				recommend = sorted(statistics.items(), key=lambda e:e[1], reverse=True)[0][0]

				f.write('    earliest_tweet = min(timeline, key=lambda x: x.id).id

        new_earliest = min(tweets, key=lambda x: x.id).id
        open_list.sort(key=lambda x: x.F)

        target_origin = min(origin_open, key=lambda x: x.F).coordinate

        target_goal = min(goal_open, key=lambda x: x.F).coordinate
            self.open_set = sorted(self.open_set, key=lambda x: x['fcost'])

            self.open_set = sorted(self.open_set, key=lambda x: x['fcost'])
        v_plus_values = sorted(v_plus_values, key=lambda x: x[1])

        e_and_values = sorted(e_and_values, key=lambda x: x[2])

            currId = min(openSet, key=lambda x: self.f_scores[x])
            c_id = min(open_set, key=lambda o: open_set[o].cost)
        min_state = min(self.open_list, key=lambda x: x.k)
            self.U.sort(key=lambda x: x[1])

            self.U.sort(key=lambda x: x[1])

        self.U.sort(key=lambda x: x[1])

                self.U.sort(key=lambda x: x[1])

            self.U.sort(key=lambda x: x[1])
                neighbor_list = sorted(neighbor_list, key=lambda x: x['cost'])
        c_id = min(open_set, key=lambda o: open_set[o].cost)
    best_path_index = paths.index(min(paths, key=lambda p: abs(p.L)))
            current_id = min(open_set, key=lambda o: open_set[o].cost)
            sorted(has_attribute, key=lambda s: getattr(s, attribute_name))
    d.add_alias("name", "name2", op=lambda x, y: x + " " + y)
    for var in sorted(variables, key=lambda v: v.id):
    return sorted(self._errors, key=lambda x: (x.filename or "", x.lineno))
  def _call_traces_to_function(call_traces, name_transform=lambda x: x):
      for annot in sorted(self.late_annotations[name], key=lambda t: t.expr):
          for cls, item in sorted(inner_cls_types, key=lambda typ: typ[1].name):
    imports = sorted(imports, key=lambda s: (s.startswith("from "), s))
      x2 = min([3.1, 4.1], key=lambda n: n)

      y2 = max([3.1, 4.1], key=lambda n: n)
        name = property(fget=lambda self: self._name)
  for location, category, name, typ in sorted(out, key=lambda x: x[0]):
def walk_binding(binding, keep_binding=lambda _: True):
    provider.backends(simulator=False, filters=lambda x: x.configuration().n_qubits > 4)
        provider.backends(filters=lambda x: x.configuration().n_qubits >= 2, simulator=False)
        for columns,g in itertools.groupby(sorted(results,key=lambda x:x.columns),key=lambda x:x.columns):
    provider.backends(simulator=False, filters=lambda x: x.configuration().n_qubits > 4)
    sorted_inst_map = sorted(instruction_map.items(), key=lambda item: item[0])
        return OrderedDict(sorted(count_ops.items(), key=lambda kv: kv[1], reverse=True))
            sorted(block_qargs, key=lambda x: wire_pos_map[x]),

            sorted(block_cargs, key=lambda x: wire_pos_map[x]),

            op_nodes.sort(key=lambda nd: nd._node_id)
            # I.e operator = ListOp([...], combo_fn=lambda x:x) will not pass this check and
                        block[i][i] = ListOp([single_terms[i]], combo_fn=lambda x: 1 - x[0] ** 2)

        return ListOp(oplist=blocks, combo_fn=lambda x: np.real(block_diag(*x))[:, perm][perm, :])
        grad_op = ListOp(diag, combo_fn=lambda x: np.diag(np.real([1 - y**2 for y in x])))
        super().__init__(oplist, combo_fn=lambda x: np.sum(x, axis=0), coeff=coeff, abelian=abelian)
        return dict(sorted(scaled_dict.items(), key=lambda x: x[1], reverse=True))
        outcomes = sorted(outcomes, key=lambda x: x[2])  # type: ignore
        return dict(sorted(scaled_dict.items(), key=lambda x: x[1], reverse=True))
        return dict(sorted(scaled_dict.items(), key=lambda x: x[1], reverse=True))
        return dict(sorted(scaled_dict.items(), key=lambda x: x[1], reverse=True))
    mapping = dict(sorted(mapping.items(), key=lambda item: item[1]))
    for symbol in sorted(expr.free_symbols, key=lambda s: s.name):

            for p in sorted(value.free_symbols, key=lambda s: s.name):
            sorted_params = sorted(tuple(instruction.parameters.items()), key=lambda x: x[0])
            return sorted(tmp, key=lambda x: -np.count_nonzero(np.array(x.to_label(), "c") == b"I"))
        sorted_probs = dict(sorted(self.items(), key=lambda item: item[1]))
        cm_nodes = [k for k, v in sorted(enumerate(cm_nodes), key=lambda item: item[1])]
            sorted_qubits = sorted(cur_qubits, key=lambda x: global_index_map[x])
            new_basis, new_circ = min(new_circs.items(), key=lambda x: len(x[1]))
            self.matched_nodes_list.sort(key=lambda x: x[1].successorstovisit)

                    self.matched_nodes_list.sort(key=lambda x: x[1].successorstovisit)
        self.substitution_list.sort(key=lambda x: x.circuit_config[0])
                largest = heapq.nlargest(survivor, range(len(metrics)), key=lambda x: metrics[x])

                matches_scenario.sort(key=lambda x: x[0])
        self.match_list.sort(key=lambda x: len(x.match), reverse=True)
        qubits = [dag.qubits[i[0]] for i in sorted(perm_circ.inputmap.items(), key=lambda x: x[0])]
            best_swaps.sort(key=lambda x: (self._bit_indices[x[0]], self._bit_indices[x[1]]))

                    swap = sorted([virtual, virtual_neighbor], key=lambda q: self._bit_indices[q])
        labels = [list(x) for x in zip(*sorted(zip(dist, labels), key=lambda pair: pair[0]))][1]

        labels = list(sorted(combined_counts.keys(), key=lambda key: combined_counts[key]))

    sorted_counts = sorted(execution.items(), key=lambda p: p[1])
        qubit_b = min(self._data[node]["q_xy"], key=lambda xy: xy[1])

        clbit_b = min(xy_plot, key=lambda xy: xy[1])

        qubit_b = min(xy, key=lambda xy: xy[1])

        qubit_t = max(xy, key=lambda xy: xy[1])

        qubit_b = min(xy, key=lambda xy: xy[1])

        qubit_t = max(xy, key=lambda xy: xy[1])
    nodes.sort(key=lambda nd: nd._node_id)
        current_cons.sort(key=lambda tup: tup[0])
        min_entry = min(qubit_coordinates, key=lambda x: min(x[0], x[1]))
        self._time_breaks = sorted(new_breaks, key=lambda x: x[0])
            sorted(output_channels.items(), key=lambda x: (x[0].index, x[0].name))

            sorted(channels.items(), key=lambda x: (x[0].index, x[0].name))

            table_data = sorted(table_data, key=lambda x: x[0])
        sorted_frame_changes = sorted(self._frames.items(), key=lambda x: x[0], reverse=True)

        sorted_waveforms = sorted(self._waveforms.items(), key=lambda x: x[0])

        sorted_frame_changes = sorted(self._frames.items(), key=lambda x: x[0])
    ordered_channels.extend(sorted(d_chans, key=lambda x: x.index))

    ordered_channels.extend(sorted(c_chans, key=lambda x: x.index))

    ordered_channels.extend(sorted(m_chans, key=lambda x: x.index))

        ordered_channels.extend(sorted(a_chans, key=lambda x: x.index))

    d_chans = sorted(d_chans, key=lambda x: x.index, reverse=True)

    u_chans = sorted(u_chans, key=lambda x: x.index, reverse=True)

    m_chans = sorted(m_chans, key=lambda x: x.index, reverse=True)

    a_chans = sorted(a_chans, key=lambda x: x.index, reverse=True)

    d_chans = sorted(d_chans, key=lambda x: x.index, reverse=True)

    m_chans = sorted(m_chans, key=lambda x: x.index, reverse=True)

    a_chans = sorted(a_chans, key=lambda x: x.index, reverse=True)

    u_chans = sorted(u_chans, key=lambda x: x.index)

    sorted_map = sorted(qubit_channel_map.items(), key=lambda x: x[0])
        qregs = sorted(qregs, key=lambda x: x.index, reverse=False)

        cregs = sorted(cregs, key=lambda x: x.index, reverse=False)

    qregs = sorted(qregs, key=lambda x: x.index, reverse=True)

    cregs = sorted(cregs, key=lambda x: x.index, reverse=True)
                sorted_keys = sorted(overlaps, key=lambda x: np.nanmax(y_coords(links[x])))
        self.passmanager.append([PassB_TP_RA_PA(), PassC_TP_RA_PA()], do_x_times=lambda x: 3)

        passmanager.append([PassB_TP_RA_PA(), PassC_TP_RA_PA()], do_x_times=lambda x: 3)

        passmanager.append([PassB_TP_RA_PA(), PassC_TP_RA_PA()], do_x_times=lambda x: 3)
        self.pass_manager.append(TrivialLayout(coupling_map), condition=lambda x: True)

        self.pass_manager.append(BarrierBeforeFinalMeasurements(), do_while=lambda x: False)
        wrap_method(Dummy, "instance", after=lambda self, mock: mock(self, "after"))

        wrap_method(Dummy, "method", before=lambda self: None)

            wrap_method(Dummy, "bad", before=lambda self: None)
            order_it = sorted(orders, key=lambda order: -order.direction)
        artifact_list.sort(key=lambda x: x.index.get_level_values("datetime").min())
            proc_yesterday = proc[[f"{c}_1" for c in cnames]].rename(columns=lambda c: c[:-2])
        sorted_grps = sorted(grps, key=lambda g: self.groups_map[g.name].position)
        self.outputs.sort(key=lambda o: (o.x, o.y))
        Match(func=lambda c: c.has_fixed_size()),

        Match(func=lambda c: c.has_fixed_ratio()),

            Match(func=lambda c: bool(c.is_transient_for()))
                self.tray_icons.sort(key=lambda icon: icon.name)
    gpurl = generic_poll_text.GenPollUrl(json=False, parse=lambda x: x, url="testing")

    gpurl = generic_poll_text.GenPollUrl(parse=lambda x: x, data=[1, 2, 3], url="testing")

    gpurl = generic_poll_text.GenPollUrl(json=False, xml=True, parse=lambda x: x, url="testing")

    gpurl = generic_poll_text.GenPollUrl(json=False, xml=True, parse=lambda x: x, url="testing")

    gpurl = generic_poll_text.GenPollUrl(json=False, parse=lambda x: x.foo, url="testing")
    def to_printer(self, printer, callback=lambda ok: None):
        self.find_css('#' + elem_id, find_id_cb, error_cb=lambda exc: None)
    return sorted(items, key=lambda v: v['name'].lower())
def _tabs(*, win_id_filter=lambda _win_id: True, add_win_id=True, cur_win_id=None):
        for values in sorted(self, key=lambda v: v.opt.name):
            sorted_items = sorted(enumerate(self.items), key=lambda e: e[1])
    for item in sorted(items, key=lambda e: (str(e.filename).lower(), e.first_lineno)):
    comment = str(soup.find(string=lambda text: isinstance(text, bs4.Comment)))
@pytest.fixture(params=_generate_cmdline_tests(), ids=lambda e: e.cmd)
    files = list(tmpdir.visit(fil=lambda path: path.isfile()))
@pytest.fixture(params=key_data.KEYS, ids=lambda k: k.attribute)

@pytest.fixture(params=key_data.MODIFIERS, ids=lambda m: m.attribute)
    languages = sorted(dictcli.available_languages(), key=lambda lang: lang.code)
    messages.sort(key=lambda x: int(x['created_at']))
            self.table.sort(key=lambda entry: entry.pid)

            self.table.sort(key=lambda entry: entry.object_size)

            self.table.sort(key=lambda entry: entry.reference_type)
        result.sort(key=lambda entry: entry["actor_id"])

        result.sort(key=lambda entry: entry["placement_group_id"])

        result.sort(key=lambda entry: entry["node_id"])

        result.sort(key=lambda entry: entry["worker_id"])

        result.sort(key=lambda entry: entry["task_id"])

        result.sort(key=lambda entry: entry["object_id"])
            splits = sorted(splits, key=lambda s: counts_cache[s._get_uuid()])

            smaller_splits = sorted(smaller_splits, key=lambda s: counts[s._get_uuid()])
    return zip(*sorted(filtered_paths, key=lambda x: x[0]))
        sorted(shuffled, key=lambda arr: arr.min()),

        sorted(base, key=lambda arr: arr.min()),

        sorted(shuffled, key=lambda arr: arr.min()),

        sorted(base, key=lambda arr: arr.min()),

        sorted(base, key=lambda arr: -arr.min()),

        sorted(base, key=lambda arr: -arr.min()),

    assert agg_ds.sort(key=lambda r: r[0]).take(3) == [("A", 5), ("B", 15), ("C", 7)]

    assert agg_ds.sort(key=lambda r: str(r[0])).take(3) == [

    assert agg_ds.sort(key=lambda r: r[0]).take(3) == [(0, 34), (1, 33), (2, 33)]

    assert agg_ds.sort(key=lambda r: r[0]).take(3) == [(0, 1683), (1, 1617), (2, 1650)]

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(3) == [

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(3) == [

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(1) == [(0, None)]

    assert agg_ds.sort(key=lambda r: r[0]).take(3) == [(0, 0), (1, 1), (2, 2)]

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(3) == [(0, 0), (1, 1), (2, 2)]

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(3) == [(0, None), (1, 1), (2, 2)]

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(1) == [(0, None)]

    assert agg_ds.sort(key=lambda r: r[0]).take(3) == [(0, 99), (1, 97), (2, 98)]

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(3) == [(0, 99), (1, 97), (2, 98)]

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(3) == [(0, None), (1, 97), (2, 98)]

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(1) == [(0, None)]

    assert agg_ds.sort(key=lambda r: r[0]).take(3) == [(0, 49.5), (1, 49.0), (2, 50.0)]

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(3) == [

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(3) == [

    assert nan_agg_ds.sort(key=lambda r: r[0]).take(1) == [(0, None)]

    result = agg_ds.sort(key=lambda r: r[0]).take(3)

    result = agg_ds.sort(key=lambda r: r[0]).take(3)

    result = nan_agg_ds.sort(key=lambda r: r[0]).take(3)

    result = nan_agg_ds.sort(key=lambda r: r[0]).take(3)

    result = nan_agg_ds.sort(key=lambda r: r[0]).take(1)

    assert agg_ds.sort(key=lambda r: r[0]).take(3) == [

    result = agg_ds.sort(key=lambda r: r[0]).take(3)
        assert ds.sort(key=lambda x: -x).take(num_items) == list(
                    for close_var, cn in sorted(common.items(), key=lambda i: -i[1])
            + sorted(node_to_replicas.values(), key=lambda lst: len(lst))
        self.sorted_routes = sorted(routes, key=lambda x: len(x), reverse=True)
        deployments.sort(key=lambda d: d["name"])
    jobs.sort(key=lambda x: x["JobID"])

    jobs.sort(key=lambda x: x["JobID"])
        checkpoints = sorted(self._top_persisted_checkpoints, key=lambda c: c.priority)
        best_path_metrics = sorted(checkpoint_paths, key=lambda x: a * x[1])
        for trial in sorted(trials, key=lambda t: t.last_update_time, reverse=True):
            for bracket in sorted(scrubbed, key=lambda b: b.completion_percentage()):
        trials.sort(key=lambda t: self._trial_state[t].last_score)

        candidates.sort(key=lambda trial: self._trial_state[trial].last_train_time)
        expected_path = max(checkpoints_metrics, key=lambda x: x[1])[0]
        trials = sorted(sched._trial_info, key=lambda t: t.trial_id)

        custom_explore_fn = MagicMock(side_effect=lambda x: x)
        build = sorted(result.results, key=lambda b: b.created_at)[-1]
    for row in sorted(rows, key=lambda item: item["instance"]):
    for build_dict in sorted(build_dict_list, key=lambda bd: -bd["number"]):
        rewards = self._group_items(rewards, agg_fn=lambda gvals: list(gvals.values()))

        dones = self._group_items(dones, agg_fn=lambda gvals: all(gvals.values()))

    def _group_items(self, items, agg_fn=lambda gvals: list(gvals.values())):
        for suspect in sorted(suspects, key=lambda s: s.memory_increase, reverse=True):
        data = f'{urlencode(sorted(data.items(), key=lambda d: d[0]))}sh0wselfh5'
            list(self.__adps.items()), key=lambda item: item[1][2])

            list(self.__regAdapterProxyDict.items()), key=lambda item: item[0])

            list(self.__regAdapterProxyDict.items()), key=lambda item: item[0])

            list(self.__regAdapterProxyDict.items()), key=lambda item: item[0])
    sorted_user_dict = sorted(user_dict.items(), key=lambda x: x[1], reverse=True)

    sorted_item_dict = sorted(item_dict.items(), key=lambda x: x[1], reverse=True)

    sorted_cat_dict = sorted(cat_dict.items(), key=lambda x: x[1], reverse=True)

        sorted_user_behavior = sorted(user_dict[user_behavior], key=lambda x: x[1])
            info.sort(key=lambda x: x[0])

        cited_paper_info.sort(key=lambda x: x[1])
                    for item in sorted(eval_res.items(), key=lambda x: x[0])

                        for item in sorted(test_res.items(), key=lambda x: x[0])
        sorted(zip(best, scores[best] / user_norms[user_id]), key=lambda x: -x[1])[1:],

        sorted(zip(best, scores[best] / item_norms[item_id]), key=lambda x: -x[1])[1:],
                    for item in sorted(eval_res.items(), key=lambda x: x[0])

                        for item in sorted(test_res.items(), key=lambda x: x[0])
            items = sorted(self.User[user], key=lambda x: x[1])
                items = sorted(items, key=lambda x: x[1])
    with patch("requests.get", side_effect=lambda url: mocked_trials_get(url, content)):
        for q in sorted(Queue.all(), key=lambda q: q.name)
            for q in sorted(query_runners.values(), key=lambda q: q.name().lower())

        return sorted(list(response.values()), key=lambda d: d["name"].lower())
            {"name": i["name"], "columns": sorted(i["columns"], key=lambda x: x["name"] if isinstance(x, dict) else x)}

            for i in sorted(schema, key=lambda x: x["name"])

        for v in sorted(self.visualizations, key=lambda v: v.id):
    fields = sorted(field_orders, key=lambda f: field_orders[f])
            columns = sorted(columns, key=lambda col: col["name"], reverse=reverse)
        return sorted(schema.values(), key=lambda x: x['name'])
    renderer = pystache.Renderer(escape=lambda u: u)
            srnames = sorted(set(srnames), key=lambda name: name.lower())
        categories = sorted(set(categories), key=lambda name: name.lower())
            srnames = sorted(srnames, key=lambda name: name.lower())
        self.rules.sort(key=lambda r: r.priority, reverse=True)
        errors.sort(key=lambda e: e.line)
    for name, state, start in sorted(steps, key=lambda t: t[2], reverse=True):
    return sorted(images.values(), key=lambda i: i.filenames[0])

    small_images.sort(key=lambda i: i.height, reverse=True)
    return sorted(rising, key=lambda x: x[1], reverse=True)
    earliest_campaign = min(campaigns, key=lambda camp: camp.start_date)
                        max_pair = max_fn(max_pair, pair, key=lambda x: x[0])
        r = sorted(q, key=lambda i: i[1][1]) # (col_name, (col_val, timestamp))
            data.sort(reverse=True, key=lambda x: x[1:])

    return sorted(results, key=lambda x: x[1], reverse=True)
    links_for_url.sort(key=lambda link: link._hot, reverse=True)
        self.mod_srs = sorted(mod_srs, key=lambda sr: sr.name.lower())

        srs.sort(key=lambda sr: sr.name.lower())

        multis.sort(key=lambda multi: multi.name.lower())

            defaults = sorted(defaults, key=lambda sr: sr._downs, reverse=True)

        srs.sort(key=lambda sr: sr.name.lower())

                bids.sort(key=lambda x: x.date, reverse=True)

        top_srs = sorted(user_srs, key=lambda sr: sr._ups, reverse=True)[:20]

        rows.sort(key=lambda row: row.info['author'].lower())

            multis.sort(key=lambda multi: multi.name.lower())

        matching.sort(key=lambda item: len(item["path"]), reverse=True)
            for child in sorted(children, key=lambda child: child._id):
        tuples = sorted(row._values().items(), key=lambda t: t[0].time)
        return sorted(result, key=lambda t: t["priority"])
        return sorted(filtered_srs, key=lambda sr: sr.name)
        pages = sorted(pages, key=lambda page: page.name)
        details.sort(key=lambda d: d.date)
    for link, bid in sorted(bid_by_link.items(), key=lambda t: t[1]):

    for name, weight in sorted(counts.keys(), key=lambda t: t[1]):
        attempt = sorted(stack, key=lambda x: x.position)

        for c in sorted(stack, key=lambda x: x.position):
    return sorted(res, key=lambda d: list(d.keys()))

    return sorted(res, key=lambda d: list(d.keys()))
    res = sorted(alias_client.search("*").docs, key=lambda x: x.id)

    res = sorted(alias_client2.search("*").docs, key=lambda x: x.id)
        await self._subscribe(p, foo=lambda x: None)
    res = sorted((await alias_client.search("*")).docs, key=lambda x: x.id)

    res = sorted((await alias_client2.search("*")).docs, key=lambda x: x.id)
        values.sort(key=lambda x: x[0], reverse=True)
        for item in sorted(input_list, key=lambda x: str(x)):

    for key in sorted(input_dict.keys(), key=lambda x: str(x)):
    filenames.sort(key=lambda filename: filename.lower())
    json = JSON.from_data({"date": date}, default=lambda d: d.isoformat())
        return stream.iter_libsvm(self.path, target_type=lambda x: x == "1")
        files.sort(key=lambda x: int(os.path.basename(x).split(".")[0][3:]))
            self._pivot = max(g.keys(), key=lambda y: f[y] / g[y])

            self._pivot = max(g.keys(), key=lambda y: g[y] / f[y])
        return min(self.code_book, key=lambda c: l1_dist(self.code_book[c], output))
@pytest.mark.parametrize("stat", load_stats(), ids=lambda stat: stat.__class__.__name__)

@pytest.mark.parametrize("stat", load_stats(), ids=lambda stat: stat.__class__.__name__)
        leaves.sort(key=lambda leaf: leaf.calculate_promise())
        pos = max(range(len(self.children)), key=lambda i: self.children[i].total_weight)

        pos = max(range(len(self.children)), key=lambda i: self.children[i].total_weight)
            self._min = min(self._buffer, key=lambda t: t[0])[0]

            _max = max(self._buffer, key=lambda t: t[0])[0]
        yield max(bandit.arms, key=lambda arm: upper_bounds[arm.index])
                    % (', '.join(sorted([e[0] for e in expected], key=lambda s: s.lower())),

                       ', '.join(sorted([t.name for t in tests], key=lambda s: s.lower())))

        tags = sorted(tags, key=lambda s: s.lower().replace('_', '').replace(' ', ''))
        for name in sorted(variables, key=lambda s: s[2:-1].lower()):
        for name in sorted(variables, key=lambda item: item.lower()):
        for name in sorted(names, key=lambda item: item.lower()):
            services = sorted(dev.services, key=lambda s: s.hndStart)

            services = sorted(dev.services, key=lambda s: s.hndStart)
    >>> first([1, 1, 3, 4, 5], key=lambda x: x % 2 == 0)
        self.assertEqual(4, first([1, 1, 3, 4, 5], key=lambda x: x % 2 == 0))
        sorted_by_time = sorted(start_times, key=lambda tup: tup[1])

        sorted_by_time = sorted(start_times, key=lambda tup: tup[1])
        self._mod_list.sort(key=lambda item: getattr(item[1], "priority", 100))
            reps = sorted(reps, reverse=True, key=lambda t: int(t.get('bandwidth')))
        query.subscribe(on_next=lambda x: self.write_message("Konami!"))
            ret = _flat_map_internal(source, mapper=lambda _: mapper)

            ret = _flat_map_internal(source, mapper=lambda _: mapper_indexed)
            reactivex.create(subscribe2).subscribe(on_error=lambda ex: _raise("ex"))
            return xs.pipe(ops.distinct_until_changed(comparer=lambda x, y: True))

            return xs.pipe(ops.distinct_until_changed(comparer=lambda x, y: False))

                ops.distinct_until_changed(comparer=lambda x, y: _raise(ex)),
            throw("ex").pipe(mapper).subscribe(on_error=lambda ex: _raise(ex))
            return xs.pipe(ops.reduce(accumulator=lambda acc, x: acc + x, seed=42))

            return xs.pipe(ops.reduce(accumulator=lambda acc, x: acc + x, seed=42))

            return xs.pipe(ops.reduce(accumulator=lambda acc, x: acc + x, seed=42))

            return xs.pipe(ops.reduce(accumulator=lambda acc, x: acc + x, seed=42))

            return xs.pipe(ops.reduce(accumulator=lambda acc, x: acc + x))

            return xs.pipe(ops.reduce(accumulator=lambda acc, x: acc + x))
        d = ys.subscribe(on_error=lambda ex: _raise("ex"), scheduler=scheduler2)

        d = ys.subscribe(on_error=lambda ex: _raise("ex"), scheduler=scheduler2)
        ys.subscribe(on_error=lambda ex: _raise("ex2"), scheduler=scheduler2)
            throw("ex").pipe(mapper).subscribe(on_error=lambda ex: _raise(ex))
            self.observers = sorted(self.observers, key=lambda x: -x.priority)
    return sorted(sub_ingredients, key=lambda x: -sub_ingredients[x])
        d = OrderedDict(sorted(d.items(), key=lambda t: t[0]))
COUNTRY_CHOICES = sorted(COUNTRY_CHOICES, key=lambda choice: choice[1])  # type: ignore
    addresses = sorted([address, address_usa], key=lambda address: address.pk)
    values_assignment.sort(key=lambda e: values_pks.index(e.value.pk))
    return sorted(items, key=lambda o: o[1])
        plugins = sorted(plugins, key=lambda p: p.name, reverse=sort_reverse)
        nodes.sort(key=lambda e: pks.index(str(e.pk)))  # preserve order in pks
    payload["order"]["lines"] = sorted(payload["order"]["lines"], key=lambda l: l["id"])

    payload["physical_lines"] = sorted(payload["physical_lines"], key=lambda l: l["id"])

    order_payload["lines"] = sorted(order_payload["lines"], key=lambda l: l["id"])
    lines.sort(key=lambda line: order.index(line.variant.pk))
        return sorted(choices, key=lambda x: x.code)
    lines = sorted(lines, key=lambda d: d.pk)

    addresses = sorted(customer.addresses.all(), key=lambda address: address.pk)

    products = sorted(collection.products.all(), key=lambda product: product.name)

    menu_items = sorted(menu.items.all(), key=lambda key: key.pk)
        for attribute_pk, attributechoice in sorted(values.items(), key=lambda x: x[0])
        self.ignore.sort(key=lambda x: -len(x))

        items = sorted(items, key=lambda item: item[1])

        content = sorted(content.items(), key=lambda k: k[0])
                self._send_req_async(load, timeout, callback=lambda f: None)
        for name in sorted(ver_info[ver_type], key=lambda x: x.lower()):
    sortedParameters = sorted(list(parameters.items()), key=lambda items: items[0])
        create_list = sorted(dmap["create"].items(), key=lambda x: x[1]["level"])
            o.sort(key=lambda option: option.name)
            "__loader__": ObjectDict(get_source=lambda name: self.code),
                locales.sort(key=lambda pair: pair[1], reverse=True)
        self.sync_future(callback=lambda future: 1 / 0)
                    server.read_bytes(1, callback=lambda data: 1 / 0)
        }, namespace={"_tt_modules": ObjectDict(Template=lambda path, **kwargs: loader.load(path).generate(**kwargs))})
    for diff, process in sorted(usage, key=lambda x: x[0], reverse=True):
    tmp.sort(key=lambda x: x.lstrip("-"))

        new_flags.sort(key=lambda x: x.lstrip("-"))
    return sorted(ret, key=lambda k: k["session_id"])
    for pkg_data in reversed(sorted(_ret, key=lambda x: LooseVersion(x["edition"]))):
    out_list = sorted(out_list, key=lambda x: sorted(x.keys()))
    snapshot_list = sorted(list_snapshots(config), key=lambda x: x["id"])
    sorted_types = sorted(_SERVICE_TYPES.items(), key=lambda x: (-x[1], x[0]))
        for w_ptr in sorted(_snapshots, key=lambda item: item["idx"], reverse=True)
        ret[pkgname] = sorted(ret[pkgname], key=lambda d: d["version"])
        _ret[pkgname] = sorted(ret[pkgname], key=lambda d: d["version"])
        for tname in sorted(data, key=lambda k: data[k].get("__run_num__", 0)):
    sorted_mappings = sorted(mappings, key=lambda m: (-len(m[0]), m[0]))
            sorted_targets = sorted(targets, key=lambda target: orders[saltenv][target])
        sorted_data = sorted(data.items(), key=lambda s: s[0])

    sorted_data = sorted(retdata.items(), key=lambda s: s[1].get("__run_num__", 0))
    sorted_data = sorted(returns.items(), key=lambda s: s[1].get("__run_num__", 0))
    for k in sorted(ret, key=lambda k: len(ret[k]["pool"]), reverse=direction):
    canonical_policy_repr = str(sorted(list(policy.items()), key=lambda x: str(x[0])))
            hostinterfaces = sorted(hostinterfaces, key=lambda k: k["main"])
    >>> sorted(L, key=lambda x: x.lower())

    return sorted(to_sort, key=lambda x: x.lower())
        for stag in sorted(running, key=lambda k: running[k].get("__run_num__", 0)):
        result = sorted(result.items(), key=lambda t: t[1]["__run_num__"])
    return sorted(mixins_or_funcs, key=lambda mf: getattr(mf, "_mixin_prio_", 1000))
SORTED_LEVEL_NAMES = [l[0] for l in sorted(LOG_LEVELS.items(), key=lambda x: x[1])]
            chunk["name"] for chunk in sorted(ret, key=lambda c: c.get("order"))
                for x in sorted(networks.nets, key=lambda y: y.name)
    XenAPI.xenapi.VM.get_is_a_template = MagicMock(side_effect=lambda x: vms[x])
    path_exists_mock = MagicMock(side_effect=lambda x: _path_exists_map[x])

    path_isfile_mock = MagicMock(side_effect=lambda x: _path_isfile_map.get(x, False))

    cmd_run_mock = MagicMock(side_effect=lambda x: _cmd_run_map[x])

    path_exists_mock = MagicMock(side_effect=lambda x: _path_exists_map[x])

    path_exists_mock = MagicMock(side_effect=lambda x: _path_exists_map[x])

    cmd_run_mock = MagicMock(side_effect=lambda x: _cmd_run_map[x])

        MagicMock(side_effect=lambda x: True if x == "/proc/1/cgroup" else False),

        MagicMock(side_effect=lambda x: True if x == "/proc/1/cgroup" else False),

        MagicMock(side_effect=lambda x: True if x == "/proc/1/cgroup" else False),

        isdir=MagicMock(side_effect=lambda x: x == "/proc"),

        MagicMock(side_effect=lambda v: v),

    path_isfile_mock = MagicMock(side_effect=lambda x: x in ["/etc/release"])

    path_exists_mock = MagicMock(side_effect=lambda x: _path_exists_map[x])

    path_isfile_mock = MagicMock(side_effect=lambda x: _path_isfile_map.get(x, False))

    cmd_run_mock = MagicMock(side_effect=lambda x: _cmd_run_map[x])
    isfile_mock = MagicMock(side_effect=lambda x: False if x == config else DEFAULT)

    isfile_mock = MagicMock(side_effect=lambda x: True if x == config else DEFAULT)
        mine, "_mine_send", MagicMock(side_effect=lambda x, y: x)

        mine, "_mine_send", MagicMock(side_effect=lambda x, y: x)

        mine, "_mine_send", MagicMock(side_effect=lambda x, y: x)

        mine, "_mine_send", MagicMock(side_effect=lambda x, y: x)

        mine, "_mine_send", MagicMock(side_effect=lambda x, y: x)
    which_mock = MagicMock(side_effect=lambda x: x)
@patch("os.path.realpath", MagicMock(wraps=lambda x: x))

@patch("os.path.realpath", MagicMock(wraps=lambda x: x))

@patch("os.path.realpath", MagicMock(wraps=lambda x: x))

@patch("os.path.realpath", MagicMock(wraps=lambda x: x))

@patch("os.path.realpath", MagicMock(wraps=lambda x: x))

@patch("os.path.realpath", MagicMock(wraps=lambda x: x))
    fopen = MagicMock(side_effect=lambda x, *args, **kwargs: MockFopen(x))

    cache_file = MagicMock(side_effect=lambda x, *args, **kwargs: x.split("/")[-1])

    with patch("os.path.expanduser", MagicMock(side_effect=lambda path: path)), patch(
    isdir_mock = MagicMock(side_effect=lambda path: DEFAULT if path != gitdir else True)
    version = MagicMock(side_effect=lambda pkgname, **_: pkgs[pkgname]["old"])

    version = MagicMock(side_effect=lambda pkgname, **_: pkgs[pkgname]["old"])

    version = MagicMock(side_effect=lambda pkgname, **_: pkgs[pkgname]["old"])
def http_basic_auth(login_cb=lambda username, password: False):
        listdict1_sorted = sorted(listdict1, key=lambda x: x[sortkey])

        listdict2_sorted = sorted(listdict2, key=lambda x: x[sortkey])
                prepare=lambda _: MagicMock(bind=lambda _: None),
                    "config.option": Mock(side_effect=lambda key, default: default),
                expected_result, sorted(result.items(), key=lambda x: x[0])

                expected_result, sorted(result.items(), key=lambda x: x[0])

                expected_result, sorted(result.items(), key=lambda x: x[0])
        sysv_enabled_mock = MagicMock(side_effect=lambda x, _: x == "baz")

        sysv_enabled_mock = MagicMock(side_effect=lambda x, _: x == "baz")

        sysv_enabled_mock = MagicMock(side_effect=lambda x, _: x == "baz")

        mock = MagicMock(side_effect=lambda x: _SYSTEMCTL_STATUS[x])

        mock = MagicMock(side_effect=lambda x: _SYSTEMCTL_STATUS[x])
        self.sam_mock = MagicMock(side_effect=lambda x: "HOST\\" + x)
                with patch.object(glob, "glob", MagicMock(side_effect=lambda x: [x])):
    props = sorted(props, key=lambda x: x.intensity_max)[::-1]
    triang1 = [np.concatenate(sorted(t, key=lambda x:tuple(x)))

    triang2 = [np.concatenate(sorted(t, key=lambda x:tuple(x)))
        ln.sort(key=lambda x: x[0])
entries = sorted(entries, key=lambda x: x[0].split()[-1])
            matches.sort(key=lambda x: -x[0])
        .assign(observed=lambda x: x["observed"] / x[weight])

        .assign(predicted=lambda x: x["predicted"] / x[weight])
    sp = SpectralClustering(n_clusters=2, affinity=lambda x, y: 1, random_state=0)
        new_feature_idx = max(scores, key=lambda feature_idx: scores[feature_idx])
    vec = Estimator(preprocessor=lambda x: x["text"], stop_words=["and"])

        Estimator(analyzer=lambda x: x.split(), input=input_type).fit_transform(data)
    model = SelectFromModel(estimator, prefit=True, max_features=lambda X: X.shape[1])
@pytest.mark.parametrize("imputer", IMPUTERS, ids=lambda x: x.__class__.__name__)
    assert pairwise_distances([[1.0]], metric=lambda x, y: 5)[0, 0] == 5

    S2 = paired_distances(X, Y, metric=lambda x, y: np.abs(x - y).sum(axis=0))
    svm = SVC(kernel=lambda x, y: np.dot(x, y.T))

        cross_val_score(clf, X, y, scoring=lambda est, X, y: score)

            cross_val_score(clf, X, y, scoring=lambda est, X, y: scores)
def _make_func(args_store, kwargs_store, func=lambda X, *a, **k: X):
    a = svm.SVC(C=1, kernel=lambda x, y: x * y.T, probability=True, random_state=0)
    clf = svm.SVR(kernel=lambda x, y: np.array([[1.0]]))

    svc = svm.SVC(kernel=lambda x, y: x)
    >>> clf = CheckingClassifier(check_X=lambda x: x.shape == (150, 4))
        results = sorted(results, key=lambda x: (x.nfail, x.mean_time))
            dendrogram(Z, link_color_func=lambda k: colors[k])
    >>> T3, Z3, sdim = schur(A, output='complex', sort=lambda x: x.imag > 0)
        s, u, sdim = schur(a, sort=lambda x: x >= 0.0)

        assert_raises(ValueError, qz, A, B, sort=lambda ar, ai, beta: ai == 0)

            AA, BB, Q, Z, sdim = qz(A, B, sort=lambda ar, ai, beta: ai == 0)
        return Jacobian(matvec=lambda v: dot(J, v),
    ...            for i in range(30)], key=lambda x: x.fun)
    >>> root = optimize.newton(f, 1.5, fprime2=lambda x: 6 * x)

    >>> root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2)
    assert_wolfe(s, phi=lambda sp: f(x + p*sp),

    assert_armijo(s, phi=lambda sp: f(x + p*sp), **kw)
        f2, p2 = welch(x, nperseg=10, detrend=lambda x: x)

        f2, p2 = csd(x, x, nperseg=10, detrend=lambda x: x)
    A = interface.LinearOperator(shape=(1, 1), matvec=lambda x: 1)
        CU.sort(key=lambda cu: cu[0] is not None)
        variants.sort(key=lambda v: cast_order(v[2]))

    ufuncs.sort(key=lambda u: u.name)
    p = orthopoly1d(x, w, hn, kn, wfunc=lambda x: 1.0, limits=(-1, 1),
            mode = max(cntr, key=lambda x: cntr[x])

    >>> tau, _ = stats.weightedtau(x, y, weigher=lambda x: 1)
    tau, p_value = stats.weightedtau(x, y, weigher=lambda x: 1)

    tau, p_value = stats.weightedtau(x, y, rank=True, weigher=lambda x: 1)

    tau, p_value = stats.weightedtau(y, x, rank=True, weigher=lambda x: 1)
    threads = [threading.Thread(target=lambda k=k: worker(k))
    authors = sorted(authors.items(), key=lambda i: name_key(i[0]))
    return sorted(targets.values(), key=lambda job: job.end, reverse=True)
    file_paths.sort(key=lambda x: list(map(int, (x.split('-')[0].split('.')))))
def unique(list_, key=lambda x: x):
        r = Request("http://www.example.com", callback=lambda x: x)
                         sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))

                         sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))
        self.assertEqual(build_component_list(d, convert=lambda x: x),

            build_component_list(None, duplicate_list, convert=lambda x: x)

        self.assertEqual(build_component_list(d, convert=lambda x: x),

        self.assertEqual(build_component_list(d, convert=lambda x: x),

        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)

        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)

        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)

        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)

        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)
        dfd = getPage(self.getURL('encoding'), body=body, response_transform=lambda r: r)
        p.scale_categorical("x", formatter=lambda x: f"{x:%}")
        parser.set_defaults(func=lambda args: parser.print_help())
        collection.sort(key=lambda x: int(x.filename.split('-')[0]))
        file for (file, size) in sorted(disconnected_files_and_sizes, key=lambda t: t[1])
        sorted_list = sorted(self.items, key=lambda item: item.score, reverse=True)
    monitors = sorted(list(monitor_dict.values()), key=lambda item: item.audit_tier, reverse=True)
		leftmost_button = sorted(buttons, key=lambda button: button.x)[0]
        sources.sort(key=lambda s: s["name"])
                [config.as_dict() for config in sorted(CONFIGURATIONS.values(), key=lambda x: x.id)]
                    "tags": sorted(serialize(tags, request.user), key=lambda x: x["name"]),
    rv.sort(key=lambda tree: (tree["id"] or "", tree["childId"] or ""))
        queryset = sorted(ApiKey.objects.filter(organization=organization), key=lambda x: x.label)
            for org in sorted(queryset, key=lambda x: x.name):
        results.sort(key=lambda x: x["name"])
            config.as_dict() for config in sorted(CONFIGURATIONS.values(), key=lambda x: x.id)
        files.sort(key=lambda item: item[0])
        providers.sort(key=lambda i: i.key)
            serialized_plugin["projectList"].sort(key=lambda x: x["projectSlug"])
            "installStats": sorted(install_stats.items(), key=lambda x: x[0]),

            "uninstallStats": sorted(uninstall_stats.items(), key=lambda x: x[0]),
    group_list.sort(key=lambda g: (g.times_seen, g.id))
            for key, group in itertools.groupby(result.data["data"], key=lambda r: r["time"])
        interface_list.sort(key=lambda x: x[0].get_display_score(), reverse=True)
        project_list = sorted(other_projects + member_projects, key=lambda x: x.slug)  # type: ignore

        team_list = sorted(other_teams + member_teams, key=lambda x: x.slug)  # type: ignore
    AUTHENTICATOR_CHOICES.sort(key=lambda x: x[0])
        rv.sort(key=lambda x: x["name"])
        items.sort(key=lambda x: (len(x), x))
    flat_variants.sort(key=lambda name_and_variant: 1 if name_and_variant[0] == "system" else 0)
        return iter(sorted(self.strategies.values(), key=lambda x: x.score and -x.score or 0))
        self.triggers.sort(key=lambda trigger: trigger.alert_threshold)
        functions.sort(key=lambda x: x["FunctionName"].lower())

        lambda_functions.sort(key=lambda x: x["FunctionName"].lower())
        keys = sorted(ProjectKey.objects.filter(project=project), key=lambda x: x.public_key)
        dynamic_fields.sort(key=lambda f: anti_gravity.get(f, (0, f)))
        (k, v) for k, v in sorted(result, key=lambda x: x[1].get_score(), reverse=True)
        >>>     name = Param(str, default=lambda self: self.user['name'])
    scopes = Param(Iterable, default=lambda self: [])

    events = Param(Iterable, default=lambda self: [])

    schema = Param(dict, default=lambda self: {})

    allowed_origins = Param(Iterable, default=lambda self: [])
    scopes = Param(Iterable, default=lambda self: [])

    events = Param(Iterable, default=lambda self: [])

    schema = Param(dict, default=lambda self: {})

    allowed_origins = Param(Iterable, default=lambda self: [])
            return sorted(x, key=lambda x: (x.type == 0, x.type))
        return sorted(project_list, key=lambda x: x.name.lower())
        commit_list.sort(key=lambda commit: commit.get("timestamp", 0), reverse=True)
        results = sorted(team_list, key=lambda x: x.name.lower())
            size = min(self.ALLOWED_SIZES, key=lambda x: abs(x - size))
    return max(notification_settings_by_provider.values(), key=lambda v: v.value)
    return sorted(commits.values(), key=lambda x: float(x["score"]), reverse=True)
        for plugin in sorted(super().all(), key=lambda x: x.get_title()):
        max_relative_change = max(relative_changes, key=lambda x: abs(x))

    return tuple(sorted(d["by"].items(), key=lambda t: t[0]))  # type: ignore
        self._priority_seq = tuple(sorted(roles, key=lambda r: r.priority))
        choices=list(sorted(comparison_types.items(), key=lambda item: item[1])),

            for key, (label, _) in sorted(comparison_intervals.items(), key=lambda item: item[1][1])
                    for model, deps in sorted(skipped, key=lambda obj: obj[0].__name__)
        queues = sorted(queues, key=lambda q: (-q[1], q[0]), reverse=reverse)
            itertools.groupby(exception.stacktrace.frames, key=lambda frame: frame.in_app),
        rows.sort(key=lambda row: row[ts_col])
    return sorted(results, key=lambda result: (str(result.key), str(result.value)))
        tags_or_values.sort(key=lambda tag: (tag["key"], tag["value"]))
    return sorted(problems, key=lambda i: (-Problem.SEVERITY_LEVELS[i.severity], i.message))
    series.sort(key=lambda group_id__count: group_id__count[1], reverse=True)

        RangeQuerySetWrapper(organizations, step=10000, result_value_getter=lambda item: item)
                    for i_id, i_data in sorted(iteritems(integrations), key=lambda x: x[1]["name"])

    platform_list.sort(key=lambda x: x["name"])
        for k in sorted(value.keys(), key=lambda x: (len(force_text(value[x])), x)):
def soft_break(value, length, process=lambda chunk: chunk):
    previous, *time_windows = sorted(time_windows, key=lambda window: window.as_tuple())
            all_requests.sort(key=lambda x: parse_date(x["date"]), reverse=True)
        dynamic_fields.sort(key=lambda f: anti_gravity.get(f) or 0)
        for span_id, child in sorted(span_tree.items(), key=lambda item: item[0]):
        data = sorted(response.data, key=lambda r: r["key"])

        data = sorted(response.data, key=lambda r: r["key"])
        assert list(sorted(response.data, key=lambda item: item["project"])) == expected
        projects.sort(key=lambda project: project.slug)
        commits.sort(key=lambda c: c.date_added)
        expected.sort(key=lambda search: (not search.is_pinned, search.name.lower()))
        response.data.sort(key=lambda x: x["id"])

        response.data.sort(key=lambda x: x["id"])
            return sorted(releases, key=lambda release: release["version"])

        commits.sort(key=lambda c: c.date_added)
    "config_name", sorted(CONFIGURATIONS.keys()), ids=lambda x: x.replace("-", "_")
@pytest.mark.parametrize("config_name", CONFIGURATIONS.keys(), ids=lambda x: x.replace("-", "_"))
@pytest.mark.parametrize("input", INPUTS, ids=lambda x: x.filename[:-5].replace("-", "_"))
    strategies = sorted(CONFIGURATIONS.keys(), key=lambda x: x.split(":")[-1])
    name = Param((str,), default=lambda self: self.user["name"])
        name = Param((str,), default=lambda self: _name)

        name = Param((str,), default=lambda self: self.user["name"])
        return tuple(sorted(d["by"].items(), key=lambda t: t[0]))  # type: ignore
        snql_query.query.select.sort(key=lambda q: q.function)
        fn = DiscoverFunction("fn", transform="", result_type_fn=lambda *_: None)

        fn = DiscoverFunction("fn", transform="", result_type_fn=lambda *_: "number")

        fn = DiscoverFunction("fn", transform="", result_type_fn=lambda *_: None, private=True)
                for x in sorted(data, key=lambda k: k["transaction"])

        sorted_data = sorted(result["data"], key=lambda k: k["transaction"])

        sorted_data = sorted(result["data"], key=lambda k: k["transaction"])

                x[alias] for x in sorted(data, key=lambda k: k["transaction"])

                x[alias] for x in sorted(data, key=lambda k: k["transaction"])

            for i, misery in enumerate(sorted(data, key=lambda k: k["transaction"])):
        snql_query.query.select.sort(key=lambda q: q.function)
        commits = sorted(((fc.commit, 2) for fc in file_changes), key=lambda fc: fc[0].id)
        retry = TimedRetryPolicy(0.3, delay=lambda i: 0.1)

        retry = TimedRetryPolicy(0.3, delay=lambda i: 0.1)

        @TimedRetryPolicy.wrap(0.3, delay=lambda i: 0.1)
        assert get_path(data, "a", filter=lambda x: x) == [1]

        assert get_path(data, "a", filter=lambda x: x) == [1]
        return tuple(sorted(d.items(), key=lambda t: t[0]))

        result["groups"].sort(key=lambda group: stable_dict(group["by"]))
        data.sort(key=lambda val: val["totalValues"], reverse=True)
        return tuple(sorted(d.items(), key=lambda t: t[0]))

    result["groups"].sort(key=lambda group: stable_dict(group["by"]))
        mock_callback = Mock(side_effect=lambda *a, **k: consumer.shutdown())

        mock_callback = Mock(side_effect=lambda *a, **k: consumer.shutdown())
        result.sort(key=lambda r: r.key)

        result.sort(key=lambda r: r.key)

        result.sort(key=lambda x: x.last_seen)
        return tuple(sorted(d.items(), key=lambda t: t[0]))

    result["groups"].sort(key=lambda group: stable_dict(group["by"]))
        return sorted(EventAttachment.objects.filter(event_id=event.event_id), key=lambda x: x.name)
        table.sort(key=lambda x: int(a % (ord(x) + i)))
        table.sort(key=lambda x: int(a % (ord(x) + i)))
            members="\n   ".join(sorted(all_members, key=lambda s: s.split(".")[-1])),
        @lambda_mapper(memoize=True, memoize_key=lambda x: x.uid)
        self.top = sorted(self.top, key=lambda x: x[1], reverse=True)

            tmp = sorted(tmp, key=lambda x: x[1] / len(self.words[x[0]]))

        self.top = sorted(self.top, key=lambda x: x[1], reverse=True)
        samples = sorted(self.tri.samples(), key=lambda x: self.tri.get(x)[1])

        return zip(data, max(now, key=lambda x: x[1])[2])
            now = heapq.nlargest(self.N, stage, key=lambda x: x[1])

        now = heapq.nlargest(1, stage, key=lambda x: x[1]+self.geteos(x[0][1]))
    result = sorted(result, key=lambda span: span.start)
                pred_label, pred_score = max(pred_cats.items(), key=lambda it: it[1])

                gold_label, gold_score = max(gold_cats.items(), key=lambda it: it[1])

                gold_label, gold_score = max(gold_cats, key=lambda it: it[1])

                pred_label, pred_score = max(pred_cats.items(), key=lambda it: it[1])
            entities = sorted(token["entities"], key=lambda d: d["label"])

        for arc in sorted(arcs, key=lambda arc: arc["end"] - arc["start"]):
                doc["ents"] = sorted(doc["ents"], key=lambda x: (x["start"], x["end"]))
    Doc.set_extension("json_test4", method=lambda doc: doc.text)
    token = Mock(doc=Mock(), idx=7, say_cheese=lambda token: "cheese")
    Token.set_extension("a", getter=lambda x: x, force=True)

    Token.set_extension("b", method=lambda x: x, force=True)
    Token.set_extension("a", getter=lambda x: x, force=True)

    Token.set_extension("b", method=lambda x: x, force=True)
    Language.factory(name, func=lambda nlp, name: Component())
    Language.component("new_pipe2", func=lambda doc: doc)

    Language.component("n_pipes", func=lambda doc: doc)
    Doc.set_extension("_test_prop", getter=lambda doc: len(doc.text))

    Doc.set_extension("_test_method", method=lambda doc, arg: f"{len(doc.text)}{arg}")
    candidates = sorted(kb.get_alias_candidates("double07"), key=lambda x: x.entity_)
    batch.sort(key=lambda eg: len(eg.predicted))
        recipes = sorted(recipes, key=lambda recipe: recipe[0])
            ctx['script_files'] = sorted(ctx['script_files'], key=lambda js: js.priority)

            ctx['css_files'] = sorted(ctx['css_files'], key=lambda css: css.priority)
        classes.sort(key=lambda cls: cls.priority)
        """__init__(o, sentinel=None, modifier=lambda x: x)"""
            classes.sort(key=lambda cls: cls.priority)

            documenters.sort(key=lambda e: (e[0].member_order, e[0].name))

            documenters.sort(key=lambda e: e[0].name)

            documenters.sort(key=lambda e: e[0].name)
        self._line_iter = modify_iter(lines, modifier=lambda s: s.rstrip())
        it = modify_iter(a, modifier=lambda s: s.rstrip())

        it = modify_iter(a, modifier=lambda s: s.rstrip())
            self.__moduleInstances = OrderedDict(sorted(self.__moduleInstances.items(), key=lambda m: m[-1]._priority))

        modules_waiting = sorted(modules_waiting.items(), key=lambda x: x[-1], reverse=True)
        workbook._sheets.sort(key=lambda ws: ws.title)
            self._listenerModules.sort(key=lambda m: m._priority)
    albums.sort(key=lambda album: album['name'].lower())
            self.breakpoints.sort(key=lambda breakp: int(breakp[COL_LINE]))

            self.breakpoints.sort(key=lambda breakp: breakp[COL_FILE])
        return sorted(providers, key=lambda p: request_order[p])
        self.servers = sorted(self.servers, key=lambda x: x.language)
    resp_snippets = sorted(resp_snippets, key=lambda x: x['label'])

    expected_snippets = sorted(expected_snippets, key=lambda x: x['label'])
        self.snippets = sorted(self.snippets, key=lambda x: x.trigger_text)
        completions += sorted(underscore, key=lambda x: str_lower(x[0]))
        panels.sort(key=lambda panel: panel.order_in_zone, reverse=True)

        panels.sort(key=lambda panel: panel.order_in_zone, reverse=True)

        panels.sort(key=lambda panel: panel.order_in_zone)

        panels.sort(key=lambda panel: panel.order_in_zone)
        sorted_code_analysis = sorted(code_analysis, key=lambda i: i[2])
        for app in sorted(apps, key=lambda x: x.lower()):
        sorted_data = sorted(layout_data, key=lambda x: (x[0], x[1]))
                attrs.sort(key=lambda t: t[0])
        shortcuts = sorted(shortcuts, key=lambda item: item.context+item.name)
        results = sorted(results, key=lambda row: row[-1])
    for software in sorted(software_list, key=lambda x: x[sort_key]):

    for software in sorted(software_list, key=lambda x: x[sort_key]):

    for entry_data in sorted(all_entries_data, key=lambda x: x['name']):
                elements.sort(key=lambda _: _.lower() if hasattr(_, "lower") else _)

        users.sort(key=lambda _: _.lower() if hasattr(_, "lower") else _)

                    colList.sort(key=lambda _: _.lower() if hasattr(_, "lower") else _)

                    tables.sort(key=lambda _: _.lower() if hasattr(_, "lower") else _)

            columns = sorted(columns, key=lambda _: cols.index(_) if _ in cols else 0)
        kb.cache.intBoundaries = kb.cache.intBoundaries or sorted(copy.deepcopy(conf.boundaries), key=lambda boundary: any(_ in (boundary.prefix or "") or _ in (boundary.suffix or "") for _ in ('"', '\'')))

        kb.cache.alphaBoundaries = kb.cache.alphaBoundaries or sorted(copy.deepcopy(conf.boundaries), key=lambda boundary: not any(_ in (boundary.prefix or "") or _ in (boundary.suffix or "") for _ in ('"', '\'')))
        candidates.sort(key=lambda _: _[0], reverse=True)
    validTechniques = sorted(getPublicTypeMembers(PAYLOAD.TECHNIQUE), key=lambda x: x[1])
                    choice = sorted(bits.items(), key=lambda _: abs(_[1]))[0][0]
    colList = filterNone(sorted(colList, key=lambda x: len(x) if x else MAX_INT))
    instances = sorted(instances, key=lambda x: x.start_timestamp)
            tk_ex_db = sorted(tk_ex_dbs, key=lambda x: x.start_timestamp)[
        tk1_ex_dbs = sorted(tk1_ex_dbs, key=lambda x: x.start_timestamp)

        tk1_ex_dbs = sorted(tk1_ex_dbs, key=lambda x: x.start_timestamp)

        tk2_ex_dbs = sorted(tk2_ex_dbs, key=lambda x: x.start_timestamp)

        tk3_ex_dbs = sorted(tk3_ex_dbs, key=lambda x: x.start_timestamp)
            components.sort(key=lambda resource: resource.updated_at)
        instances = sorted(instances, key=lambda k: k.url)
        for route in sorted(self.routes.matchlist, key=lambda r: r.routepath):
        return sorted(self._result, key=lambda execution: execution.start_timestamp)
    errors = sorted(errors, key=lambda e: (e["type"], e["schema_path"]))

        completed_tasks = sorted(completed_tasks, key=lambda x: x.end_timestamp)
        return sorted(errors, key=lambda x: x.get("task_id", None))
    for opt in sorted(options, key=lambda x: x["opt"].name):
    alerts.sort(key=lambda alert: alert[3], reverse=True)
    model.learn(500, callback=lambda _locals, _globals: True)
parser.add_argument('--split', type=lambda x: Split[x.upper()], default=Split.TRAIN_DEV_TEST,
    parser.add_argument('--wordvec_type', type=lambda x: WVType[x.upper()], default='word2vec', help='Different vector types have different options, such as google 300d replacing numbers with #')

    parser.add_argument('--extra_wordvec_method', type=lambda x: ExtraVectors[x.upper()], default='none', help='How to train extra dimensions of word vectors, if at all')
    parser.add_argument('--transition_scheme', default=TransitionScheme.IN_ORDER, type=lambda x: TransitionScheme[x.upper()],

    parser.add_argument('--sentence_boundary_vectors', default=SentenceBoundary.EVERYTHING, type=lambda x: SentenceBoundary[x.upper()],

    parser.add_argument('--constituency_composition', default=ConstituencyComposition.MAX, type=lambda x: ConstituencyComposition[x.upper()],
    parser.add_argument('--dev_eval_scoring', type=lambda x: DevScoring[x.upper()], default=DevScoring.ACCURACY,

    parser.add_argument('--loss', type=lambda x: Loss[x.upper()], default=Loss.CROSS,

                        items.sort(key=lambda x: -x[0])
        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: (counter[k], k), reverse=True))
        ordered = sorted(enumerate(data), key=lambda x: key(x[1]), reverse=reverse)

        ordered = sorted(enumerate(data), key=lambda x: x[1], reverse=reverse)

    backidx = [x[0] for x in sorted(enumerate(oidx), key=lambda x: x[1])]
        epoch_data.sort(key=lambda x: len(x[1]))
        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))
        self._id2unit = constant.VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))
        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))
        self._id2unit = [PAD, UNK] + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))
        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))
    paragraphs = list(sorted(paragraphs, key=lambda x: x[3], reverse=True))
    for processor in sorted(ending_to_processor.keys(), key=lambda x: -len(x)):
    Word.add_property('upos_xpos', getter=lambda self: f"{self.upos}_{self.xpos}")

    Sentence.add_property('classname', getter=lambda self: int2str[self._classname] if self._classname is not None else None, setter=setter)
    def loo_likelihood(self, bw, func=lambda x: x):

    def loo_likelihood(self, bw, func=lambda x: x):
def expect_mc(dist, func=lambda x: 1, size=50000):

    def expect(self, func=lambda x: 1, lower=(-10,-10), upper=(10,10)):

def quad2d(func=lambda x: 1, lower=(-10,-10), upper=(10,10)):
        CustomKernel.__init__(self, shape=lambda x: 0.5 * np.ones(x.shape), h=h,

        CustomKernel.__init__(self, shape=lambda x: 1 - abs(x), h=h,

        CustomKernel.__init__(self, shape=lambda x: 0.75*(1 - x*x), h=h,

        CustomKernel.__init__(self, shape=lambda x: 0.9375*(1 - x*x)**2, h=h,

        CustomKernel.__init__(self, shape=lambda x: 1.09375*(1 - x*x)**3, h=h,

        CustomKernel.__init__(self, shape=lambda x: 1 + np.cos(2.0 * np.pi * x)

        CustomKernel.__init__(self,shape=lambda x: 0.864197530864 * (1 - abs(x)**3)**3,
    r, e = inner_cont(polysc, -1, 1, weight=lambda x: (1-x*x)**(-1/2.))
    for s in sorted(list(set(ssli)), key=lambda x: len(set(x)))[::-1]:
    ics = sorted(ics, key=lambda x: x[1][key_loc])

        aic = sorted(ics, key=lambda r: r[1][0])

        bic = sorted(ics, key=lambda r: r[1][1])

        hqic = sorted(ics, key=lambda r: r[1][2])
API_CLASSES = sorted(set(API_CLASSES), key=lambda v: v[0])
merged_pull_data = sorted(merged_pull_data, key=lambda x: x["number"])
    wave_crest = heapq.nlargest(5, enumerate(arr), key=lambda x: x[1])

    wave_base = heapq.nsmallest(5, enumerate(arr), key=lambda x: x[1])
    wave_crest = heapq.nlargest(max_point, enumerate(arr), key=lambda x: x[1])

    wave_base = heapq.nsmallest(max_point, enumerate(arr), key=lambda x: x[1])
        aggregated_activities.sort(key=lambda a: a.updated_at, reverse=True)
        for channel in sorted(res, key=lambda x: x['channelid']):
    def walk_back(self, cls=None, f=lambda x: x):

        self.bandwidth = self.attr("bandwidth", parser=lambda b: float(b) / 1000.0, required=True)
        streams = plugin.streams(sorting_excludes=lambda q: not q.endswith("p"))

        streams = plugin.streams(sorting_excludes=lambda q: False)
            self.content(segments, cond=lambda s: s.num >= 4),

            self.content(segments, cond=lambda s: s.num != 2 and s.num != 3),

            self.content(segments, cond=lambda s: s.num >= 4),

            self.content(segments, cond=lambda s: s.num < 8),

            self.content(segments, cond=lambda s: s.num > 1),
        self.assertEqual(data, self.content(segments, cond=lambda s: 0 < s.num < 3), "Respects the offset and duration")

        expected = self.content(segments, prop="content_plain", cond=lambda s: s.num >= 1)

        expected = self.content(segments, prop="content_plain", cond=lambda s: s.num >= 1)
            self.content(segments, cond=lambda s: s.num % 4 > 1),

        self.assertEqual(data, self.content(segments, cond=lambda s: s.num >= 2))

        self.assertEqual(self.await_read(), self.content(segments, cond=lambda s: s.num % 2 > 0))
    assert update_qsd("http://test.se?foo=?", {"bar": "!"}, quote_via=lambda s, *_: s) == "http://test.se?foo=?&bar=!", \
i2 = st.multiselect("multiselect 2", options, format_func=lambda x: x.capitalize())
i2 = st.radio("radio 2", options, 0, format_func=lambda x: x.capitalize())
i2 = st.selectbox("selectbox 2", options, 0, format_func=lambda x: x.capitalize())
                        sorted(cargs[1].items(), key=lambda t: t[0])  # type: ignore
                st.radio("radio", ["a", "b", "c"], 0, on_change=lambda x: x)

            st.form_submit_button(on_click=lambda x: x)
        st.multiselect("the label", arg_options, format_func=lambda x: x["name"])
        st.radio("the label", arg_options, format_func=lambda x: x["name"])
        st.selectbox("the label", arg_options, format_func=lambda x: x["name"])
    return sorted(args_list, key=lambda args: args[0])
        user_ratings.sort(key=lambda x: x[0], reverse=True)
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        neighbors = sorted(neighbors, key=lambda x: x[1], reverse=True)
        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[0])

        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[1])

        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[1])

        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[1])
        others.sort(key=lambda tple: tple[1], reverse=True)
        for i in sorted(new_authors, key=lambda x: x.lower()):
    c_missing_doc = sorted(c_missing_doc, key=lambda x: int(x.split()[1][:-1]))

    c_missing_doctest = sorted(c_missing_doctest, key=lambda x: int(x.split()[1][:-1]))

    c_indirect_doctest = sorted(c_indirect_doctest, key=lambda x: int(x.split()[1][:-1]))

    f_missing_doc = sorted(f_missing_doc, key=lambda x: int(x.split()[1][:-1]))

    f_missing_doctest = sorted(f_missing_doctest, key=lambda x: int(x.split()[1][:-1]))

    f_indirect_doctest = sorted(f_indirect_doctest, key=lambda x: int(x.split()[1][:-1]))
    references.sort(key=lambda x: -len(x))
    timings.sort(key=lambda x: -x[0])
    for optim in sorted(optimizations, key=lambda opt: opt.priority, reverse=True):

def create_expand_pow_optimization(limit, *, base_req=lambda b: b.is_symbol):

    >>> opt2 = create_expand_pow_optimization(3, base_req=lambda b: not b.is_Function)
        >>> Permutation.from_sequence('SymPy', key=lambda x: x.lower())

            ic.sort(key=lambda x: key(x[0]))
            orbits.sort(key=lambda x: -len(x))

            sorted_orbits[i].sort(key=lambda point: base_ordering[point])

                temp_orbit.sort(key=lambda point: base_ordering[point])

        gens = sorted(gens, key=lambda x: x.order(), reverse=True)
    items.sort(key=lambda x: len(x[1]), reverse=True)
        TAB1.sort(key=lambda x: x[-1])
    l.sort(key=lambda w: default_sort_key(w, order='rev-lex'))
    assert Permutation.from_sequence('SymPy', key=lambda x: x.lower()) == \
        >>> sorted([S(1)/2, I, -I], key=lambda x: x.sort_key())

        >>> sorted(_, key=lambda x: x.sort_key())
            surds.sort(key=lambda x: -x.args[0])

        terms = sorted(Add.make_args(s.removeO()), key=lambda i: int(i.as_coeff_exponent(k)[1]))
        ix = topological_sort((V, E), key=lambda i: O[v(i)])
    def __init__(self, transform, filter=lambda x: True):
    >>> sorted(eq.args, key=lambda i: default_sort_key(i, order='rev-lex'))
        cpart.sort(key=lambda expr: expr.sort_key(order=order))
    >>> uniquely_named_symbol('x', x, modify=lambda s: 2*s)
    assert sorted([-m, s], key=lambda arg: arg.sort_key()) == [-m, s]
            min_distance = max(path_dict.values(), key=lambda x:x[0])[0]
    return ''.join(sorted(message, key=lambda i: next(p)))

    idx = sorted(range(len(ciphertext)), key=lambda i: next(p))
    com = sorted(com, key=lambda x: x[0])
    return tuple(sorted(p, key=lambda x: x.args))

        p.sort(key=lambda x: x.args)

        p.sort(key=lambda x: x.args)
                name, (self, line), modify=lambda s: '_' + s, real=True)
        power_gens = sorted(power_gens.items(), key=lambda k: str(k[0]))
        compl.sort(key=lambda x : x[1])

        compl.sort(key=lambda x : x[2])
    conds.sort(key=lambda x: (x[0] - x[1], count_ops(x[2])))

    conds.sort(key=lambda x: (-x[0], cnt(x[1])))
    return sorted(u, key=lambda x: x.sort_key(), reverse=r)
    return sorted(iter(newterms.items()), key=lambda item: item[0].sort_key())

            others.sort(key=lambda i: i[1])
        x = uniquely_named_symbol(x, diagonal_elements, modify=lambda s: '_' + s)

    x = uniquely_named_symbol(x, berk_vector, modify=lambda s: '_' + s)
        eigenvects = sorted(eigenvects, key=lambda x: default_sort_key(x[0]))
        x = uniquely_named_symbol('x', self, modify=lambda s: '_' + s)
    >>> m.rref(iszerofunc=lambda x:abs(x)<1e-9)
        return [tuple(k + (self[k],)) for k in sorted(list(self.todok().keys()), key=lambda k: list(reversed(k)))]
    MatrixClass = max(matrices, key=lambda M: M._class_priority).__class__
                indmin = min(range(len(trace_arg.args)), key=lambda x: default_sort_key(trace_arg.args[x]))
    assert m.is_anti_symmetric(simplify=lambda x: x) is False
    assert isinstance(m.eigenvals(simplify=lambda x: x, multiple=False), dict)

    assert isinstance(m.eigenvals(simplify=lambda x: x, multiple=True), list)

    assert count_ops(m.eigenvals(simplify=lambda x: x)) > \
        assert A.inv(method=method, iszerofunc=lambda x: x == 0) == \

    assert m.is_anti_symmetric(simplify=lambda x: x) is False

    m_rref = m.rref(iszerofunc=lambda x: abs(x)<6e-15)[0]
        tokens_escape.sort(key=lambda x: -len(x))
        parse_expr('x', transformations=lambda x,y: 1))
        dumstruct.sort(key=lambda x: keydict[x])

    result = sorted(all_dums, key=lambda x: dumkey[x])

        return min(a, b, key=lambda x: default_sort_key(_get_indices(x, ind)))
    #     spinor_free.sort(key=lambda x: x[2])
    return sorted(set(rv), key=lambda x: (len(x), x))
            keys = sorted(d.keys(), key=lambda x: x.index)
        return sorted(terms, key=lambda term: O(term[0]), reverse=True)
    return sorted(f, key=lambda term: O(term[0]), reverse=True)

        g, _, gp = min(Th, key=lambda x: x[1])

    L = sorted(((list(f), i) for f, i in S), key=lambda p: O(sdm_LM(p[0])),
    plot(cos(x), line_color=lambda a: a)

    plot_parametric(cos(x), sin(x), line_color=lambda a: a)

    plot3d_parametric_line(cos(x), sin(x), x, line_color=lambda a: a)
    L.sort(key=lambda k_l: O_to(_incr_k(S[k_l[1]], k_l[0])), reverse=True)

            L.sort(key=lambda k_l: O_to(_incr_k(S[k_l[1]], k_l[0])), reverse=True)

            return sorted(G, key=lambda g: O_to(g.LM), reverse=True)
    p, fsqf = min(a, key=lambda x: len(x[1]))
        pr = min(P, key=lambda pair: order(monomial_lcm(f[pair[0]].LM, f[pair[1]].LM)))

        h = min([f[x] for x in F], key=lambda f: order(f.LM))

        G1 = sorted(G, key=lambda g: order(f[g].LM))

    Gr = sorted(Gr, key=lambda f: order(f.LM), reverse=True)

    B.sort(key=lambda f: order(Polyn(f).LM), reverse=True)

    CP.sort(key=lambda cp: cp_key(cp, ring), reverse=True)

            CP.sort(key=lambda cp: cp_key(cp, ring), reverse=True)

    return sorted(H, key=lambda f: order(f.LM), reverse=True)

    G.sort(key=lambda g: order(g.LM))

    G.sort(key=lambda g: order(g.LM))
        items2.sort(key=lambda e: e[0][iv])

    items.sort(key=lambda e: e[0][iv])

    m = min(p, key=lambda k: k[index])[index]

    m = min(p, key=lambda k: k[index])[index]

    m = min(p, key=lambda k: k[index])[index]

    return min(series, key=lambda t: t[i])[i]
        ks.sort(key=lambda x: (x, -1) if x <= h else (abs(x - n), 1))
                sorted(roots, key=lambda r: (1 if r.imag else 0, r.real, abs(r.imag), sign(r.imag)))))

                    sorted(roots, key=lambda r: (1 if r.imag else 0, r.real, abs(r.imag), sign(r.imag)))))
    _roots, roots = sorted(roots, key=lambda r: (r.ax, r.ay)), []
            return sorted(seq, key=lambda monom: monom[0], reverse=True)

            return sorted(seq, key=lambda monom: order(monom[0]), reverse=True)

                replacements = sorted(list(x.items()), key=lambda k: gens_map[k[0]])
        reals = sorted(reals, key=lambda r: r[0].a)
            ip = max(range(i, m), key=lambda ip: abs(a[ip][j]))
    a.sort(key=lambda z: z[1])
        (x - 1)*(x + 1), x, predicate=lambda r: r.is_positive) == {S.One: 1}

    _nroots = sorted(_nroots, key=lambda x: x.sort_key())

    _sroots = sorted(_sroots, key=lambda x: x.sort_key())
        return sorted(indices, key=lambda x: score_table[x])
def dotedges(expr, atom=lambda x: not isinstance(x, Basic), pos=(), repeat=True):
            **do.kwargs(apply=lambda arg: self._print(arg), exclude=excl)

            **idl.kwargs(apply=lambda arg: self._print(arg))

        ).format(**prog.kwargs(apply=lambda arg: self._print(arg)))

        ).format(**mod.kwargs(apply=lambda arg: self._print(arg)))
                args = sorted(args, key=lambda x: isinstance(x, Quantity) or

            inneritems.sort(key=lambda x: x[0].__str__())
        for key in sorted(modifier_dict.keys(), key=lambda k:len(k), reverse=True) :
        args = sorted(args, key=lambda x: isinstance(x, Quantity) or
    Omega.sort(key=lambda x: nodes[x[1]].ht(), reverse=True)
            order_symbols = sorted(order_symbols.items(), key=lambda x: default_sort_key(x[0]))
def fu(rv, measure=lambda x: (L(x), x.count_ops())):

    >>> fu(sin(x)/cos(x), measure=lambda x: -x.count_ops()) # maximize op count
    funcs = sorted(funcs, key=lambda f: len(f.args))
                bucket.sort(key=lambda x: default_sort_key(x[0]))

                items.sort(key=lambda x: x - x0, reverse=flip)

        possible.sort(key=lambda x: x[0])

    monomials.sort(key=lambda x: x[1])

        l.sort(key=lambda x: x[1])
        c, d = min(newsol, key=lambda x: len(x[0].terms()) + len(x[1].terms()))
    assert fu(sin(x)/cos(x), measure=lambda x: x.count_ops()) == \

    assert fu(sin(x)/cos(x), measure=lambda x: -x.count_ops()) == \
                    for ba in sorted(bessels, key=lambda x: re(x.args[0])):
        return max(list(ordered(fterms)), key=lambda x: x.count(func))
            p = min(H, key=lambda h: h.degree())
                        v = _eval_simplify(ratio=2, measure=lambda x: 1)
    eqs = sorted(eqs, key=lambda eq: len(eq.free_symbols & set(syms)))
        return min([sol1, sol2], key=lambda x: ode_sol_simplicity(x, fx, trysolving=not simplify))
    >>> min([eq1, eq2], key=lambda i: ode_sol_simplicity(i, f(x)))
def _select_equations(eqs, funcs, key=lambda x: x):
        sorted_items = sorted(items, key=lambda val_cumprob: val_cumprob[1])
            sorted_ris = sorted(ris, key=lambda ri: ri.key)

        args_list = sorted(args_list, key=lambda x: x.args[0].key)
    >>> rl = minimize(inc, dec, objective=lambda x: -x)  # maximize
    >>> fn = greedy(tree, objective=lambda x: -x)  # maximize
    rl = minimize(inc, dec, objective=lambda x: -x)
    assert treeapply(3, {}, leaf=lambda x: x**2) == 9

    assert treeapply(tree, {list: min, tuple: max}, leaf=lambda x: x+1) == \

    maximize = partial(minimize, objective=lambda x: -x)

    fn = greedy(tree, objective=lambda x: -x)

    highest = greedy(tree, objective=lambda x: -x)
        free = sorted(self._free, key=lambda x: x[1])
        self.dum.sort(key=lambda x: x[0])

        dum.sort(key=lambda x: x[0])

        free = sorted(self.free, key=lambda x: x[1])

        sorted_free.sort(key=lambda x: x[0])

        return sorted(self.dum, key=lambda x: x[0])

        return sorted(set(self.index_types), key=lambda x: x.name)

        dummy_data.sort(key=lambda x: x[3])

                typ1 = sorted(set(cv[j-1].component.index_types), key=lambda x: x.name)

                typ2 = sorted(set(cv[j].component.index_types), key=lambda x: x.name)

        free.sort(key=lambda x: x[1])

    free = sorted(t_r.free, key=lambda x: x[1])
        indices_ret.sort(key=lambda x: free_indices.index(x))
    pairing_indices.sort(key=lambda x: min(x))

    diag_indices.sort(key=lambda x: flattened_indices.index(x))
        ps.sort(key=lambda x: x[1])

        sorting_keys.sort(key=lambda x: x[1])

        new_pos = sorted(range(len(expr.args)), key=lambda x: (0, default_sort_key(expr.args[x])) if fully_contracted[x] else (1,))

        sorted_data = sorted(enumerate(args), key=lambda x: default_sort_key(x[1]))
                funcs.sort(key=lambda x: inspect.getsourcelines(x)[1])

        tests.sort(key=lambda x: -x.lineno)

            sorted_slow = sorted(self.slow_test_functions, key=lambda r: r[1])
        output_args.sort(key=lambda x: str(x.name))

        output_args.sort(key=lambda x: str(x.name))

        output_args.sort(key=lambda x: str(x.name))
        >>> topological_sort((V, E), key=lambda v: -v)

    take = sorted(ms.items(), key=lambda x:(x[1], x[0]))

    ms = sorted(ms.items(), key=lambda x: x[1])
    assert topological_sort((V, E), key=lambda v: -v) == \
    a, b = sorted(expr.free_symbols, key=lambda s: s.name)
        items.sort(key=lambda x: x[0].__str__())
        items.sort(key=lambda x: x[0].__str__())
    events.sort(key=lambda e: e.depth)
    events.sort(key=lambda e: e.depth)
        sorted_events = sorted(events, key=lambda x: x.depth)
    return sorted(joined_domains.items(), key=lambda d: d[1])

            auth_chain.sort(key=lambda e: e.depth)
        events.sort(key=lambda e: -rank_map[e.event_id])
            events.sort(key=lambda e: e.internal_metadata.order)
        all_members.sort(key=lambda e: e.origin_server_ts)
        rooms_in_order.sort(key=lambda r: -(notifs_by_room[r][-1].received_ts or 0))
        waiting_list.sort(key=lambda t: t[0])
        updates = list(heapq.merge(room_rows, global_rows, key=lambda row: row[0]))
            admin_users.sort(key=lambda user: user_power[user])
                thumbnail_info = min(crop_info_list, key=lambda t: t[:-1])[-1]

                thumbnail_info = min(crop_info_list2, key=lambda t: t[:-1])[-1]

                thumbnail_info = min(info_list, key=lambda t: t[:-1])[-1]

                thumbnail_info = min(info_list2, key=lambda t: t[:-1])[-1]
    event_ids.sort(key=lambda ev_id: order_map[ev_id])
        notifs.sort(key=lambda r: r.stream_ordering)

        notifs.sort(key=lambda r: -(r.received_ts or 0))
        rows.sort(key=lambda row: (-int(row["priority_class"]), -int(row["priority"])))

        rows.sort(key=lambda row: (-int(row["priority_class"]), -int(row["priority"])))
        items = sorted(self.tables.items(), key=lambda i: (i[1]["perc"], i[0]))
        self._events = sorted(events.values(), key=lambda e: e.depth)
        state_rows.sort(key=lambda r: r.state_key)

            state_rows.sort(key=lambda r: r.state_key)
@patch("random.randint", new=lambda a, b: 0)

@patch("random.randint", new=lambda a, b: 0)
            sorted(channel.json_body["external_ids"], key=lambda k: k["auth_provider"]),
        callback_mock = Mock(side_effect=user_may_join_room, spec=lambda *x: None)

        callback_mock = Mock(side_effect=user_may_join_room, spec=lambda *x: None)

        mock = Mock(return_value=make_awaitable(True), spec=lambda *x: None)
        for node_id in lexicographical_topological_sort(graph_copy, key=lambda e: e):

        res = list(lexicographical_topological_sort(graph, key=lambda x: x))
        cache: LruCache[str, List[int]] = LruCache(5, size_callback=lambda x: 0)
        return sorted(jobs, key=lambda job: job.next_run_time or paused_sort_key)
                    modes[char] = max(items, key=lambda x: x[1])
            base_tzpath.sort(key=lambda x: not os.path.exists(x))
        assert expect_upper == a.decode(formatter=lambda x: x.upper())
    languages = sorted(languages, key=lambda x: x[1], reverse=True)

    return sorted(merge, key=lambda x: x[1], reverse=True)

    return sorted(results, key=lambda x: x[1], reverse=True)
        list.sort(key=lambda a: a.lower())
        cookies.sort(key=lambda a: len(a.path), reverse=True)
    def __init__(self, spec, adapter=lambda spec: spec.loader):
        >>> first_true(range(10), pred=lambda x: x > 5)

        >>> first_true(range(10), default='missing', pred=lambda x: x > 9)
    merged.sort(key=lambda i: i[0].startswith('oauth_'))
        >>> s = bucket(iterable, key=lambda x: x[0])  # Bucket by 1st character

def consecutive_groups(iterable, ordering=lambda x: x):

        enumerate(iterable), key=lambda x: x[0] - ordering(x[1])

def map_if(iterable, pred, func, func_else=lambda x: x):

        >>> list(unique_in_window('abAcda', 3, key=lambda x: x.lower()))
        sorted_params = sorted(params.items(), key=lambda val: val[0])
    for key in sorted(args, key=lambda x: x.lower()):
                    for bandwidth in sorted(bandwidthData, key=lambda x: x.at):
                fatals.sort(key=lambda e: -e.loc)

                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parserElement))))

                fatals.sort(key=lambda e: -e.loc)

                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parserElement))))
    return sorted(resolved, key=lambda diag: diag.index)
        self.assertEqual(json.loads(s, object_pairs_hook=lambda x: x), p)
            json.dumps(a, item_sort_key=lambda kv: kv[0].lower()))

            json.dumps(a, item_sort_key=lambda kv: kv[1]))
        self.assertEqual(json.loads(s, object_pairs_hook=lambda x: x), p)
        self.pools = RecentlyUsedContainer(num_pools, dispose_func=lambda p: p.close())
    results = sorted(results, key=lambda k: k[default_sort].lower())

        results = sorted(results, key=lambda k: sort_helper(k, sort_key, sort_keys), reverse=reverse)
                    times = sorted(times, key=lambda k: k['time'])

        library_stats = {k: list(v) for k, v in groupby(library_stats, key=lambda x: x['section_type'])}
        results = sorted(results, key=lambda k: k['sort_title'].lower())

                results = sorted(results, key=lambda k: helpers.cast_to_int(k['media_index']), reverse=reverse)

                results = sorted(results, key=lambda k: helpers.cast_to_int(k[sort_key]), reverse=reverse)

                results = sorted(results, key=lambda k: helpers.cast_to_int(k[sort_key].replace('4k', '2160p').rstrip('p')), reverse=reverse)

                results = sorted(results, key=lambda k: k[sort_key].lower(), reverse=reverse)
PLATFORM_NAMES = OrderedDict(sorted(list(PLATFORM_NAMES.items()), key=lambda k: k[0], reverse=True))

VIDEO_QUALITY_PROFILES = OrderedDict(sorted(list(VIDEO_QUALITY_PROFILES.items()), key=lambda k: k[0], reverse=True))

AUDIO_QUALITY_PROFILES = OrderedDict(sorted(list(AUDIO_QUALITY_PROFILES.items()), key=lambda k: k[0], reverse=True))
            templog = sorted(templog, key=lambda k: k[sort])
    return tuple(a['name'] for a in sorted(available_newsletter_agents(), key=lambda k: k['label']))

                filtered_children.sort(key=lambda x: helpers.cast_to_int(x['parent_media_index']))

                filtered_children.sort(key=lambda x: x['added_at'])
        filtered.sort(key=lambda x: x[sortcolumn])
        clean_servers.sort(key=lambda s: (s['label'], -int(s['local']), s['ip']))
    return tuple(a['name'] for a in sorted(available_notification_agents(), key=lambda k: k['label']))
            output = {'recently_added': sorted(recents_list, key=lambda k: k['added_at'], reverse=True)[:int(count)]}

        output = {'recently_added': sorted(recents_list, key=lambda k: k['added_at'], reverse=True)}

        session_list = sorted(session_list, key=lambda k: k['session_key'])
            @client.on(events.NewMessage(func=lambda e: e.is_private))
    insert_at.sort(key=lambda t: t[0])
def sanitize_parse_mode(mode, *, _nop_parse=lambda t: (t, []), _nop_unparse=lambda t, e: t):
            self.next_deadline = min(self.map.items(), key=lambda entry_state: entry_state[1].deadline)[0]
        files.sort(key=lambda t: t[1])

        type_to_constructors[t] = list(sorted(cs, key=lambda c: c.name))

    methods = sorted(methods, key=lambda m: m.name)

    cs = sorted(cs, key=lambda c: c.name)

    type_names = fmt(types, formatter=lambda x: x)
            tlobjects.sort(key=lambda x: x.name)
      return max(range(len(l)), key=lambda i: l[i])
      return max(range(len(l)), key=lambda i: l[i])
  TFManager.register('get_queue', callable=lambda qname: _get_queue(qname))

  TFManager.register('get', callable=lambda key: _get(key))

  TFManager.register('set', callable=lambda key, value: _set(key, value))
    sorted_cluster_info = sorted(cluster_info, key=lambda k: k['executor_id'])
    >>> net = tl.layers.Dense(n_units=100, act=lambda x : tl.act.lrelu(x, 0.2), name='dense')(net)

    >>> net = tl.layers.Dense(n_units=100, act=lambda x : tl.act.leaky_relu6(x, 0.2), name='dense')(net)

    >>> net = tl.layers.Dense(n_units=100, act=lambda x : tl.act.leaky_twice_relu6(x, 0.2, 0.2), name='dense')(net)
    word_counts.sort(key=lambda x: x[1], reverse=True)

    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))
    images_folder_list.sort(key=lambda s: int(s.split('/')[-1]))  # folder/images/ddd

        tmp.sort(key=lambda s: int(s.split('.')[-2]))  # ddd.jpg

    # tag_folder_list.sort(key=lambda s: int(s.split("/")[-1]))  # folder/images/ddd

    tag_folder_list.sort(key=lambda s: int(os.path.basename(s)))

        tmp.sort(key=lambda s: int(s.split('.')[-2]))  # ddd.txt
    images_folder_list.sort(key=lambda s: int(s.split('/')[-1]))  # folder/images/ddd

        tmp.sort(key=lambda s: int(s.split('.')[-2]))  # ddd.jpg

    # tag_folder_list.sort(key=lambda s: int(s.split("/")[-1]))  # folder/images/ddd

    tag_folder_list.sort(key=lambda s: int(os.path.basename(s)))

        tmp.sort(key=lambda s: int(s.split('.')[-2]))  # ddd.txt
    >>> out = tl.layers.ElementwiseLambda(fn=lambda x, y, z: x + y * tf.exp(z * 0.5), name='elementwiselambda')([noise, mean, std])
        net = tl.layers.Dense(n_units=100, act=lambda x: tl.act.lrelu(x, 0.2), name='dense')(net)

        net = tl.layers.Dense(n_units=100, act=lambda x: tl.act.leaky_relu6(x, 0.2), name='dense')(net)

        net = tl.layers.Dense(n_units=100, act=lambda x: tl.act.leaky_twice_relu6(x, 0.2, 0.2), name='dense')(net)
        first_var = min(var_list, key=lambda x: x.name)
        out = tl.layers.ElementwiseLambda(fn=lambda x, y, z: x + y * tf.exp(z * 0.5),
        text_sim_pairs = sorted(text_sim_pairs, key=lambda x: -x[1])
        def avg(assessments, weighted=lambda w: 1):
            list(scandir(path)), key=lambda entry: (not entry.is_dir(), entry.name)
        otimes.sort(key=lambda t: (t[1], t[4], t[5]), reverse=True)

        otimes.sort(key=lambda t: (t[1], t[4], t[5]), reverse=True)

        atimes.sort(reverse=True, key=lambda t: (t[1], t[3]))

        items.sort(key=lambda a: a[1], reverse=True)
    table = sorted(table, key=lambda t: str(t[1]))

    table_multiple_ops = sorted(table_multiple_ops, key=lambda t: (t[1], t[2]))

    table_op_class = sorted(iteritems(table_op_class), key=lambda t: t[1])

        big_key_files = sorted(big_key_files, key=lambda t: str(t[1]))
        opts.sort(key=lambda obj: (position_dict[obj.name], obj.name))

        opts.sort(key=lambda obj: (self.__position__[obj.name], obj.name))
            for i in sorted(iteritems(callbacks_time), key=lambda a: -a[1]):

        lll = sorted(zip(prof, ll), key=lambda a: a[0])

            for i in sorted(iteritems(callbacks_time), key=lambda a: a[1]):

            not_used.sort(key=lambda nu: (nu[0], str(nu[1])))

            not_used.sort(key=lambda nu: (nu[0], str(nu[1])))
            not_used.sort(key=lambda nu: (nu[0], str(nu[1])))
        rtransp = [i for i, _ in sorted(enumerate(transp), key=lambda x:x[1])]
            for i in sorted(iteritems(prof[12]), key=lambda a: a[1]):
            for i in sorted(iteritems(prof[6]), key=lambda a: a[1])[::-1]:
def test_get_diagonal_subtensor_view(wrap=lambda a: a):
        X_sum, updates = theano.scan(fn=lambda x: x.sum(),
    rules = [Rule(match=lambda _: False),
        full.sort(key=lambda el: el.split(':')[0])

        hosts.sort(key=lambda el: el.split(':')[0])
    for box in tqdm(sorted(boxes, key=lambda x: x['min_dist'], reverse=OVERLAP_TILES)):
            o.sort(key=lambda option: option.name)
            "__loader__": ObjectDict(get_source=lambda name: self.code),
                locales.sort(key=lambda pair: pair[1], reverse=True)
                sorted(list1, key=lambda x: x.pk), sorted(list2, key=lambda x: x.pk), msg=msg

                sorted(list1, key=lambda x: x[sorted_key]),

                sorted(list2, key=lambda x: x[sorted_key]),
                    inst = min(instances, key=lambda i: i.pos)
        predictions = sorted(prelim_predictions, key=lambda x: x["score"], reverse=True)[:n_best_size]

        predictions = sorted(prelim_predictions, key=lambda x: x["score"], reverse=True)[:n_best_size]
    records = list(sorted(records, key=lambda x: x["id"]))
        predictions = sorted(prelim_predictions, key=lambda x: x["score"], reverse=True)[:n_best_size]

        predictions = sorted(prelim_predictions, key=lambda x: x["score"], reverse=True)[:n_best_size]
                        best_hyp = sorted(hypotheses[b], key=lambda x: x[0], reverse=True)
            for group_id, idxs in sorted(buffer_per_group.items(), key=lambda x: x[0]):
            output.sort(key=lambda tup: (tup[2] * tup[3]), reverse=True)
            for _, span, label in sorted(predictions, key=lambda o: o[0], reverse=True):
        predictions = sorted(prelim_predictions, key=lambda x: x["score"], reverse=True)[:n_best_size]

        predictions = sorted(prelim_predictions, key=lambda x: x["score"], reverse=True)[:n_best_size]
        predictions = sorted(prelim_predictions, key=lambda x: x["score"], reverse=True)[:n_best_size]

        predictions = sorted(prelim_predictions, key=lambda x: x["score"], reverse=True)[:n_best_size]
        comments = sorted([comment for comment in issue.get_comments()], key=lambda i: i.created_at, reverse=True)
        merges = sorted(merges, key=lambda val: val[2])
            sorted_hyps = sorted(beam_hyp.beams, key=lambda x: x[0])

            sorted_hyps = sorted(beam_hyp.beams, key=lambda x: x[0])
            sorted_hyps = sorted(hypotheses.beams, key=lambda x: x[0])
    megabatches = [list(sorted(megabatch, key=lambda i: lengths[i], reverse=True)) for megabatch in megabatches]
        dlist.sort(key=lambda x: x.duration, reverse=True)
            added_tok_encoder_sorted = list(sorted(added_tok_encoder.items(), key=lambda x: x[1]))
    return os.path.join(folder, max(checkpoints, key=lambda x: int(_re_checkpoint.search(x).groups()[0])))
        memory_curr_trace = sorted(memory_curr_trace, key=lambda x: x.cpu_gpu.bytes, reverse=True)
    replacements.sort(key=lambda x: len(x[0]))
        enable_parser.set_defaults(func=lambda args: LfsEnableCommand(args))

        upload_parser.set_defaults(func=lambda args: LfsUploadCommand(args))
        login_parser.set_defaults(func=lambda args: LoginCommand(args))

        whoami_parser.set_defaults(func=lambda args: WhoamiCommand(args))

        logout_parser.set_defaults(func=lambda args: LogoutCommand(args))

        repo_create_parser.set_defaults(func=lambda args: RepoCreateCommand(args))
    qid_list = sorted(na_probs, key=lambda k: na_probs[k])

    qid_list = sorted(na_probs, key=lambda k: na_probs[k])

    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)

        prelim_predictions = sorted(prelim_predictions, key=lambda x: (x.start_logit + x.end_logit), reverse=True)
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))
                bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
    state_dict = torch.load(model_file, map_location=lambda s, l: default_restore_location(s, "cpu"))
        scores = sorted(scores, key=lambda x: x[1], reverse=True)
        scores = sorted(scores, key=lambda x: x[1], reverse=True)
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):
                pairs = sorted(v.items(), key=lambda x: x[1], reverse=True)

            ymls.sort(key=lambda x: results.index(x[:-4]))
            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))
            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):
            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):

    spans = sorted(span_dict.items(), key=lambda span_value: _get_span_length_key(span_value[0]), reverse=True)

    selected_spans.sort(key=lambda span_value: span_value[0][0])
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))

            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
            max_entity = max(entities, key=lambda entity: entity["scores"].max())
        answers = sorted(answers, key=lambda x: x["score"], reverse=True)[:top_k]
            dict_scores.sort(key=lambda x: x["score"], reverse=True)
            for score, candidate_label in sorted(zip(scores, candidate_labels), key=lambda x: -x[0])
            expected_height = max(expected_values, key=lambda item: item[0])[0]

            expected_width = max(expected_values, key=lambda item: item[1])[1]
            expected_height = max(expected_values, key=lambda item: item[0])[0]

            expected_width = max(expected_values, key=lambda item: item[1])[1]
            expected_height = max(expected_values, key=lambda item: item[0])[0]

            expected_width = max(expected_values, key=lambda item: item[1])[1]
            expected_height = max(expected_values, key=lambda item: item[0])[0]

            expected_width = max(expected_values, key=lambda item: item[1])[1]
        targets2 = [el["token_str"] for el in sorted(outputs, key=lambda x: x["score"], reverse=True)]
        cbs1 = list(sorted(cbs1, key=lambda cb: cb.__name__ if isinstance(cb, type) else cb.__class__.__name__))

        cbs2 = list(sorted(cbs2, key=lambda cb: cb.__name__ if isinstance(cb, type) else cb.__class__.__name__))
        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda x: 1.0)

            checkpoint_dir = sorted(checkpoints, key=lambda x: int(x.replace("checkpoint-", "")))[0]
    onnx_model_names.sort(key=lambda x: x.upper())
        sorted_indices = sort_objects(keys_to_sort, key=lambda x: x[1])

        sorted_indices = [x[0] for x in sorted(keys_to_sort, key=lambda x: x[1])]
        sorted_model_reports = sorted(model_reports, key=lambda s: s.split("] ")[-1])

        sorted_module_reports = sorted(other_module_reports, key=lambda s: s.split("] ")[-1])

        sorted_dict = sorted(self.model_results.items(), key=lambda t: t[0])
    sorted_index = sorted(localized_model_index.items(), key=lambda x: x[0].lower())

    all_models = sorted(all_models, key=lambda x: re.search(r"\*\*\[([^\]]*)", x).groups()[0].lower())
        sorted_dict = sorted(self.doc_test_results.items(), key=lambda t: t[0])
            blocks = sorted(blocks, key=lambda x: _re_identifier.search(x).groups()[0])
    proto.connection_made(Mock(is_closing=lambda: False, write=lambda b: fp.write(b.decode())))
            removed_peer = min(channel_peers, key=lambda x: x.last_response)

        return sorted(channel_peers, key=lambda x: x.last_response, reverse=True)[0:limit]
        assert chan_peers == sorted(chan_peers, key=lambda x: x.last_response, reverse=True)
            return sorted(commit_queue[:-1], key=lambda x: int(x.status == TODELETE) - 1 / x.timestamp)
            popular = set(heapq.nlargest(count, alive, key=lambda t: t[1]))
        for request, status_code in sorted(tribler_performed_requests, key=lambda rq: rq[0].time):
        for request, status_code in sorted(tribler_performed_requests, key=lambda rq: rq[0].time)[-30:]:
        for channel_info in sorted(channels, key=lambda x: x.get('state') != 'Personal'):
    for file in sorted(files, key=lambda x: x['index']):
        Column.CATEGORY:   d('category',   "",               width=30, tooltip_filter=lambda data: data),

        Column.SIZE:       d('size',       tr("Size"),       width=90, display_filter=lambda data: (format_size(float(data)) if data != "" else "")),

        Column.HEALTH:     d('health',     tr("Health"),     width=120, tooltip_filter=lambda data: f"{data}" + ('' if data == HEALTH_CHECKING else '\n(Click to recheck)'),),

        Column.UPDATED:    d('updated',    tr("Updated"),    width=120, display_filter=lambda timestamp: pretty_date(timestamp) if timestamp and timestamp > BITTORRENT_BIRTHDAY else 'N/A',),

        Column.VOTES:      d('votes',      tr("Popularity"), width=120, display_filter=format_votes, tooltip_filter=lambda data: get_votes_rating_description(data) if data is not None else None,),

        Column.STATE:      d('state',      "",               width=80, tooltip_filter=lambda data: data, sortable=False),
    def subtree(self, filter_by=lambda x: True):
        self.plot_data = dict(sorted(self.plot_data.items(), key=lambda x: x[0]))
@pytest.mark.parametrize("lockcls", [Lock, StrictFIFOLock], ids=lambda fn: fn.__name__)
                batch.sort(key=lambda t: t._counter)
    wav_paths.sort(key=lambda x: Path(x).stem)

    feat_paths.sort(key=lambda x: Path(x).stem)
    def _get_receptive_field_size(layers, stacks, kernel_size, dilation=lambda x: 2**x):
    def _get_receptive_field_size(layers, stacks, kernel_size, dilation=lambda x: 2**x):
    _whenRunning = attrib(type=Callable[..., None], default=lambda **_: None)

    _reactorExited = attrib(type=Callable[..., None], default=lambda **_: None)
        self.result = Deferred(canceller=lambda d: d.errback(self.cancelException))
        primesKeys = sorted(self.primes.keys(), key=lambda i: abs(i - bits))
        result = checkers.readAuthorizedKeyFile(fileobj, parseKey=lambda x: x)

        keydb = checkers.UNIXAuthorizedKeysFiles(self.userdb, parseKey=lambda x: x)

        keydb = checkers.UNIXAuthorizedKeysFiles(self.userdb, parseKey=lambda x: x)

        keydb = checkers.UNIXAuthorizedKeysFiles(self.userdb, parseKey=lambda x: x)

        keydb = checkers.UNIXAuthorizedKeysFiles(self.userdb, parseKey=lambda x: x)
        self.calls.sort(key=lambda a: a.getTime())
                (Certificate(x) for x in addedCerts), key=lambda cert: cert.digest()

            sorted(expectedCerts, key=lambda cert: cert.digest()),
        cooperator = Cooperator(scheduler=lambda x: reactor.callLater(0.00001, x))
        E.sort(key=lambda o: o[0])

        expect.sort(key=lambda o: o[0])
        self.servers.sort(key=lambda record: (record.priority, record.weight))
    optList.sort(key=lambda o: o.get("short", None) or o.get("long", None))
            sorted(names, key=lambda name: name.split(".")[:4]),
    def __init__(self, quiescentCallback=lambda c: None):
        release_to_download = sorted(j["releases"][version], key=lambda x: bool(x["packagetype"] == "bdist_wheel"))[-1]
        self._streams = sorted(streams, key=lambda k: k.quality)

        self._streams = sorted(self._streams, key=lambda k: k.mediatype)
        nodes.sort(key=lambda n: scores[n], reverse=True)

        # nodes.sort(key=lambda n: scores[n], reverse=True)
        result.sort(key=lambda v: (v[2], v[1]))
        result.sort(key=lambda v: (v[2], v[1]))

        result.sort(key=lambda v: (v[2], v[1]))
        value = max(values, key=lambda x: len(x))
    participants.sort(key=lambda participant: participant.relative_rssi)
            for rng in sorted(ranges, key=lambda r: r.score, reverse=True):

                addr_len = max(frequencies, key=lambda x: (frequencies[x], -x))

            address_length = max(counted, key=lambda x: (counted[x], -x))
        return sorted(self.existing_message_types.keys(), key=lambda x: x.name)
                result.append(max(checksums_for_length, key=lambda x: x.score))
            chosen_window_length = max(possible_window_lengths, key=lambda x: (possible_window_lengths[x], x))
                    next_allowed = min(allowed_values, key=lambda x: abs(x - value))
        message_types = sorted(copy.deepcopy(protocol.message_types), key=lambda x: x.name)
thread = threading.Thread(target=lambda : tornado.ioloop.IOLoop.current().start())
            self.chunks.sort(key=lambda x: x[0])

        #     results.sort(key=lambda x: x[0])
        datasets_list.sort(key=lambda datasets: datasets[0].variables[sort_property], reverse=not ascending)

        datasets_centering.sort(key=lambda dataset: dataset.variables[sort_property], reverse=not ascending)
                    chunks = [chunk for (i1, chunk) in sorted(chunks_map[expression].items(), key=lambda i1_and_chunk: i1_and_chunk[0])]
                results.sort(cmp=lambda a, b: cmp(a[0], b[0]))
				results.sort(cmp=lambda a, b: cmp(a[0], b[0]))
            columns = sorted(sparse_groups[id(sparse_matrix)], key=lambda col: dataset.columns[col].column_index)
    def create_slider(self, parent, label_text, value_min, value_max, getter, setter, value_steps=1000, format=" {0:<0.3f}", transform=lambda x: x, inverse=lambda x: x):

                                             )  # inverse=lambda x: math.log10(x), transform=lambda x: 10**x)

                                             )  # inverse=lambda x: math.log10(x), transform=lambda x: 10**x)

        # )#inverse=lambda x: math.log10(x), transform=lambda x: 10**x)

        # )#inverse=lambda x: math.log10(x), transform=lambda x: 10**x)
    def __init__(self, parent, label_text, value_min, value_max, value_steps, getter, setter, name=None, format="{0:<0.3f}", transform=lambda x: x, inverse=lambda x: x, update=lambda: None, uselog=False, numeric_type=float):
				self.dialog.create_slider(page, "scale: ", 1./100, 100., lambda : self.scale_dispersion, setter, format=" {0:>05.2f}", transform=lambda x: 10**x, inverse=lambda x: np.log10(x))
				self.layer.plot_window.create_slider(page, "scale: ", 1./20, 20., lambda : self.layer.plot_window.widget_volume.vector3d_scale, setter, format=" {0:>05.2f}", transform=lambda x: 10**x, inverse=lambda x: np.log10(x))
			label, slider, label_value = self.make_slider(page, "sigma_%d" % i, 0.0001, 1., 1000, "{0:<0.3f}", getter, setter, transform=lambda x: 10**x, inverse=lambda x: np.log10(x))

			label, slider, label_value = self.make_slider(page, "opacity_%d" % i, 0.0001, 1., 1000, "{0:<0.3f}", getter, setter, transform=lambda x: 10**x, inverse=lambda x: np.log10(x))

		label, slider, label_value = self.make_slider(page, "opacity_fg", 0.0001, 10., 1000, "{0:<0.3f}", getter, setter, transform=lambda x: 10**x, inverse=lambda x: np.log10(x))

		label, slider, label_value = self.make_slider(page, "opacity_bg", 0.0001, 10., 1000, "{0:<0.3f}", getter, setter, transform=lambda x: 10**x, inverse=lambda x: np.log10(x))

	def make_slider(self, parent, label_text, value_min, value_max, value_steps, format, getter, setter, name=None, transform=lambda x: x, inverse=lambda x: x):
            yield PathRefToDest(host, dest=lambda self, s: self.bin_dir / s.name)
        yield PathRefToDest(exe, dest=lambda self, _: self.dest / "Python", must=RefMust.COPY)

        yield PathRefToDest(resources, dest=lambda self, _: self.dest / "Resources")

        yield PathRefToDest(exe, dest=lambda self, _: self.dest / ".Python", must=RefMust.SYMLINK)
            yield PathRefToDest(host_include_marker.parent, dest=lambda self, _: self.include)

            yield PathRefToDest(make_file, dest=lambda self, s: self.dest / under_prefix)

            yield PathRefToDest(libs, dest=lambda self, s: self.dest / s.name)
            yield PathRefToDest(host_include_marker.parent, dest=lambda self, _: self.include)

            yield PathRefToDest(host_lib, dest=lambda self, _: self.lib)

        yield PathRefToDest(Path(interpreter.system_prefix) / "libs", dest=lambda self, s: self.dest / s.name)
        choices = sorted(choices, key=lambda a: 0 if a == "builtin" else 1)
    choices = sorted(choices, key=lambda a: 0 if a == "builtin" else 1)
        for _, group in groupby(u_log.versions, key=lambda v: v.wheel.version_tuple[0:2]):
    items.sort(key=lambda i: 2 if i.location[0].startswith(int_location) else (1 if "slow" in i.keywords else 0))
            itertools.chain.from_iterable(["--find-links", str(e)] for e in sorted(expected, key=lambda x: str(x))),
    return mocker.patch("virtualenv.seed.wheels.acquire.download_wheel", side_effect=lambda *a, **k: next(do))
        self._bars = dict(sorted(self._bars.items(), key=lambda tp: tp[0]))
def _get_external_signatures(global_ctx, sig_formatter=lambda x: x):
        self.deallocated_mem.sort(key=lambda k: k.position)
        location = sorted((i.location for i in value_type), key=lambda k: k.value)[-1]
    distances = sorted([(i, levenshtein_norm(key, i)) for i in namespace], key=lambda k: k[1])
            return sorted(types_list, key=lambda k: (k._bits, not k._is_signed), reverse=True)
        self.menu_items.sort(key=lambda item: item.order)
        matches.sort(key=lambda match: match[2])
        for item in sorted(menu_items, key=lambda i: i.order):
        self.summary_items.sort(key=lambda p: p.order)
    return sorted(BLANK_CHOICE_DASH + language_choices, key=lambda l: l[1].lower())
        rules.sort(key=lambda t: t[0])

        rules.sort(key=lambda t: t[0])

        rules.sort(key=lambda t: t[0])
    bulk_actions_list.sort(key=lambda x: x.action_priority)
            errors.sort(key=lambda e: e.id)
    tabs.sort(key=lambda tab: tab.order)

        tab_panels.sort(key=lambda panel: panel.order)
        context["panels"] = sorted(panels, key=lambda p: p.order)
    task_types.sort(key=lambda task_type: task_type[0].lower())

        task_types.sort(key=lambda task_type: task_type[0].lower())

        task_type_choices.sort(key=lambda task_type: task_type[1].lower())
    timeline.sort(key=lambda t: t["time"])
    page_types.sort(key=lambda page_type: page_type[0].lower())
        label=_("Type"), queryset=lambda request: get_content_types_for_filter()
        current_blocks.sort(key=lambda x: x[1].creation_counter)
            routes_for_class.sort(key=lambda route: route[1])
        columns.sort(key=lambda col: col["order"])

        rows.sort(key=lambda row: row["order"])
        sorted_results = sorted(results, key=lambda doc: doc.title)
        sorted_results = sorted(results, key=lambda img: img.title)
            document["searchtests_novel__characters"].sort(key=lambda c: c["name"])
            document["searchtests_novel__characters"].sort(key=lambda c: c["name"])
            document["searchtests_novel__characters"].sort(key=lambda c: c["name"])
        self.menu_items.sort(key=lambda item: item.order)
        SNIPPET_MODELS.sort(key=lambda x: x._meta.verbose_name)
        sorted_results = sorted(results, key=lambda page: page.url_path)
            all_backups.sort(key=lambda bi: bi.last_modified)
        echo.echo_class(LoggerTrick, write=lambda msg: class_module_logger.info(msg))
echo_events = functools.partial(echo.echo, write=lambda msg: logger.info(msg))
            sorted(children, key=lambda item: item.style['order'])):
                detected.sort(key=lambda a: a[-1] * a[-2])
    highlights.sort(key=lambda x: x[0])
            for x in sorted(self.values(), key=lambda x: x.name)

            for x in sorted(self.values(), key=lambda x: x.name)
    def setup(self, update, logger=lambda x: x):
    return sorted(choices, key=lambda tup: locale.strxfrm(key(tup)))
        return sorted(self.filter(id__in=ids), key=lambda unit: ids.index(unit.id))
        result.append({language.name: sorted(authors, key=lambda item: item[2])})
    def get_choices(self, empty=False, exclude=(), cond=lambda x: True):
    return sorted(result, key=lambda x: x[1], reverse=True)
        for k, v in sorted(iteritems(where), key=lambda t: t[0]):

            sorted_values = sorted(values.items(), key=lambda t: t[0])

        values = sorted(values.items(), key=lambda t: t[0])
        return sorted(self.keys(), key=lambda k: self[k], reverse=True)

        >>> uniq(["Foo", "foo", "bar"], key=lambda s: s.lower())
            addrs.sort(key=lambda x: x[0])
            res = self._get("groups/get", result_processor=lambda x: x["groups"])
        res = self._get("api_getwxcategory", result_processor=lambda x: x["category_list"])
        res = self._post("shakearound/device/search", data=data, result_processor=lambda x: x["data"])

        res = self._post("shakearound/page/add", data=data, result_processor=lambda x: x["data"])

        res = self._post("shakearound/page/update", data=data, result_processor=lambda x: x["data"])

        res = self._post("shakearound/page/search", data=data, result_processor=lambda x: x["data"])
        res = self._post("device/list", data=data, result_processor=lambda x: x["data"])
        return self._get("wxa/get_category", result_processor=lambda x: x["category_list"])

        return self._get("wxa/get_page", result_processor=lambda x: x["page_list"])

        return self._post("cgi-bin/open/create", data={"appid": appid}, result_processor=lambda x: x["open_appid"])

        return self._post("cgi-bin/open/get", data={"appid": appid}, result_processor=lambda x: x["open_appid"])
        res = self._get("merchant/group/getall", result_processor=lambda x: x["groups_detail"])
        res = self._get("merchant/express/getall", result_processor=lambda x: x["template_info"])
        res = self._get("merchant/shelf/getall", result_processor=lambda x: x["shelves"])
        self.players.sort(key=lambda x: -x.score)
        self.players = sorted(self.players, key=lambda a, b: unicodecmp(a.name, b.name))
    __bool__ = _ProxyLookup(bool, fallback=lambda self: False)

    __dir__ = _ProxyLookup(dir, fallback=lambda self: [])  # type: ignore
            self._rules.sort(key=lambda x: x.match_compare_key())

                rules.sort(key=lambda x: x.build_compare_key())
    sorted_environ = sorted(req.environ.items(), key=lambda x: repr(x[0]).lower())
            d.get("foo", type=lambda x: switch[x])
    rv = send_file(txt_path, environ, max_age=lambda p: 10)
        json.dumps(request.environ, default=lambda x: str(x)),
    vif_score_tuples = sorted(vif_score_tuples, key=lambda tup: -tup[1])
    env_vars.sort(key=lambda x: getattr(x[0], "pattern", x[0]))
        sorted(palette.keys(), reverse=True), key=lambda k: color_dist(x, palette[k])
            times = sorted(times, key=lambda x: x[1])
            suggested.items(), key=lambda x: suggestion_sort_helper(x[0].lower(), cmd)
        files.sort(key=lambda x: os.path.getmtime(x), reverse=newest_first)
            f.sort(key=lambda x: x[1].__code__.co_firstlineno)

            s.sort(key=lambda x: len(x[1]), reverse=True)
        macro.patch.sort(key=lambda x: x[2],reverse=True)
  llines.sort(key=lambda x: x.lineno)
            k for k, _ in sorted(data.meta["labels"].items(), key=lambda i: i[1])

            k for k, _ in sorted(data.meta["labels"].items(), key=lambda i: i[1])
            k for k, _ in sorted(data.meta["labels"].items(), key=lambda i: i[1])

            k for k, _ in sorted(data.meta["labels"].items(), key=lambda i: i[1])

            k for k, _ in sorted(data.meta["labels"].items(), key=lambda i: i[1])

            k for k, _ in sorted(data.meta["labels"].items(), key=lambda i: i[1])
    return None if not paths else max(paths, key=lambda path: os.lstat(path).st_mtime)
def filter_dict(dct, cndn=lambda _, v: v is not None):
def passthrough_module(parent, child, *, callback=lambda _: None):
            formats = sorted(formats, key=lambda f: f[0])
        entries = sorted(entries, key=lambda entry: entry.get('episode_number'))
        thumbnails.sort(key=lambda x: x['width'] * x['height'], reverse=True)
                for vod_item in sorted(vod_items, key=lambda x: x.get('episodeOrder', -1))),
            course['lessonData'].values(), key=lambda data: data['index'])
            'cast': traverse_obj(delivery, ('Contributors', ..., 'DisplayName'), default=[], expected_type=lambda x: x or None),
                for entry in sorted(product, key=lambda x: int_or_none(x.get('number', 0))):
            chapter_list.sort(key=lambda c: c['start_time'] or 0)
        gen_extractor_classes()), key=lambda ie: ie.IE_NAME.lower())
                category = min(cats, key=lambda c: c[2] - c[1])[0]
        return sorted(events, key=lambda k: k['timestamp'])

            keys = max(zones.keys(), key=lambda a: len(a))  # get longest key -- best match.
    @with_defaults(foo=lambda self: self.x + self.y)

with_default_shape = with_defaults(shape=lambda self: self.default_shape)
        return cls(*sorted(cols, key=lambda col: col.dataset))
                sorted(set(map(type, self.inputs)), key=lambda t: t.__name__),
            sorted(list(month_codes.items()), key=lambda item: item[1]),
    def __init__(self, cache=None, cleanup=lambda value_to_clean: None):
        sorted(adjustments, key=lambda adj: adj[0])
    rows = sorted(rows, key=lambda r: r[0], reverse=True)
    return [label for label, sort_value in sorted(label_sort_values.items(), key=lambda x: x[1])]
    rows = sorted(rows, key=lambda r: r[0], reverse=True)
IGNORED_PHRASES.sort(key=lambda regex: len(regex), reverse=True)
    for record in sorted(records, key=lambda e: ".".join(reversed(e.name.split(".")))):
        emoji_catalog[category].sort(key=lambda emoji_code: sort_order[emoji_code])
    return sorted((stream.to_dict() for stream in streams), key=lambda elt: elt["name"])

    return sorted((group.to_dict() for group in groups), key=lambda elt: elt["name"])
    streams_to_create.sort(key=lambda x: x.name)
        yield from sorted(messages_for_one_day, key=lambda m: m["ts"])
    messages.sort(key=lambda message: message.date_sent)

        msg = min(msg_list, key=lambda msg: msg.date_sent)

        max_message_id = max(msg_list, key=lambda msg: msg.id).id

    bucket_tups = sorted(bucket_tups, key=lambda x: x[1])
                state["streams"].sort(key=lambda elt: elt["name"])

            state["realm_user_groups"].sort(key=lambda group: group["id"])

        user_dicts = sorted(user_dicts, key=lambda x: x["user_id"])
    data["zerver_userprofile"].sort(key=lambda r: r["id"])
        table.sort(key=lambda row: row["id"])

    records.sort(key=lambda record: record["path"])

    return sorted(exports_dict.values(), key=lambda export_dict: export_dict["id"])
    rules[domain].sort(key=lambda x: x[0])
    streams.sort(key=lambda elt: elt["name"])
    subscribed.sort(key=lambda x: x["name"])

    unsubscribed.sort(key=lambda x: x["name"])

    never_subscribed.sort(key=lambda x: x["name"])
        streams = sorted(streams, key=lambda x: x.name)
    rows = sorted(rows, key=lambda tup: tup[1])

    return sorted(history, key=lambda x: -x["max_id"])
    return sorted(group_dicts.values(), key=lambda group_dict: group_dict["id"])
        arguments = sorted(arguments, key=lambda argument: "deprecated" in argument)
        to_process.sort(key=lambda x: x["start"])

                size_name_tuples.sort(reverse=True, key=lambda x: x[1]["h"])

    matches = sorted(matches, key=lambda k: k["index"])
            lang_list.sort(key=lambda lang: collator.sort_key(lang["name"]))
